[
["index.html", "Data Skills for Reproducible Science Overview 0.1 Course Aims 0.2 Intended Learning Outcomes 0.3 Course Outline 0.4 Course Resources 0.5 Formative Exercises 0.6 Resources", " Data Skills for Reproducible Science 2020-09-02 Overview This course provides an overview of skills needed for reproducible research and open science using the statistical programming language R. Students will learn about data visualisation, data tidying and wrangling, archiving, iteration and functions, probability and data simulations, general linear models, and reproducible workflows. Learning is reinforced through weekly assignments that involve working with different types of data. 0.1 Course Aims This course aims to teach students the basic principles of reproducible research and to provide practical training in data processing and analysis in the statistical programming language R. 0.2 Intended Learning Outcomes By the end of this course students will be able to: Draw on a range of specialised skills and techniques to formulate a research design appropriate to various kinds of questions in psychology and neuroscience Write scripts in R to organise and transform data sets using best accepted practices Explain basics of probability and its role in statistical inference Critically analyse data and report descriptive and inferential statistics in a reproducible manner 0.3 Course Outline The overview below lists the beginner learning outcomes only. Some lessons have additional learning outcomes for intermediate or advanced students. Getting Started Understand the components of the RStudio IDE Type commands into the console Understand function syntax Install a package Organise a project Create and compile an Rmarkdown document Working with Data Load built-in datasets Import data from CSV and Excel files Create a data table Understand the use the basic data types Understand and use the basic container types (list, vector) Use vectorized operations Data Visualisation Understand what types of graphs are best for different types of data Create common types of graphs with ggplot2: geom_bar(), geom_density(), geom_freqpoly(), geom_histogram(), geom_violin(), geom_boxplot(), geom_col(), geom_point(), geom_smooth() Set custom labels and colours Represent factorial designs with different colours or facets Save plots as an image file Tidy Data Understand the concept of tidy data Be able to use the 4 basic tidyr verbs: gather(), separate(), spread(), unite() Be able to chain functions using pipes Data Wrangling Be able to use the 6 main dplyr one-table verbs: select(), filter(), arrange(), mutate(), summarise(), group_by() Data Relations Be able to use the 4 mutating join verbs: left_join(), right_join(), inner_join(), full_join() Use the by argument to set the join columns Iteration &amp; Functions Work with iteration functions: rep(), seq(), and replicate() Use arguments by order or name Write your own custom functions with function() Set default values for the arguments in your functions Probability &amp; Simulation Understand what types of data are best modeled by different distributions: uniform, binomial, normal, poisson Generate and plot data randomly sampled from the above distributions Test sampled distributions against a null hypothesis using: exact binomial test, t-test (1-sample, independent samples, paired samples), correlation (pearson, kendall and spearman) Define the following statistical terms: p-value, alpha, power, smallest effect size of interest (SESOI), false positive (type I error), false negative (type II error), confidence interval (CI) Calculate power using iteration and a sampling function Introduction to GLM Define the components of the GLM Simulate data using GLM equations Identify the model parameters that correspond to the data-generation parameters Understand and plot residuals Predict new values using the model Explain the differences among coding schemes Reproducible Workflows Create a reproducible script in R Markdown Edit the YAML header to add table of contents and other options Include a table Include a figure Use source() to include code from an external file Report the output of an analysis using inline R 0.4 Course Resources Each chapter has several short video lectures for the main learning outcomes at the playlist Data Skills. The videos are captioned and watching with the captioning on is a useful way to learn the jargon of computational reproducibility. There is a custom package for this course called dataskills. You can install it with the code below. It will download all of the packages that are used in the book, along with an offline copy of this book, the shiny apps used in the book, and the exercises. devtools::install_github(&quot;psyteachr/msc-data-skills&quot;) 0.5 Formative Exercises Exercises are available at the end of each lesson’s webpage. These are not marked or mandatory, but if you can work through each of these (using web resources, of course), you will easily complete the marked assessments. Download all exercises and data files below as a ZIP archive. 01 intro: Intro to R, functions, R markdown 02 data: Vectors, tabular data, data import, pipes Essential Skills: You must be able to complete these exercises to advance in the class beyond the first two lectures 03 ggplot: Data visualisation 04 tidyr: Tidy Data 05 dplyr: Data wrangling 06 joins: Data relations 07 functions: Functions and iteration 08 simulation: Simulation 09 glm: GLM 0.6 Resources Miscellanous materials added throughout the semester, such as tips on installation, or the results of live-coding demos, can be found in the Appendices. Glasgow Psychology RStudio Learning Statistics with R by Navarro R for Data Science by Grolemund and Wickham 0.6.1 Online tutorials swirl R for Reproducible Scientific Analysis codeschool.com datacamp Improving your statistical inferences on Coursera 0.6.2 Cheat sheets You can access several cheatsheets in RStudio under the Help menu Or get the most recent RStudio Cheat Sheets 0.6.3 Other Style guide for R programming #rstats on twitter highly recommended! "],
["intro.html", "Chapter 1 Getting Started 1.1 Learning Objectives 1.2 Resources 1.3 What is R? 1.4 Getting Started 1.5 Add-on packages 1.6 Organising a project 1.7 Glossary 1.8 Exercises", " Chapter 1 Getting Started 1.1 Learning Objectives Understand the components of the RStudio IDE (video) Type commands into the console (video) Understand function syntax (video) Install a package (video) Organise a project (video) Create and compile an Rmarkdown document (video) 1.2 Resources Chapter 1: Introduction in R for Data Science RStudio IDE Cheatsheet Introduction to R Markdown R Markdown Cheatsheet R Markdown Reference 1.3 What is R? R is a programming environment for data processing and statistical analysis. We use R in Psychology at the University of Glasgow to promote reproducible research. This refers to being able to document and reproduce all of the steps between raw data and results. R allows you to write scripts that combine data files, clean data, and run analyses. There are many other ways to do this, including writing SPSS syntax files, but we find R to be a useful tool that is free, open source, and commonly used by research psychologists. See Appendix A for more information on on how to install R and associated programs. 1.3.1 The Base R Console If you open up the application called R, you will see an “R Console” window that looks something like this. Figure 1.1: The R Console window. You can close R and never open it again. We’ll be working entirely in RStudio in this class. ALWAYS REMEMBER: Launch R though the RStudio IDE Launch (RStudio.app), not (R.app). 1.3.2 RStudio RStudio is an Integrated Development Environment (IDE). This is a program that serves as a text editor, file manager, and provides many functions to help you read and write R code. Figure 1.2: The RStudio IDE RStudio is arranged with four window panes. By default, the upper left pane is the source pane, where you view and edit source code from files. The bottom left pane is usually the console pane, where you can type in commands and view output messages. The right panes have several different tabs that show you information about your code. You can change the location of panes and what tabs are shown under Preferences &gt; Pane Layout. Your browser does not support the video tag. 1.3.3 Configure RStudio In this class, you will be learning how to develop reproducible scripts. This means scripts that completely and transparently perform some analysis from start to finish in a way that yields the same result for different people using the same software on different computers. Transparency is a key value of science, as embodied in the “trust but verify” motto. When you do things reproducibly, others can understand and check your work. This benefits science, but there is a selfish reason, too: the most important person who will benefit from a reproducible script is your future self. When you return to an analysis after two weeks of vacation, you will thank your earlier self for doing things in a transparent, reproducible way, as you can easily pick up right where you left off. There are two tweaks that you should do to your RStudio installation to maximize reproducibility. Go to the preferences/settings menu, and uncheck the box that says Restore .RData into workspace at startup. If you keep things around in your workspace, things will get messy, and unexpected things will happen. You should always start with a clear workspace. This also means that you never want to save your workspace when you exit, so set this to Never. The only thing you want to save are your scripts. Figure 1.3: Alter these settings for increased reproducibility. Your settings should have: Restore .RData into workspace at startup: Checked Not Checked Save workspace to .RData on exit: Always Never Ask 1.4 Getting Started 1.4.1 Console commands We are first going to learn about how to interact with the console. In general, you will be developing R script or R Markdown files, rather than working directly in the console window. However, you can consider the console a kind of “sandbox” where you can try out lines of code and adapt them until you get them to do what you want. Then you can copy them back into the script editor. Mostly, however, you will be typing into the script editor window (either into an R script or an R Markdown file) and then sending the commands to the console by placing the cursor on the line and holding down the Ctrl key while you press Enter. The Ctrl+Enter key sequence sends the command in the script to the console. One simple way to learn about the R console is to use it as a calculator. Enter the lines of code below and see if your results match. Be prepared to make lots of typos (at first). 1 + 1 ## [1] 2 The R console remembers a history of the commands you typed in the past. Use the up and down arrow keys on your keyboard to scroll backwards and forwards through your history. It’s a lot faster than re-typing. 1 + 1 + 3 ## [1] 5 You can break up mathematical expressions over multiple lines; R waits for a complete expression before processing it. ## here comes a long expression ## let&#39;s break it over multiple lines 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 ## [1] 55 Text inside quotes is called a string. &quot;Good afternoon&quot; ## [1] &quot;Good afternoon&quot; You can break up text over multiple lines; R waits for a close quote before processing it. If you want to include a double quote inside this quoted string, escape it with a backslash. africa &lt;- &quot;I hear the drums echoing tonight But she hears only whispers of some quiet conversation She&#39;s coming in, 12:30 flight The moonlit wings reflect the stars that guide me towards salvation I stopped an old man along the way Hoping to find some old forgotten words or ancient melodies He turned to me as if to say, \\&quot;Hurry boy, it&#39;s waiting there for you\\&quot; - Toto&quot; cat(africa) # cat() prints the string ## I hear the drums echoing tonight ## But she hears only whispers of some quiet conversation ## She&#39;s coming in, 12:30 flight ## The moonlit wings reflect the stars that guide me towards salvation ## I stopped an old man along the way ## Hoping to find some old forgotten words or ancient melodies ## He turned to me as if to say, &quot;Hurry boy, it&#39;s waiting there for you&quot; ## ## - Toto 1.4.2 Objects Often you want to store the result of some computation for later use. You can store it in an object (also sometimes called a variable). An object in R: contains only letters, numbers, full stops, and underscores starts with a letter or a full stop and a letter distinguishes uppercase and lowercase letters (rickastley is not the same as RickAstley) The following are valid and different objects: songdata SongData song_data song.data .song.data never_gonna_give_you_up_never_gonna_let_you_down The following are not valid objects: _song_data 1song .1song song data song-data Use the assignment operator&lt;-` to assign the value on the right to the object named on the left. ## use the assignment operator &#39;&lt;-&#39; ## R stores the number in the object x &lt;- 5 Now that we have set x to a value, we can do something with it: x * 2 ## R evaluates the expression and stores the result in the object boring_calculation boring_calculation &lt;- 2 + 2 ## [1] 10 Note that it doesn’t print the result back at you when it’s stored. To view the result, just type the object name on a blank line. boring_calculation ## [1] 4 Once an object is assigned a value, its value doesn’t change unless you reassign the object, even if the objects you used to calculate it change. Predict what the code below does and test yourself: this_year &lt;- 2019 my_birth_year &lt;- 1976 my_age &lt;- this_year - my_birth_year this_year &lt;- 2020 After all the code above is run: this_year = 43 44 1976 2019 2020 my_birth_year = 43 44 1976 2019 2020 my_age = 43 44 1976 2019 2020 1.4.3 The environment Anytime you assign something to a new object, R creates a new entry in the global environment. Objects in the global environment exist until you end your session; then they disappear forever (unless you save them). Look at the Environment tab in the upper right pane. It lists all of the objects you have created. Click the broom icon to clear all of the objects and start fresh. You can also use the following functions in the console to view all objects, remove one object, or remove all objects. ls() # print the objects in the global environment rm(&quot;x&quot;) # remove the object named x from the global environment rm(list = ls()) # clear out the global environment In the upper right corner of the Environment tab, change List to Grid. Now you can see the type, length, and size of your objects, and reorder the list by any of these attributes. 1.4.4 Whitespace R mostly ignores whitespace: spaces, tabs, and line breaks. This means that you can use whitespace to help you organise your code. # a and b are identical a &lt;- list(ctl = &quot;Control Condition&quot;, exp1 = &quot;Experimental Condition 1&quot;, exp2 = &quot;Experimental Condition 2&quot;) # but b is much easier to read b &lt;- list(ctl = &quot;Control Condition&quot;, exp1 = &quot;Experimental Condition 1&quot;, exp2 = &quot;Experimental Condition 2&quot;) When you see &gt; at the beginning of a line, that means R is waiting for you to start a new command. However, if you see a + instead of &gt; at the start of the line, that means R is waiting for you to finish a command you started on a previous line. If you want to cancel whatever command you started, just press the Esc key in the console window and you’ll get back to the &gt; command prompt. # R waits until next line for evaluation (3 + 2) * 5 ## [1] 25 It is often useful to break up long functions onto several lines. cat(&quot;3, 6, 9, the goose drank wine&quot;, &quot;The monkey chewed tobacco on the streetcar line&quot;, &quot;The line broke, the monkey got choked&quot;, &quot;And they all went to heaven in a little rowboat&quot;, sep = &quot; \\n&quot;) ## 3, 6, 9, the goose drank wine ## The monkey chewed tobacco on the streetcar line ## The line broke, the monkey got choked ## And they all went to heaven in a little rowboat 1.4.5 Function syntax A lot of what you do in R involves calling a function and storing the results. A function is a named section of code that can be reused. For example, sd is a function that returns the standard deviation of the vector of numbers that you provide as the input argument. Functions are set up like this: function_name(argument1, argument2 = \"value\"). The arguments in parentheses can be named (like, argument1 = 10) or you can skip the names if you put them in the exact same order that they’re defined in the function. You can check this by typing ?sd (or whatever function name you’re looking up) into the console and the Help pane will show you the default order under Usage. You can also skip arguments that have a default value specified. Most functions return a value, but may also produce side effects like printing to the console. To illustrate, the function rnorm() generates random numbers from the standard normal distribution. The help page for rnorm() (accessed by typing ?rnorm in the console) shows that it has the syntax rnorm(n, mean = 0, sd = 1) where n is the number of randomly generated numbers you want, mean is the mean of the distribution, and sd is the standard deviation. The default mean is 0, and the default standard deviation is 1. There is no default for n, which means you’ll get an error if you don’t specify it: rnorm() ## Error in rnorm(): argument &quot;n&quot; is missing, with no default If you want 10 random numbers from a normal distribution with mean of 0 and standard deviation, you can just use the defaults. rnorm(10) ## [1] -0.20384277 0.96688267 1.88396001 1.45831749 0.03896113 -0.10014306 ## [7] 0.74044033 -0.76699267 -1.74144231 -0.16500428 If you want 10 numbers from a normal distribution with a mean of 100: rnorm(10, 100) ## [1] 97.73660 99.07303 99.19990 100.90338 96.50546 100.93076 98.95138 ## [8] 99.28327 99.36264 99.95147 This would be an equivalent but less efficient way of calling the function: rnorm(n = 10, mean = 100) ## [1] 99.20185 100.21152 99.23138 98.40846 100.62282 100.62669 101.09040 ## [8] 100.28609 99.86802 99.08204 We don’t need to name the arguments because R will recognize that we intended to fill in the first and second arguments by their position in the function call. However, if we want to change the default for an argument coming later in the list, then we need to name it. For instance, if we wanted to keep the default mean = 0 but change the standard deviation to 100 we would do it this way: rnorm(10, sd = 100) ## [1] 31.43331 131.26876 -83.81535 -188.45527 -39.90805 -76.82985 ## [7] 126.52885 19.00928 50.61698 129.40370 Some functions give a list of options after an argument; this means the default value is the first option. The usage entry for the power.t.test() function looks like this: power.t.test(n = NULL, delta = NULL, sd = 1, sig.level = 0.05, power = NULL, type = c(&quot;two.sample&quot;, &quot;one.sample&quot;, &quot;paired&quot;), alternative = c(&quot;two.sided&quot;, &quot;one.sided&quot;), strict = FALSE, tol = .Machine$double.eps^0.25) What is the default value for sd? NULL 1 0.05 two.sample What is the default value for type? NULL two.sample one.sample paired Which is equivalent to power.t.test(100, 0.5)? power.t.test(100, 0.5, sig.level = 1, sd = 0.05) power.t.test() power.t.test(n = 100) power.t.test(delta = 0.5, n = 100) 1.4.6 Getting help Start up help in a browser using the function help.start(). If a function is in base R or a loaded package, you can use the help(\"function_name\") function or the ?function_name shortcut to access the help file. If the package isn’t loaded, specify the package name as the second argument to the help function. # these methods are all equivalent ways of getting help help(&quot;rnorm&quot;) ?rnorm help(&quot;rnorm&quot;, package=&quot;stats&quot;) When the package isn’t loaded or you aren’t sure what package the function is in, use the shortcut ??function_name. What is the first argument to the mean function? trim na.rm mean x What package is read_excel in? readr readxl base stats 1.5 Add-on packages One of the great things about R is that it is user extensible: anyone can create a new add-on software package that extends its functionality. There are currently thousands of add-on packages that R users have created to solve many different kinds of problems, or just simply to have fun. There are packages for data visualisation, machine learning, neuroimaging, eyetracking, web scraping, and playing games such as Sudoku. Add-on packages are not distributed with base R, but have to be downloaded and installed from an archive, in the same way that you would, for instance, download and install a fitness app on your smartphone. The main repository where packages reside is called CRAN, the Comprehensive R Archive Network. A package has to pass strict tests devised by the R core team to be allowed to be part of the CRAN archive. You can install from the CRAN archive through R using the install.packages() function. There is an important distinction between installing a package and loading a package. 1.5.1 Installing a package This is done using install.packages(). This is like installing an app on your phone: you only have to do it once and the app will remain installed until you remove it. For instance, if you want to use PokemonGo on your phone, you install it once from the App Store or Play Store, and you don’t have to re-install it each time you want to use it. Once you launch the app, it will run in the background until you close it or restart your phone. Likewise, when you install a package, the package will be available (but not loaded) every time you open up R. You may only be able to permanently install packages if you are using R on your own system; you may not be able to do this on public workstations if you lack the appropriate privileges. Install the ggExtra package on your system. This package lets you create plots with marginal histograms. install.packages(&quot;ggExtra&quot;) If you don’t already have packages like ggplot2 and shiny installed, it will also install these dependencies for you. If you don’t get an error message at the end, the installation was successful. 1.5.2 Loading a package This is done using library(packagename). This is like launching an app on your phone: the functionality is only there where the app is launched and remains there until you close the app or restart. Likewise, when you run library(packagename) within a session, the functionality of the package referred to by packagename will be made available for your R session. The next time you start R, you will need to run the library() function again if you want to access its functionality. You can load the functions in ggExtra for your current R session as follows: library(ggExtra) You might get some red text when you load a package, this is normal. It is usually warning you that this package has functions that have the same name as other packages you’ve already loaded. You can use the convention package::function() to indicate in which add-on package a function resides. For instance, if you see readr::read_csv(), that refers to the function read_csv() in the readr add-on package. Now you can run the function ggExtra::runExample(), which runs an interactive example of marginal plots using shiny. ggExtra::runExample() 1.5.3 Install from GitHub Many R packages are not yet on CRAN because they are still in development. Increasingly, datasets and code for papers are available as packages you can download from github. You’ll need to install the devtools package to be able to install packages from github. Check if you have a package installed by trying to load it (e.g., if you don’t have devtools installed, library(\"devtools\") will display an error message) or by searching for it in the packages tab in the lower right pane. All listed packages are installed; all checked packages are currently loaded. Figure 1.4: Check installed and loaded packages in the packages tab in the lower right pane. # install devtools if you get # Error in loadNamespace(name) : there is no package called ‘devtools’ # install.packages(&quot;devtools&quot;) devtools::install_github(&quot;psyteachr/msc-data-skills&quot;) After you install the dataskills package, load it using the library() function. You can then try out some of the functions below. book() opens a local copy of this book in your web browser. app(\"plotdemo\") opens a shiny app that lets you see how simulated data would look in different plot styles exercise(1) creates and opens a file containing the exercises for this chapter ?disgust shows you the documentation for the built-in dataset disgust, which we will be using in future lessons library(dataskills) book() app(&quot;plotdemo&quot;) exercise(1) ?disgust How many different ways can you find to discover what functions are available in the dataskills package? 1.6 Organising a project Projects in RStudio are a way to group all of the files you need for one project. Most projects include scripts, data files, and output files like the PDF version of the script or images. Make a new directory where you will keep all of your materials for this class. If you’re using a lab computer, make sure you make this directory in your network drive so you can access it from other computers. Choose New Project… under the File menu to create a new project called 01-intro in this directory. 1.6.1 Structure Here is what an R script looks like. Don’t worry about the details for now. # load add-on packages library(tidyverse) # set object ---- n &lt;- 100 # simulate data ---- data &lt;- data.frame( id = 1:n, dv = c(rnorm(n/2, 0), rnorm(n/2, 1)), condition = rep(c(&quot;A&quot;, &quot;B&quot;), each = n/2) ) # plot data ---- ggplot(data, aes(condition, dv)) + geom_violin(trim = FALSE) + geom_boxplot(width = 0.25, aes(fill = condition), show.legend = FALSE) # save plot ---- ggsave(&quot;sim_data.png&quot;, width = 8, height = 6) It’s best if you follow the following structure when developing your own scripts: load in any add-on packages you need to use define any custom functions load or simulate the data you will be working with work with the data save anything you need to save Often when you are working on a script, you will realize that you need to load another add-on package. Don’t bury the call to library(package_I_need) way down in the script. Put it in the top, so the user has an overview of what packages are needed. You can add comments to an R script by with the hash symbol (#). The R interpreter will ignore characters from the hash to the end of the line. ## comments: any text from &#39;#&#39; on is ignored until end of line 22 / 7 # approximation to pi ## [1] 3.142857 If you add 4 or more dashes to the end of a comment, it acts like a header and creates an outline that you can see in the document outline (shift-cmd-O). 1.6.2 Reproducible reports with R Markdown We will make reproducible reports following the principles of literate programming. The basic idea is to have the text of the report together in a single document along with the code needed to perform all analyses and generate the tables. The report is then “compiled” from the original format into some other, more portable format, such as HTML or PDF. This is different from traditional cutting and pasting approaches where, for instance, you create a graph in Microsoft Excel or a statistics program like SPSS and then paste it into Microsoft Word. We will use R Markdown to create reproducible reports, which enables mixing of text and code. A reproducible script will contain sections of code in code blocks. A code block starts and ends with backtick symbols in a row, with some infomation about the code between curly brackets, such as {r chunk-name, echo=FALSE} (this runs the code, but does not show the text of the code block in the compiled document). The text outside of code blocks is written in markdown, which is a way to specify formatting, such as headers, paragraphs, lists, bolding, and links. Figure 1.5: A reproducible script. If you open up a new R Markdown file from a template, you will see an example document with several code blocks in it. To create an HTML or PDF report from an R Markdown (Rmd) document, you compile it. Compiling a document is called knitting in RStudio. There is a button that looks like a ball of yarn with needles through it that you click on to compile your file into a report. Create a new R Markdown file from the File &gt; New File &gt; R Markdown… menu. Change the title and author, then click the knit button to create an html file. 1.6.3 Working Directory Where should you put all of your files? When developing an analysis, you usually want to have all of your scripts and data files in one subtree of your computer’s directory structure. Usually there is a single working directory where your data and scripts are stored. Your script should only reference files in three locations, using the appropriate format. Where Example on the web “https://psyteachr.github.io/msc-data-skills/data/disgust_scores.csv” in the working directory “disgust_scores.csv” in a subdirectory “data/disgust_scores.csv” Never set or change your working directory in a script. If you are working with an R Markdown file, it will automatically use the same directory the .Rmd file is in as the working directory. If you are working with R scripts, store your main script file in the top-level directory and manually set your working directory to that location. You will have to reset the working directory each time you open RStudio, unless you create a project and access the script from the project. For instance, if you are on a Windows machine your data and scripts are in the directory C:\\Carla's_files\\thesis2\\my_thesis\\new_analysis, you will set your working directory in one of two ways: (1) by going to the Session pull down menu in RStudio and choosing Set Working Directory, or (2) by typing setwd(\"C:\\Carla's_files\\thesis2\\my_thesis\\new_analysis\") in the console window. It’s tempting to make your life simple by putting the setwd() command in your script. Don’t do this! Others will not have the same directory tree as you (and when your laptop dies and you get a new one, neither will you). When manually setting the working directory, always do so by using the Session &gt; Set Working Directory pull-down option or by typing setwd() in the console. If your script needs a file in a subdirectory of new_analysis, say, data/questionnaire.csv, load it in using a relative path so that it is accessible if you move the folder new_analysis to another location or computer: dat &lt;- read_csv(&quot;data/questionnaire.csv&quot;) # correct Do not load it in using an absolute path: dat &lt;- read_csv(&quot;C:/Carla&#39;s_files/thesis22/my_thesis/new_analysis/data/questionnaire.csv&quot;) # wrong Also note the convention of using forward slashes, unlike the Windows-specific convention of using backward slashes. This is to make references to files platform independent. 1.7 Glossary Each chapter ends with a glossary table defining the jargon introduced in this chapter. The links below take you to the glossary book, which you can also download for offline use with devtools::install_github(\"psyteachr/glossary\") and access the glossary offline with glossary::book(). term definition absolute path A file path that starts with / and is not appended to the working directory argument A variable that provides input to a function. assignment operator The symbol &lt;-, which functions like = and assigns the value on the right to the object on the left base r The set of R functions that come with a basic installation of R, before you add external packages cran The Comprehensive R Archive Network: a network of ftp and web servers around the world that store identical, up-to-date, versions of code and documentation for R. escape Include special characters like \" inside of a string by prefacing them with a backslash. function A named section of code that can be reused. global environment The interactive workspace where your script runs ide Integrated Development Environment: a program that serves as a text editor, file manager, and provides functions to help you read and write code. RStudio is an IDE for R. knit To create an HTML, PDF, or Word document from an R Markdown (Rmd) document markdown A way to specify formatting, such as headers, paragraphs, lists, bolding, and links. normal distribution A symmetric distribution of data where values near the centre are most probable. object A word that identifies and stores the value of some data for later use. package A group of R functions. panes RStudio is arranged with four window “panes”. project A way to organise related files in RStudio r markdown The R-specific version of markdown: a way to specify formatting, such as headers, paragraphs, lists, bolding, and links, as well as code blocks and inline code. relative path The location of a file in relation to the working directory. reproducible research Research that documents all of the steps between raw data and results in a way that can be verified. script A plain-text file that contains commands in a coding language, such as R. scripts NA standard deviation A statistic that measures how spread out data are relative to the mean. string A piece of text inside of quotes. variable A word that identifies and stores the value of some data for later use. vector A type of data structure that is basically a list of things like T/F values, numbers, or strings. whitespace Spaces, tabs and line breaks working directory The filepath where R is currently reading and writing files. 1.8 Exercises Download the first set of exercises and put it in the project directory you created earlier for today’s exercises. See the answers only after you’ve attempted all the questions. # run this to access the exercise dataskills::exercise(1) # run this to access the answers dataskills::exercise(1, answers = TRUE) "],
["data.html", "Chapter 2 Working with Data 2.1 Learning Objectives 2.2 Resources 2.3 Setup 2.4 Data tables 2.5 Basic data types 2.6 Basic container types 2.7 Troubleshooting 2.8 Glossary 2.9 Exercises", " Chapter 2 Working with Data 2.1 Learning Objectives Load built-in datasets Import data from CSV and Excel files Create a data table Understand the use the basic data types Understand and use the basic container types (list, vector) Use vectorized operations 2.2 Resources Chapter 11: Data Import in R for Data Science RStudio Data Import Cheatsheet Scottish Babynames Developing an analysis in R/RStudio: Scottish babynames (1/2) Developing an analysis in R/RStudio: Scottish babynames (2/2) 2.3 Setup You’ll need the following packages. # libraries needed for these examples library(tidyverse) library(dataskills) 2.4 Data tables 2.4.1 Built-in data R comes with built-in datasets. Some packages, like tidyr and dataskills, also contain data. The data() function lists the datasets available in a package. # lists datasets in dataskills data(package = &quot;dataskills&quot;) Type the name of a dataset into the console to see the data. Type ?smalldata into the console to see the dataset description. smalldata ## # A tibble: 10 x 4 ## id group pre post ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 S01 control 98.5 107. ## 2 S02 control 104. 89.1 ## 3 S03 control 105. 124. ## 4 S04 control 92.4 70.7 ## 5 S05 control 124. 125. ## 6 S06 exp 97.5 102. ## 7 S07 exp 87.8 126. ## 8 S08 exp 77.2 72.3 ## 9 S09 exp 97.0 109. ## 10 S10 exp 102. 114. You can also use the data() function to load a dataset into your global environment. # loads smalldata into the environment data(&quot;smalldata&quot;) Always, always, always, look at your data once you’ve created or loaded a table. Also look at it after each step that transforms your table. There are three main ways to look at your tibble: print(), glimpse(), and View(). The print() method can be run explicitly, but is more commonly called by just typing the variable name on the blank line. The default is not to print the entire table, but just the first 10 rows. It’s rare to print your data in a script; that is something you usually are doing for a sanity check, and you should just do it in the console. Let’s look at the smalldata table that we made above. smalldata ## # A tibble: 10 x 4 ## id group pre post ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 S01 control 98.5 107. ## 2 S02 control 104. 89.1 ## 3 S03 control 105. 124. ## 4 S04 control 92.4 70.7 ## 5 S05 control 124. 125. ## 6 S06 exp 97.5 102. ## 7 S07 exp 87.8 126. ## 8 S08 exp 77.2 72.3 ## 9 S09 exp 97.0 109. ## 10 S10 exp 102. 114. The function glimpse() gives a sideways version of the tibble. This is useful if the table is very wide and you can’t see all of the columns. It also tells you the data type of each column in angled brackets after each column name. We’ll learn about data types below. glimpse(smalldata) ## Rows: 10 ## Columns: 4 ## $ id &lt;chr&gt; &quot;S01&quot;, &quot;S02&quot;, &quot;S03&quot;, &quot;S04&quot;, &quot;S05&quot;, &quot;S06&quot;, &quot;S07&quot;, &quot;S08&quot;, &quot;S09&quot;, … ## $ group &lt;chr&gt; &quot;control&quot;, &quot;control&quot;, &quot;control&quot;, &quot;control&quot;, &quot;control&quot;, &quot;exp&quot;, &quot;… ## $ pre &lt;dbl&gt; 98.46606, 104.39774, 105.13377, 92.42574, 123.53268, 97.48676, … ## $ post &lt;dbl&gt; 106.70508, 89.09030, 123.67230, 70.70178, 124.95526, 101.61697,… The other way to look at the table is a more graphical spreadsheet-like version given by View() (capital ‘V’). It can be useful in the console, but don’t ever put this one in a script because it will create an annoying pop-up window when the user runs it. Now you can click on smalldata in the environment pane to open it up in a viewer that looks a bit like Excel. You can get a quick summary of a dataset with the summary() function. summary(smalldata) ## id group pre post ## Length:10 Length:10 Min. : 77.15 Min. : 70.70 ## Class :character Class :character 1st Qu.: 93.57 1st Qu.: 92.22 ## Mode :character Mode :character Median : 97.98 Median :107.76 ## Mean : 98.57 Mean :103.79 ## 3rd Qu.:103.88 3rd Qu.:121.19 ## Max. :123.53 Max. :126.30 You can even do things like calculate the difference between the means of two columns. pre_mean &lt;- mean(smalldata$pre) post_mean &lt;- mean(smalldata$post) post_mean - pre_mean ## [1] 5.223055 2.4.2 Importing data Built-in data are nice for examples, but you’re probably more interested in your own data. There are many different types of files that you might work with when doing data analysis. These different file types are usually distinguished by the three letter extension following a period at the end of the file name. Here are some examples of different types of files and the functions you would use to read them in or write them out. Extension File Type Reading Writing .csv Comma-separated values readr::read_csv() readr::write_csv() .tsv, .txt Tab-separated values readr::read_tsv() readr::write_tsv() .xls, .xlsx Excel workbook readxl::read_excel() NA .sav, .mat, … Multiple types rio::import() NA The double colon means that the function on the right comes from the package on the left, so readr::read_csv() refers to the read_csv() function in the readr package, and readxl::read_excel() refers to the function read_excel() in the package readxl. The function rio::import() from the rio package will read almost any type of data file, including SPSS and Matlab. Check the help with ?rio::import to see a full list. You can get a directory of data files used in this class for tutorials and exercises with the following code, which will create a directory called “data” in your project directory. Alternatively, you can download a zip file of the datasets. dataskills::getdata() Probably the most common file type you will encounter is .csv (comma-separated values). As the name suggests, a CSV file distinguishes which values go with which variable by separating them with commas, and text values are sometimes enclosed in double quotes. The first line of a file usually provides the names of the variables. For example, here are the first few lines of a CSV containing personality scores: ``` subj_id,O,C,E,A,N S01,4.428571429,4.5,3.333333333,5.142857143,1.625 S02,5.714285714,2.9,3.222222222,3,2.625 S03,5.142857143,2.8,6,3.571428571,2.5 S04,3.142857143,5.2,1.333333333,1.571428571,3.125 S05,5.428571429,4.4,2.444444444,4.714285714,1.625 ``` There are six variables in this dataset, and their names are given in the first line of the file: subj_id, O, C, E, A, and N. You can see that the values for each of these variables are given in order, separated by commas, on each subsequent line of the file. When you read in CSV files, it is best practice to use the readr::read_csv() function. The readr package is automatically loaded as part of the tidyverse package, which we will be using in almost every script. Note that you would normally want to store the result of the read_csv() function to an object, as so: csv_data &lt;- read_csv(&quot;data/5factor.csv&quot;) ## Parsed with column specification: ## cols( ## subj_id = col_character(), ## O = col_double(), ## C = col_double(), ## E = col_double(), ## A = col_double(), ## N = col_double() ## ) The read_csv() and read_tsv() functions will give you some information about the data you just read in so you can check the column names and data types. For now, it’s enough to know that col_double() refers to columns with numbers and col_character() refers to columns with words. We’ll learn in the toroubleshooting section below how to fix it if the function guesses the wrong data type. tsv_data &lt;- read_tsv(&quot;data/5factor.txt&quot;) xls_data &lt;- readxl::read_xls(&quot;data/5factor.xls&quot;) # you can load sheets from excel files by name or number rep_data &lt;- readxl::read_xls(&quot;data/5factor.xls&quot;, sheet = &quot;replication&quot;) spss_data &lt;- rio::import(&quot;data/5factor.sav&quot;) Once loaded, you can view your data using the data viewer. In the upper right hand window of RStudio, under the Environment tab, you will see the object babynames listed. If you click on the View icon (), it will bring up a table view of the data you loaded in the top left pane of RStudio. This allows you to check that the data have been loaded in properly. You can close the tab when you’re done looking at it, it won’t remove the object. 2.4.3 Creating data If we are creating a data table from scratch, we can use the tibble::tibble() function, and type the data right in. The tibble package is part of the tidyverse package that we loaded at the start of this chapter. Let’s create a small table with the names of three Avatar characters and their bending type. The tibble() function takes arguments with the names that you want your columns to have. The values are vectors that list the column values in order. If you don’t know the value for one of the cells, you can enter NA, which we have to do for Sokka because he doesn’t have any bending ability. If all the values in the column are the same, you can just enter one value and it will be copied for each row. avatar &lt;- tibble( name = c(&quot;Katara&quot;, &quot;Toph&quot;, &quot;Sokka&quot;), bends = c(&quot;water&quot;, &quot;earth&quot;, NA), friendly = TRUE ) # print it avatar ## # A tibble: 3 x 3 ## name bends friendly ## &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 Katara water TRUE ## 2 Toph earth TRUE ## 3 Sokka &lt;NA&gt; TRUE 2.4.4 Writing Data If you have data that you want to save to a CSV file, use readr::write_csv(), as follows. write_csv(avatar, &quot;avatar.csv&quot;) This will save the data in CSV format to your working directory. Create a new table called family with the first name, last name, and age of your family members. Save it to a CSV file called “family.csv”. Clear the object from your environment by restarting R or with the code remove(family). Load the data back in and view it. We’ll be working with tabular data a lot in this class, but tabular data is made up of vectors, which group together data with the same basic data type. The following sections explain some of this terminology to help you understand the functions we’ll be learning to process and analyse data. 2.5 Basic data types Data can be numbers, words, true/false values or combinations of these. In order to understand some later concepts, it’s useful to have a basic understanding of data types in R: numeric, character, and logical There is also a specific data type called a factor, which will probably give you a headache sooner or later, but we can ignore it for now. 2.5.1 Numeric data All of the real numbers are numeric data types (imaginary numbers are “complex”). There are two types of numeric data, integer and double. Integers are the whole numbers, like -1, 0 and 1. Doubles are numbers that can have fractional amounts. If you just type a plain number such as 10, it is stored as a double, even if it doesn’t have a decimal point. If you want it to be an exact integer, use the L suffix (10L). If you ever want to know the data type of something, use the typeof function. typeof(10) # double typeof(10.0) # double typeof(10L) # integer typeof(10i) # complex ## [1] &quot;double&quot; ## [1] &quot;double&quot; ## [1] &quot;integer&quot; ## [1] &quot;complex&quot; If you want to know if something is numeric (a double or an integer), you can use the function is.numeric() and it will tell you if it is numeric (TRUE) or not (FALSE). is.numeric(10L) is.numeric(10.0) is.numeric(&quot;Not a number&quot;) ## [1] TRUE ## [1] TRUE ## [1] FALSE 2.5.2 Character data Character strings are any text between quotation marks. typeof(&quot;This is a character string&quot;) typeof(&#39;You can use double or single quotes&#39;) ## [1] &quot;character&quot; ## [1] &quot;character&quot; This can include quotes, but you have to escape it using a backslash to signal the the quote isn’t meant to be the end of the string. my_string &lt;- &quot;The instructor said, \\&quot;R is cool,\\&quot; and the class agreed.&quot; cat(my_string) # cat() prints the arguments ## The instructor said, &quot;R is cool,&quot; and the class agreed. 2.5.3 Logical Data Logical data (also sometimes called “boolean” values) is one of two values: true or false. In R, we always write them in uppercase: TRUE and FALSE. class(TRUE) class(FALSE) ## [1] &quot;logical&quot; ## [1] &quot;logical&quot; When you compare two values with an operator, such as checking to see if 10 is greater than 5, the resulting value is logical. is.logical(10 &gt; 5) ## [1] TRUE You might also see logical values abbreviated as T and F, or 0 and 1. This can cause some problems down the road, so we will always spell out the whole thing. What data types are these: 100 integer double character logical factor 100L integer double character logical factor \"100\" integer double character logical factor 100.0 integer double character logical factor -100L integer double character logical factor factor(100) integer double character logical factor TRUE integer double character logical factor \"TRUE\" integer double character logical factor FALSE integer double character logical factor 1 == 2 integer double character logical factor 2.6 Basic container types Individual data values can be grouped together into containers. The main types of containers we’ll work with are vectors, lists, and data tables. 2.6.1 Vectors A vector in R is like a vector in mathematics: a set of ordered elements. All of the elements in a vector must be of the same data type (numeric, character, logical). You can create a vector by enclosing the elements in the function c(). ## put information into a vector using c(...) c(1, 2, 3, 4) c(&quot;this&quot;, &quot;is&quot;, &quot;cool&quot;) 1:6 # shortcut to make a vector of all integers x:y ## [1] 1 2 3 4 ## [1] &quot;this&quot; &quot;is&quot; &quot;cool&quot; ## [1] 1 2 3 4 5 6 What happens when you mix types? What class is the variable mixed? mixed &lt;- c(2, &quot;good&quot;, 2L, &quot;b&quot;, TRUE) You can’t mix data types in a vector; all elements of the vector must be the same data type. If you mix them, R will “coerce” them so that they are all the same. If you mix doubles and integers, the integers will be changed to doubles. If you mix characters and numeric types, the numbers will be coerced to characters, so 10 would turn into “10”. 2.6.1.1 Selecting values from a vector If we wanted to pick specific values out of a vector by position, we can use square brackets (an extract operator, or []) after the vector. values &lt;- c(10, 20, 30, 40, 50) values[2] # selects the second value ## [1] 20 You can select more than one value from the vector by putting a vector of numbers inside the square brackets. For example, you can select the 18th, 19th, 20th, 21st, 4th, 9th and 15th letter from the built-in vector LETTERS (which gives all the uppercase letters in the Latin alphabet). word &lt;- c(18, 19, 20, 21, 4, 9, 15) LETTERS[word] ## [1] &quot;R&quot; &quot;S&quot; &quot;T&quot; &quot;U&quot; &quot;D&quot; &quot;I&quot; &quot;O&quot; Can you decode the secret message? secret &lt;- c(14, 5, 22, 5, 18, 7, 15, 14, 14, 1, 7, 9, 22, 5, 25, 15, 21, 21, 16) You can also create ‘named’ vectors, where each element has a name. For example: vec &lt;- c(first = 77.9, second = -13.2, third = 100.1) vec ## first second third ## 77.9 -13.2 100.1 We can then access elements by name using a character vector within the square brackets. We can put them in any order we want, and we can repeat elements: vec[c(&quot;third&quot;, &quot;second&quot;, &quot;second&quot;)] ## third second second ## 100.1 -13.2 -13.2 We can get the vector of names using the names() function, and we can set or change them using something like names(vec2) &lt;- c(“n1”, “n2”, “n3”). Another way to access elements is by using a logical vector within the square brackets. This will pull out the elements of the vector for which the corresponding element of the logical vector is TRUE. If the logical vector doesn’t have the same length as the original, it will repeat. You can find out how long a vector is using the length() function. length(LETTERS) LETTERS[c(TRUE, FALSE)] ## [1] 26 ## [1] &quot;A&quot; &quot;C&quot; &quot;E&quot; &quot;G&quot; &quot;I&quot; &quot;K&quot; &quot;M&quot; &quot;O&quot; &quot;Q&quot; &quot;S&quot; &quot;U&quot; &quot;W&quot; &quot;Y&quot; 2.6.1.2 Repeating Sequences Here are some useful tricks to save typing when creating vectors. In the command x:y the : operator would give you the sequence of number starting at x, and going to y in increments of 1. 1:10 15.3:20.5 0:-10 ## [1] 1 2 3 4 5 6 7 8 9 10 ## [1] 15.3 16.3 17.3 18.3 19.3 20.3 ## [1] 0 -1 -2 -3 -4 -5 -6 -7 -8 -9 -10 What if you want to create a sequence but with something other than integer steps? You can use the seq() function. Look at the examples below and work out what the arguments do. seq(from = -1, to = 1, by = 0.2) seq(0, 100, length.out = 11) seq(0, 10, along.with = LETTERS) ## [1] -1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0 ## [1] 0 10 20 30 40 50 60 70 80 90 100 ## [1] 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 4.0 4.4 4.8 5.2 5.6 ## [16] 6.0 6.4 6.8 7.2 7.6 8.0 8.4 8.8 9.2 9.6 10.0 What if you want to repeat a vector many times? You could either type it out (painful) or use the rep() function, which can repeat vectors in different ways. rep(0, 10) # ten zeroes rep(c(1L, 3L), times = 7) # alternating 1 and 3, 7 times rep(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), each = 2) # A to C, 2 times each ## [1] 0 0 0 0 0 0 0 0 0 0 ## [1] 1 3 1 3 1 3 1 3 1 3 1 3 1 3 ## [1] &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; The rep() function is useful to create a vector of logical values (TRUE/FALSE or 1/0) to select values from another vector. # Get subject IDs in the pattern Y Y N N ... subject_ids &lt;- 1:40 yynn &lt;- rep(c(TRUE, FALSE), each = 2, length.out = length(subject_ids)) subject_ids[yynn] ## [1] 1 2 5 6 9 10 13 14 17 18 21 22 25 26 29 30 33 34 37 38 2.6.1.3 Vectorized Operations R performs calculations on vectors in a special way. Let’s look at an example using \\(z\\)-scores. A \\(z\\)-score is a deviation score(a score minus a mean) divided by a standard deviation. Let’s say we have a set of four IQ scores. ## example IQ scores: mu = 100, sigma = 15 iq &lt;- c(86, 101, 127, 99) If we want to subtract the mean from these four scores, we just use the following code: iq - 100 ## [1] -14 1 27 -1 This subtracts 100 from each element of the vector. R automatically assumes that this is what you wanted to do; it is called a vectorized operation and it makes it possible to express operations more efficiently. To calculate \\(z\\)-scores we use the formula: \\(z = \\frac{X - \\mu}{\\sigma}\\) where X are the scores, \\(\\mu\\) is the mean, and \\(\\sigma\\) is the standard deviation. We can expression this formula in R as follows: ## z-scores (iq - 100) / 15 ## [1] -0.93333333 0.06666667 1.80000000 -0.06666667 You can see that it computed all four \\(z\\)-scores with a single line of code. In later chapters, we’ll use vectorised operations to process our data, such as reverse-scoring some questionnaire items. 2.6.2 Lists Recall that vectors can contain data of only one type. What if you want to store a collection of data of different data types? For that purpose you would use a list. Define a list using the list() function. data_types &lt;- list( double = 10.0, integer = 10L, character = &quot;10&quot;, logical = TRUE ) str(data_types) # str() prints lists in a condensed format ## List of 4 ## $ double : num 10 ## $ integer : int 10 ## $ character: chr &quot;10&quot; ## $ logical : logi TRUE You can refer to elements of a list using square brackets like a vector, but you can also use the dollar sign notation ($) if the list items have names. data_types$logical ## [1] TRUE Explore the 5 ways shown below to extract a value from a list. What data type is each object? What is the difference between the single and double brackets? Which one is the same as the dollar sign? bracket1 &lt;- data_types[1] bracket2 &lt;- data_types[[1]] name1 &lt;- data_types[&quot;double&quot;] name2 &lt;- data_types[[&quot;double&quot;]] dollar &lt;- data_types$double 2.6.3 Tables The built-in, imported, and created data above are tabular data, data arranged in the form of a table. Tabular data structures allow for a collection of data of different types (characters, integers, logical, etc.) but subject to the constraint that each “column” of the table (element of the list) must have the same number of elements. The base R version of a table is called a data.frame, while the ‘tidyverse’ version is called a tibble. Tibbles are far easier to work with, so we’ll be using those. To learn more about differences between these two data structures, see vignette(\"tibble\"). Tabular data becomes especially important for when we talk about tidy data in chapter 4, which consists of a set of simple principles for structuring data. 2.6.3.1 Creating a table We learned how to create a table by importing a Excel or CSV file, and creating a table from scratch using the tibble() function. You can also use the tibble::tribble() function to create a table by row, rather than by column. You start by listing the column names, each preceded by a tilde (~), then you list the values for each column, row by row, separated by commas (don’t forget a comma at the end of each row). This method can be easier for some data, but doesn’t let you use shortcuts, like setting all of the values in a column to the same value or a repeating sequence. # by column using tibble avatar_by_col &lt;- tibble( name = c(&quot;Katara&quot;, &quot;Toph&quot;, &quot;Sokka&quot;, &quot;Azula&quot;), bends = c(&quot;water&quot;, &quot;earth&quot;, NA, &quot;fire&quot;), friendly = rep(c(TRUE, FALSE), c(3, 1)) ) # by row using tribble avatar_by_row &lt;- tribble( ~name, ~bends, ~friendly, &quot;Katara&quot;, &quot;water&quot;, TRUE, &quot;Toph&quot;, &quot;earth&quot;, TRUE, &quot;Sokka&quot;, NA, TRUE, &quot;Azula&quot;, &quot;fire&quot;, FALSE ) 2.6.3.2 Table info We can get information about the table using the functions ncol() (number of columns), nrow() (number of rows), dim() (the number of rows and number of columns), and name() (the column names). nrow(avatar) # how many rows? ncol(avatar) # how many columns? dim(avatar) # what are the table dimensions? names(avatar) # what are the column names? ## [1] 3 ## [1] 3 ## [1] 3 3 ## [1] &quot;name&quot; &quot;bends&quot; &quot;friendly&quot; 2.6.3.3 Accessing rows and columns There are various ways of accessing specific columns or rows from a table. The ones below are from base R and are useful to know about, but you’ll be learning easier (and more readable) ways in the tidyr and dplyr lessons. Examples of these base R accessing functions are provided here for reference, since you might see them in other people’s scripts. katara &lt;- avatar[1, ] # first row type &lt;- avatar[, 2] # second column (bends) benders &lt;- avatar[c(1, 2), ] # selected rows (by number) bends_name &lt;- avatar[, c(&quot;bends&quot;, &quot;name&quot;)] # selected columns (by name) friendly &lt;- avatar$friendly # by column name 2.7 Troubleshooting What if you import some data and it guesses the wrong column type? The most common reason is that a numeric column has some non-numbers in it somewhere. Maybe someone wrote a note in an otherwise numeric column. Columns have to be all one data type, so if there are any characters, the whole column is converted to character strings, and numbers like 1.2 get represented as “1.2”, which will cause very weird errors like \"100\" &lt; \"9\" == TRUE. You can catch this by looking at the output from read_csv() or using glimpse() to check your data. The data directory you created with dataskills::getdata() contains a file called “mess.csv”. Let’s try loading this dataset. mess &lt;- read_csv(&quot;data/mess.csv&quot;) ## Parsed with column specification: ## cols( ## `This is my messy dataset` = col_character() ## ) ## Warning: 27 parsing failures. ## row col expected actual file ## 1 -- 1 columns 7 columns &#39;data/mess.csv&#39; ## 2 -- 1 columns 7 columns &#39;data/mess.csv&#39; ## 3 -- 1 columns 7 columns &#39;data/mess.csv&#39; ## 4 -- 1 columns 7 columns &#39;data/mess.csv&#39; ## 5 -- 1 columns 7 columns &#39;data/mess.csv&#39; ## ... ... ......... ......... ............... ## See problems(...) for more details. You’ll get a warning with many parsing errors and mess is just a single column of the word “junk”. View the file data/mess.csv by clicking on it in the File pane, and choosing “View File”. Here are the first 10 lines. What went wrong? This is my messy dataset junk,order,score,letter,good,min_max,date junk,1,-1,a,1,1 - 2,2020-01-1 junk,missing,0.72,b,1,2 - 3,2020-01-2 junk,3,-0.62,c,FALSE,3 - 4,2020-01-3 junk,4,2.03,d,T,4 - 5,2020-01-4 First, the file starts with a note: “This is my messy dataset”. We want to skip the first two lines. You can do this with the argument skip in read_csv(). mess &lt;- read_csv(&quot;data/mess.csv&quot;, skip = 2) ## Parsed with column specification: ## cols( ## junk = col_character(), ## order = col_character(), ## score = col_double(), ## letter = col_character(), ## good = col_character(), ## min_max = col_character(), ## date = col_character() ## ) mess ## # A tibble: 26 x 7 ## junk order score letter good min_max date ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 junk 1 -1 a 1 1 - 2 2020-01-1 ## 2 junk missing 0.72 b 1 2 - 3 2020-01-2 ## 3 junk 3 -0.62 c FALSE 3 - 4 2020-01-3 ## 4 junk 4 2.03 d T 4 - 5 2020-01-4 ## 5 junk 5 NA e 1 5 - 6 2020-01-5 ## 6 junk 6 0.99 f 0 6 - 7 2020-01-6 ## 7 junk 7 0.03 g T 7 - 8 2020-01-7 ## 8 junk 8 0.67 h TRUE 8 - 9 2020-01-8 ## 9 junk 9 0.570 i 1 9 - 10 2020-01-9 ## 10 junk 10 0.9 j T 10 - 11 2020-01-10 ## # … with 16 more rows OK, that’s a little better, but this table is still a serious mess in several ways: junk is a column that we don’t need order should be an integer column good should be a logical column good uses all kinds of different ways to record TRUE and FALSE values min_max contains two pieces of numeric information, but is a character column date should be a date column We’ll learn how to deal with this mess in the chapters on tidy data and data wrangling, but we can fix a few things by setting the col_types argument in read_csv() to specify the column types for our two columns that were guessed wrong and skip the “junk” column. The argument col_types takes a list where the name of each item in the list is a column name and the value is from the table below. You can use the function, like col_double() or the abbreviation, like \"l\". Omitted column names are guessed. function abbreviation col_logical() l logical values col_integer() i integer values col_double() d numeric values col_character() c strings col_factor(levels, ordered) f a fixed set of values col_date(format = \"\") D with the locale’s date_format col_time(format = \"\") t with the locale’s time_format col_datetime(format = \"\") T ISO8601 date time col_number() n numbers containing the grouping_mark col_skip() _, - don’t import this column col_guess() ? parse using the “best” type based on the input # omitted values are guessed # ?col_date for format options ct &lt;- list( junk = &quot;-&quot;, # skip this column order = &quot;i&quot;, good = &quot;l&quot;, date = col_date(format = &quot;%Y-%m-%d&quot;) ) tidier &lt;- read_csv(&quot;data/mess.csv&quot;, skip = 2, col_types = ct) ## Warning: 1 parsing failure. ## row col expected actual file ## 2 order an integer missing &#39;data/mess.csv&#39; You will get a message about “1 parsing failure” when you run this. Warnings look scary at first, but always start by reading the message. The table tells you what row (2) and column (order) the error was found in, what kind of data was expected (integer), and what the actual value was (missing). If you specifically tell read_csv() to import a column as an integer, any characters in the column will produce a warning like this and then be recorded as NA. You can manually set what the missing values are recorded as with the na argument. tidiest &lt;- read_csv(&quot;data/mess.csv&quot;, skip = 2, na = &quot;missing&quot;, col_types = ct) Now order is an integer where “missing” is now NA, good is a logical value, where 0 and F are converted to FALSE and 1 and T are converted to TRUE, and date is a date type (adding leading zeros to the day). We’ll learn in later chapters how to fix the other problems. tidiest ## # A tibble: 26 x 6 ## order score letter good min_max date ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;date&gt; ## 1 1 -1 a TRUE 1 - 2 2020-01-01 ## 2 NA 0.72 b TRUE 2 - 3 2020-01-02 ## 3 3 -0.62 c FALSE 3 - 4 2020-01-03 ## 4 4 2.03 d TRUE 4 - 5 2020-01-04 ## 5 5 NA e TRUE 5 - 6 2020-01-05 ## 6 6 0.99 f FALSE 6 - 7 2020-01-06 ## 7 7 0.03 g TRUE 7 - 8 2020-01-07 ## 8 8 0.67 h TRUE 8 - 9 2020-01-08 ## 9 9 0.57 i TRUE 9 - 10 2020-01-09 ## 10 10 0.9 j TRUE 10 - 11 2020-01-10 ## # … with 16 more rows 2.8 Glossary term definition base r The set of R functions that come with a basic installation of R, before you add external packages character A data type representing strings of text. csv Comma-separated variable: a file type for representing data where each variable is separated from the next by a comma. data type The kind of data represented by an object. deviation score A score minus the mean double A data type representing a real decimal number escape Include special characters like \" inside of a string by prefacing them with a backslash. extension The end part of a file name that tells you what type of file it is (e.g., .R or .Rmd). extract operator A symbol used to get values from a container object, such as [, [[, or $ factor A data type where a specific set of values are stored with labels global environment The interactive workspace where your script runs integer A data type representing whole numbers. list A container data type that allows items with different data types to be grouped together. logical A data type representing TRUE or FALSE values. numeric A data type representing a real decimal number or integer operator A symbol that performs a mathematical operation, such as +, -, *, / tabular data Data in a rectangular table format, where each row has an entry for each column. tidy data A format for data that maps the meaning onto the structure. tidyverse A set of R packages that help you create and work with tidy data vector A type of data structure that is basically a list of things like T/F values, numbers, or strings. vectorized An operator or function that acts on each element in a vector 2.9 Exercises Download the exercises. See the answers only after you’ve attempted all the questions. # run this to access the exercise dataskills::exercise(2) # run this to access the answers dataskills::exercise(2, answers = TRUE) "],
["ggplot.html", "Chapter 3 Data Visualisation 3.1 Learning Objectives 3.2 Resources 3.3 Setup 3.4 Common Variable Combinations 3.5 Basic Plots 3.6 Customisation 3.7 Combination Plots 3.8 Overlapping Discrete Data 3.9 Overlapping Continuous Data 3.10 Interactive Plots 3.11 Quiz 3.12 Exercises", " Chapter 3 Data Visualisation Take the quiz to see if you need to review this chapter. 3.1 Learning Objectives 3.1.1 Basic Understand what types of graphs are best for different types of data 1 discrete 1 continuous 2 discrete 2 continuous 1 discrete, 1 continuous 3 continuous Create common types of graphs with ggplot2 geom_bar() geom_density() geom_freqpoly() geom_histogram() geom_violin() geom_boxplot() geom_col() geom_point() geom_smooth() Set custom labels and colours Represent factorial designs with different colours or facets Save plots as an image file 3.1.2 Intermediate Superimpose different types of graphs Add lines to graphs Deal with overlapping data Create less common types of graphs geom_tile() geom_density2d() geom_bin2d() geom_hex() geom_count() 3.1.3 Advanced Arrange plots in a grid using cowplot Adjust axes (e.g., flip coordinates, set axis limits) Change the theme Create interactive graphs with plotly 3.2 Resources Look at Data from Data Vizualization for Social Science Chapter 3: Data Visualisation of R for Data Science Chapter 28: Graphics for communication of R for Data Science Graphs in Cookbook for R ggplot2 cheat sheet ggplot2 documentation The R Graph Gallery (this is really useful) Top 50 ggplot2 Visualizations R Graphics Cookbook by Winston Chang ggplot extensions plotly for creating interactive graphs Stub for this lesson 3.3 Setup # libraries needed for these graphs library(tidyverse) library(plotly) library(cowplot) set.seed(30250) # makes sure random numbers are reproducible 3.4 Common Variable Combinations Continuous variables are properties you can measure, like height. Discrete (or categorical) variables are things you can count, like the number of pets you have. Categorical variables can be nominal, where the categories don’t really have an order, like cats, dogs and ferrets (even though ferrets are obviously best). They can also be ordinal, where there is a clear order, but the distance between the categories isn’t something you could exactly equate, like points on a Likert rating scale. Different types of visualisations are good for different types of variables. Before you read ahead, come up with an example of each type of variable combination and sketch the types of graphs that would best display these data. 1 discrete 1 continuous 2 discrete 2 continuous 1 discrete, 1 continuous 3 continuous 3.4.1 Data The code below creates some data frames with different types of data. We’ll learn how to simulate data like this in the Probability &amp; Simulation chapter, but for now just run the code chunk below. pets has a column with pet type pet_happy has happiness and age for 500 dog owners and 500 cat owners x_vs_y has two correlated continuous variables (x and y) overlap has two correlated ordinal variables and 1000 observations so there is a lot of overlap overplot has two correlated continuous variables and 10000 observations First, think about what kinds of graphs are best for representing these different types of data. pets &lt;- tibble( pet = sample( c(&quot;dog&quot;, &quot;cat&quot;, &quot;ferret&quot;, &quot;bird&quot;, &quot;fish&quot;), 100, TRUE, c(0.45, 0.40, 0.05, 0.05, 0.05) ) ) pet_happy &lt;- tibble( pet = rep(c(&quot;dog&quot;, &quot;cat&quot;), each = 500), happiness = c(rnorm(500, 55, 10), rnorm(500, 45, 10)), age = rpois(1000, 3) + 20 ) x_vs_y &lt;- tibble( x = rnorm(100), y = x + rnorm(100, 0, 0.5) ) overlap &lt;- tibble( x = rbinom(1000, 10, 0.5), y = x + rbinom(1000, 20, 0.5) ) overplot &lt;- tibble( x = rnorm(10000), y = x + rnorm(10000, 0, 0.5) ) 3.5 Basic Plots 3.5.1 Bar plot Bar plots are good for categorical data where you want to represent the count. ggplot(pets, aes(pet)) + geom_bar() Figure 3.1: Bar plot 3.5.2 Density plot Density plots are good for one continuous variable, but only if you have a fairly large number of observations. ggplot(pet_happy, aes(happiness)) + geom_density() Figure 3.2: Density plot You can represent subsets of a variable by assigning the category variable to the argument group, fill, or color. ggplot(pet_happy, aes(happiness, fill = pet)) + geom_density(alpha = 0.5) Figure 3.3: Grouped density plot Try changing the alpha argument to figure out what it does. 3.5.3 Frequency Polygons If you don’t want smoothed distributions, try geom_freqpoly(). ggplot(pet_happy, aes(happiness, color = pet)) + geom_freqpoly(binwidth = 1) Figure 3.4: Frequency ploygon plot Try changing the binwidth argument to 5 and 0.1. How do you figure out the right value? 3.5.4 Histogram Histograms are also good for one continuous variable, and work well if you don’t have many observations. Set the binwidth to control how wide each bar is. ggplot(pet_happy, aes(happiness)) + geom_histogram(binwidth = 1, fill = &quot;white&quot;, color = &quot;black&quot;) Figure 3.5: Histogram Histograms in ggplot look pretty bad unless you set the fill and color. If you show grouped histograms, you also probably want to change the default position argument. ggplot(pet_happy, aes(happiness, fill=pet)) + geom_histogram(binwidth = 1, alpha = 0.5, position = &quot;dodge&quot;) Figure 3.6: Grouped Histogram Try changing the position argument to “identity”, “fill”, “dodge”, or “stack”. 3.5.5 Column plot Column plots are the worst way to represent grouped continuous data, but also one of the most common. To make column plots with error bars, you first need to calculate the means, error bar uper limits (ymax) and error bar lower limits (ymin) for each category. You’ll learn more about how to use the code below in the next two lessons. # calculate mean and SD for each pet avg_pet_happy &lt;- pet_happy %&gt;% group_by(pet) %&gt;% summarise( mean = mean(happiness), sd = sd(happiness) ) ## `summarise()` ungrouping output (override with `.groups` argument) ggplot(avg_pet_happy, aes(pet, mean, fill=pet)) + geom_col(alpha = 0.5) + geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd), width = 0.25) + geom_hline(yintercept = 40) Figure 3.7: Column plot What do you think geom_hline() does? 3.5.6 Boxplot Boxplots are great for representing the distribution of grouped continuous variables. They fix most of the problems with using barplots for continuous data. ggplot(pet_happy, aes(pet, happiness, fill=pet)) + geom_boxplot(alpha = 0.5) Figure 3.8: Box plot 3.5.7 Violin plot Violin pots are like sideways, mirrored density plots. They give even more information than a boxplot about distribution and are especially useful when you have non-normal distributions. ggplot(pet_happy, aes(pet, happiness, fill=pet)) + geom_violin( trim = FALSE, draw_quantiles = c(0.25, 0.5, 0.75), alpha = 0.5 ) Figure 3.9: Violin plot Try changing the numbers in the draw_quantiles argument. 3.5.8 Scatter plot Scatter plots are a good way to represent the relationship between two continuous variables. ggplot(x_vs_y, aes(x, y)) + geom_point() Figure 3.10: Scatter plot using geom_point() 3.5.9 Line graph You often want to represent the relationship as a single line. ggplot(x_vs_y, aes(x, y)) + geom_smooth(method=&quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 3.11: Line plot using geom_smooth() 3.6 Customisation 3.6.1 Labels You can set custom titles and axis labels in a few different ways. ggplot(x_vs_y, aes(x, y)) + geom_smooth(method=&quot;lm&quot;) + ggtitle(&quot;My Plot Title&quot;) + xlab(&quot;The X Variable&quot;) + ylab(&quot;The Y Variable&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 3.12: Set custom labels with ggtitle(), xlab() and ylab() ggplot(x_vs_y, aes(x, y)) + geom_smooth(method=&quot;lm&quot;) + labs(title = &quot;My Plot Title&quot;, x = &quot;The X Variable&quot;, y = &quot;The Y Variable&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 3.13: Set custom labels with labs() 3.6.2 Colours You can set custom values for colour and fill using functions like scale_colour_manual() and scale_fill_manual(). The Colours chapter in Cookbook for R has many more ways to customise colour. ggplot(pet_happy, aes(pet, happiness, colour = pet, fill = pet)) + geom_violin() + scale_color_manual(values = c(&quot;darkgreen&quot;, &quot;dodgerblue&quot;)) + scale_fill_manual(values = c(&quot;#CCFFCC&quot;, &quot;#BBDDFF&quot;)) Figure 3.14: Set custom colour 3.6.3 Save as File You can save a ggplot using ggsave(). It saves the last ggplot you made, by default, but you can specify which plot you want to save if you assigned that plot to a variable. You can set the width and height of your plot. The default units are inches, but you can change the units argument to “in”, “cm”, or “mm”. box &lt;- ggplot(pet_happy, aes(pet, happiness, fill=pet)) + geom_boxplot(alpha = 0.5) violin &lt;- ggplot(pet_happy, aes(pet, happiness, fill=pet)) + geom_violin(alpha = 0.5) ggsave(&quot;demog_violin_plot.png&quot;, width = 5, height = 7) ggsave(&quot;demog_box_plot.jpg&quot;, plot = box, width = 5, height = 7) 3.7 Combination Plots 3.7.1 Violinbox plot To demonstrate the use of facet_grid() for factorial designs, we create a new column called agegroup to split the data into participants older than the meadian age or younger than the median age. New factors will display in alphabetical order, so we can use the factor() function to set the levels in the order we want. pet_happy %&gt;% mutate(agegroup = ifelse(age&lt;median(age), &quot;Younger&quot;, &quot;Older&quot;), agegroup = factor(agegroup, levels = c(&quot;Younger&quot;, &quot;Older&quot;))) %&gt;% ggplot(aes(pet, happiness, fill=pet)) + geom_violin(trim = FALSE, alpha=0.5, show.legend = FALSE) + geom_boxplot(width = 0.25, fill=&quot;white&quot;) + facet_grid(.~agegroup) + scale_fill_manual(values = c(&quot;orange&quot;, &quot;green&quot;)) Figure 3.15: Violin-box plot Set the show.legend argument to FALSE to hide the legend. We do this here because the x-axis already labels the pet types. 3.7.2 Violin-point-range plot You can use stat_summary() to superimpose a point-range plot showning the mean ± 1 SD. You’ll learn how to write your own functions in the lesson on Iteration and Functions. ggplot(pet_happy, aes(pet, happiness, fill=pet)) + geom_violin( trim = FALSE, alpha = 0.5 ) + stat_summary( fun.y = mean, fun.ymax = function(x) {mean(x) + sd(x)}, fun.ymin = function(x) {mean(x) - sd(x)}, geom=&quot;pointrange&quot; ) ## Warning: `fun.y` is deprecated. Use `fun` instead. ## Warning: `fun.ymin` is deprecated. Use `fun.min` instead. ## Warning: `fun.ymax` is deprecated. Use `fun.max` instead. Figure 3.16: Point-range plot using stat_summary() 3.7.3 Violin-jitter plot If you don’t have a lot of data points, it’s good to represent them individually. You can use geom_jitter to do this. pet_happy %&gt;% sample_n(50) %&gt;% # choose 50 random observations from the dataset ggplot(aes(pet, happiness, fill=pet)) + geom_violin( trim = FALSE, draw_quantiles = c(0.25, 0.5, 0.75), alpha = 0.5 ) + geom_jitter( width = 0.15, # points spread out over 15% of available width height = 0, # do not move position on the y-axis alpha = 0.5, size = 3 ) Figure 3.17: Violin-jitter plot 3.7.4 Scatter-line graph If your graph isn’t too complicated, it’s good to also show the individual data points behind the line. ggplot(x_vs_y, aes(x, y)) + geom_point(alpha = 0.25) + geom_smooth(method=&quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 3.18: Scatter-line plot 3.7.5 Grid of plots You can use the cowplot package to easily make grids of different graphs. First, you have to assign each plot a name. Then you list all the plots as the first arguments of plot_grid() and provide a list of labels. my_hist &lt;- ggplot(pet_happy, aes(happiness, fill=pet)) + geom_histogram( binwidth = 1, alpha = 0.5, position = &quot;dodge&quot;, show.legend = FALSE ) my_violin &lt;- ggplot(pet_happy, aes(pet, happiness, fill=pet)) + geom_violin( trim = FALSE, draw_quantiles = c(0.5), alpha = 0.5, show.legend = FALSE ) my_box &lt;- ggplot(pet_happy, aes(pet, happiness, fill=pet)) + geom_boxplot(alpha=0.5, show.legend = FALSE) my_density &lt;- ggplot(pet_happy, aes(happiness, fill=pet)) + geom_density(alpha=0.5, show.legend = FALSE) my_bar &lt;- pet_happy %&gt;% group_by(pet) %&gt;% summarise( mean = mean(happiness), sd = sd(happiness) ) %&gt;% ggplot(aes(pet, mean, fill=pet)) + geom_bar(stat=&quot;identity&quot;, alpha = 0.5, color = &quot;black&quot;, show.legend = FALSE) + geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd), width = 0.25) plot_grid( my_violin, my_box, my_density, my_bar, labels = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;) ) Figure 3.19: Grid of plots using cowplot 3.8 Overlapping Discrete Data 3.8.1 Reducing Opacity You can deal with overlapping data points (very common if you’re using Likert scales) by reducing the opacity of the points. You need to use trial and error to adjust these so they look right. ggplot(overlap, aes(x, y)) + geom_point(size = 5, alpha = .05) + geom_smooth(method=&quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 3.20: Deal with overlapping data using transparency 3.8.2 Proportional Dot Plots Or you can set the size of the dot proportional to the number of overlapping observations using geom_count(). overlap %&gt;% ggplot(aes(x, y)) + geom_count(color = &quot;#663399&quot;) Figure 3.21: Deal with overlapping data using geom_count() Alternatively, you can transform your data to create a count column and use the count to set the dot colour. overlap %&gt;% group_by(x, y) %&gt;% summarise(count = n()) %&gt;% ggplot(aes(x, y, color=count)) + geom_point(size = 5) + scale_color_viridis_c() ## `summarise()` regrouping output by &#39;x&#39; (override with `.groups` argument) Figure 3.22: Deal with overlapping data using dot colour The viridis package changes the colour themes to be easier to read by people with colourblindness and to print better in greyscale. Viridis is built into ggplot2 since v3.0.0. It uses scale_colour_viridis_c() and scale_fill_viridis_c() for continuous variables and scale_colour_viridis_d() and scale_fill_viridis_d() for discrete variables. 3.9 Overlapping Continuous Data Even if the variables are continuous, overplotting might obscure any relationships if you have lots of data. overplot %&gt;% ggplot(aes(x, y)) + geom_point() Figure 3.23: Overplotted data 3.9.1 2D Density Plot Use geom_density2d() to create a contour map. overplot %&gt;% ggplot(aes(x, y)) + geom_density2d() Figure 3.24: Contour map with geom_density2d() You can use stat_density_2d(aes(fill = ..level..), geom = \"polygon\") to create a heatmap-style density plot. overplot %&gt;% ggplot(aes(x, y)) + stat_density_2d(aes(fill = ..level..), geom = &quot;polygon&quot;) + scale_fill_viridis_c() Figure 3.25: Heatmap-density plot 3.9.2 2D Histogram Use geom_bin2d() to create a rectangular heatmap of bin counts. Set the binwidth to the x and y dimensions to capture in each box. overplot %&gt;% ggplot(aes(x, y)) + geom_bin2d(binwidth = c(1,1)) Figure 3.26: Heatmap of bin counts 3.9.3 Hexagonal Heatmap Use geomhex() to create a hexagonal heatmap of bin counts. Adjust the binwidth, xlim(), ylim() and/or the figure dimensions to make the hexagons more or less stretched. overplot %&gt;% ggplot(aes(x, y)) + geom_hex(binwidth = c(0.25, 0.25)) Figure 3.27: Hexagonal heatmap of bin counts 3.9.4 Correlation Heatmap I’ve included the code for creating a correlation matrix from a table of variables, but you don’t need to understand how this is done yet. We’ll cover mutate() and gather() functions in the dplyr and tidyr lessons. # generate two sets of correlated variables (a and b) heatmap &lt;- tibble( a1 = rnorm(100), b1 = rnorm(100) ) %&gt;% mutate( a2 = a1 + rnorm(100), a3 = a1 + rnorm(100), a4 = a1 + rnorm(100), b2 = b1 + rnorm(100), b3 = b1 + rnorm(100), b4 = b1 + rnorm(100) ) %&gt;% cor() %&gt;% # create the correlation matrix as.data.frame() %&gt;% # make it a data frame rownames_to_column(var = &quot;V1&quot;) %&gt;% # set rownames as V1 gather(&quot;V2&quot;, &quot;r&quot;, a1:b4) # wide to long (V2) Once you have a correlation matrix in the correct (long) format, it’s easy to make a heatmap using geom_tile(). ggplot(heatmap, aes(V1, V2, fill=r)) + geom_tile() + scale_fill_viridis_c() Figure 3.28: Heatmap using geom_tile() The file type is set from the filename suffix, or by specifying the argument device, which can take the following values: “eps”, “ps”, “tex”, “pdf”, “jpeg”, “tiff”, “png”, “bmp”, “svg” or “wmf”. 3.10 Interactive Plots You can use the plotly package to make interactive graphs. Just assign your ggplot to a variable and use the function ggplotly(). demog_plot &lt;- ggplot(pet_happy, aes(pet, happiness, fill=pet)) + geom_point(position = position_jitter(width= 0.2, height = 0), size = 2) ggplotly(demog_plot) Figure 3.29: Interactive graph using plotly Hover over the data points above and click on the legend items. 3.11 Quiz Generate a plot like this from the built-in dataset iris. Make sure to include the custom axis labels. Solution ggplot(iris, aes(Species, Petal.Width, fill = Species)) + geom_boxplot(show.legend = FALSE) + xlab(&quot;Flower Species&quot;) + ylab(&quot;Petal Width (in cm)&quot;) # there are many ways to do things, the code below is also correct ggplot(iris) + geom_boxplot(aes(Species, Petal.Width, fill = Species), show.legend = FALSE) + labs(x = &quot;Flower Species&quot;, y = &quot;Petal Width (in cm)&quot;) You have just created a plot using the following code. How do you save it? ggplot(cars, aes(speed, dist)) + geom_point() + geom_smooth(method = lm) ggsave() ggsave(\"figname\") ggsave(\"figname.png\") ggsave(\"figname.png\", plot = cars) Debug the following code. ggplot(iris) + geom_point(aes(Petal.Width, Petal.Length, colour = Species)) + geom_smooth(method = lm) + facet_grid(Species) Solution ggplot(iris, aes(Petal.Width, Petal.Length, colour = Species)) + geom_point() + geom_smooth(method = lm) + facet_grid(~Species) ## `geom_smooth()` using formula &#39;y ~ x&#39; Generate a plot like this from the built-in dataset ChickWeight. Solution ggplot(ChickWeight, aes(weight, Time)) + geom_hex(binwidth = c(10, 1)) + scale_fill_viridis_c() Generate a plot like this from the built-in dataset iris. ## `geom_smooth()` using formula &#39;y ~ x&#39; Solution pw &lt;- ggplot(iris, aes(Petal.Width, color = Species)) + geom_density() + xlab(&quot;Petal Width (in cm)&quot;) pl &lt;- ggplot(iris, aes(Petal.Length, color = Species)) + geom_density() + xlab(&quot;Petal Length (in cm)&quot;) + coord_flip() pw_pl &lt;- ggplot(iris, aes(Petal.Width, Petal.Length, color = Species)) + geom_point() + geom_smooth(method = lm) + xlab(&quot;Petal Width (in cm)&quot;) + ylab(&quot;Petal Length (in cm)&quot;) cowplot::plot_grid( pw, pl, pw_pl, labels = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), nrow = 3 ) 3.12 Exercises Download the exercises. See the plots to see what your plots should look like (this doesn’t contain the answer code). See the answers only after you’ve attempted all the questions. # run this to access the exercise dataskills::exercise(3) # run this to access the answers dataskills::exercise(3, answers = TRUE) "],
["tidyr.html", "Chapter 4 Tidy Data 4.1 Learning Objectives 4.2 Resources 4.3 Setup 4.4 Three Rules for Tidy Data 4.5 Tidying Data 4.6 Pipes 4.7 More Complex Example 4.8 Quiz 4.9 Exercises", " Chapter 4 Tidy Data Take the quiz to see if you need to review this chapter. 4.1 Learning Objectives 4.1.1 Basic Understand the concept of tidy data Be able to use the 4 basic tidyr verbs gather() separate() spread() unite() Be able to chain functions using pipes 4.1.2 Intermediate Be able to use arguments like sep, extra, and convert to handle less straightforward data cleaning 4.1.3 Advanced Be able to use regular expressions to separate complex columns 4.2 Resources Tidy Data Chapter 12: Tidy Data in R for Data Science Chapter 18: Pipes in R for Data Science Data wrangling cheat sheet 4.3 Setup # libraries needed library(tidyverse) library(readxl) set.seed(8675309) # makes sure random numbers are reproducible 4.4 Three Rules for Tidy Data Each variable must have its own column Each observation must have its own row Each value must have its own cell This table has three observations per row and the total_meanRT column contains two values. id score_1 score_2 score_3 rt_1 rt_2 rt_3 total_meanRT 1 4 3 7 857 890 859 14 (869) 2 3 1 1 902 900 959 5 (920) 3 2 5 4 757 823 901 11 (827) 4 6 2 6 844 788 624 14 (752) 5 1 7 2 659 764 690 10 (704) This is the tidy version. id trial rt score total mean_rt 1 1 857 4 14 869 1 2 890 3 14 869 1 3 859 7 14 869 2 1 902 3 5 920 2 2 900 1 5 920 2 3 959 1 5 920 3 1 757 2 11 827 3 2 823 5 11 827 3 3 901 4 11 827 4 1 844 6 14 752 4 2 788 2 14 752 4 3 624 6 14 752 5 1 659 1 10 704 5 2 764 7 10 704 5 3 690 2 10 704 4.5 Tidying Data {#data-personality} Download the data from personality.csv. These data are from a 5-factor (OCEAN) personality questionnaire. Each question is labelled with the domain (Op = openness, Co = concientiousness, Ex = extraversion, Ag = agreeableness, and Ne = neuroticism) and the question number. ocean &lt;- read_csv(&quot;https://psyteachr.github.io/msc-data-skills/data/personality.csv&quot;) user_id date Op1 Ne1 Ne2 Op2 Ex1 Ex2 Co1 Co2 Ne3 Ag1 Ag2 Ne4 Ex3 Co3 Op3 Ex4 Op4 Ex5 Ag3 Co4 Co5 Ne5 Op5 Ag4 Op6 Co6 Ex6 Ne6 Co7 Ag5 Co8 Ex7 Ne7 Co9 Op7 Ne8 Ag6 Ag7 Co10 Ex8 Ex9 0 2006-03-23 3 4 0 6 3 3 3 3 0 2 1 3 3 2 2 1 3 3 1 3 0 3 6 1 0 6 3 1 3 3 3 3 NA 3 0 2 NA 3 1 2 4 1 2006-02-08 6 0 6 0 0 0 0 0 0 0 6 6 6 0 6 0 0 0 0 6 6 0 6 0 6 0 6 6 6 6 0 6 0 6 6 0 6 0 6 0 6 2 2005-10-24 6 0 6 0 0 0 0 0 0 0 6 6 5 1 5 1 1 1 1 5 5 1 5 1 5 1 5 5 5 5 1 5 1 5 5 1 5 1 5 1 5 5 2005-12-07 6 4 4 4 2 3 3 3 1 4 0 2 5 3 5 3 6 6 1 5 5 4 2 4 1 4 3 1 1 0 1 4 2 4 5 1 2 1 5 4 5 8 2006-07-27 6 1 2 6 2 3 5 4 0 6 5 3 3 4 5 3 6 3 0 5 5 1 5 6 6 6 0 0 3 2 3 1 0 3 5 1 3 1 3 3 5 108 2006-02-28 3 2 1 4 4 4 4 3 1 5 4 2 3 4 4 3 3 3 4 3 3 1 4 5 4 5 4 1 4 5 4 2 2 4 4 1 4 3 5 4 2 4.5.1 gather() gather(data, key = \"key\", value = \"value\", ..., na.rm = FALSE, convert = FALSE, factor_key = FALSE) key is what you want to call the new column that the gathered column headers will go into; it’s “question” in this example. value is what you want to call the values in the gathered columns; they’re “score” in this example. ... refers to the columns you want to gather. You can refer to them by their names, like col1, col2, col3, col4 or col1:col4 or by their numbers, like 8, 9, 10 or 8:10. na.rm determines whether rows with NA values should be removed convert whether to automatically convert the results to another data type factor_key whether to store the key values as a factor (with the same order as in the table) or character vector ocean is in wide format, with a separate column for each question. Change it to long format, with a row for each user/question observation. The resulting dataframe should have the columns: user_id, date, question, and score. ocean_gathered &lt;- gather(ocean, &quot;question&quot;, &quot;score&quot;, Op1:Ex9) user_id date question score 0 2006-03-23 Op1 3 1 2006-02-08 Op1 6 2 2005-10-24 Op1 6 5 2005-12-07 Op1 6 8 2006-07-27 Op1 6 108 2006-02-28 Op1 3 4.5.2 separate() separate(data, col, into, sep = \"[^[:alnum:]]+\", remove = TRUE, convert = FALSE, extra = \"warn\", fill = \"warn\") col is the column you want to separate into is a vector of new column names sep is the character(s) that separate your new columns. This defaults to anything that isn’t alphanumeric, like .,_-/: remove determines whether the separated column (col) will be removed from the new data table. The default is to remove it. convert whether to automatically convert the results to another data type extra controls what happens when there are too many pieces fill controls what happens when there are not enough pieces Split the question column into two columns: domain and qnumber. There is no character to split on, here, but you can separate a column after a specific number of characters by setting sep to an integer. For example, to split “abcde” after the third character, use sep = 3, which results in c(\"abc\", \"de\"). You can also use negative number to split before the nth character from the right. For example, to split a column that has words of various lengths and 2-digit suffixes (like “lisa03”“,”amanda38\"), you can use sep = -2. ocean_sep &lt;- separate(ocean_gathered, question, c(&quot;domain&quot;, &quot;qnumber&quot;), sep = 2) user_id date domain qnumber score 0 2006-03-23 Op 1 3 1 2006-02-08 Op 1 6 2 2005-10-24 Op 1 6 5 2005-12-07 Op 1 6 8 2006-07-27 Op 1 6 108 2006-02-28 Op 1 3 If you want to separate just at full stops, you need to use sep = \"\\\\.\", not sep = \".\". The two slashes escape the full stop, making it interpreted as a literal full stop and not the regular expression for any character. 4.5.3 unite() unite(data, col, ..., sep = \"_\", remove = TRUE) col is your new united column ... refers to the columns you want to unite sep is the character(s) that will separate your united columns remove determines whether the united columns (...) will be removed from the new data table. The default is to remove them. Put the domain and qnumber columns back together into a new column named domain_n. Make it in a format like “Op_Q1”. ocean_unite &lt;- unite(ocean_sep, &quot;domain_n&quot;, domain, qnumber, sep = &quot;_Q&quot;) user_id date domain_n score 0 2006-03-23 Op_Q1 3 1 2006-02-08 Op_Q1 6 2 2005-10-24 Op_Q1 6 5 2005-12-07 Op_Q1 6 8 2006-07-27 Op_Q1 6 108 2006-02-28 Op_Q1 3 4.5.4 spread() spread(data, key, value, fill = NA, convert = FALSE, drop = TRUE, sep = NULL) You can reverse the processes above, as well. For example, you can convert data from long format into wide format. key is the column that contains your new column headers value is the column that contains the values in the new spread columns ocean_spread &lt;- spread(ocean_unite, domain_n, score) user_id date Ag_Q1 Ag_Q2 Ag_Q3 Ag_Q4 Ag_Q5 Ag_Q6 Ag_Q7 Co_Q1 Co_Q10 Co_Q2 Co_Q3 Co_Q4 Co_Q5 Co_Q6 Co_Q7 Co_Q8 Co_Q9 Ex_Q1 Ex_Q2 Ex_Q3 Ex_Q4 Ex_Q5 Ex_Q6 Ex_Q7 Ex_Q8 Ex_Q9 Ne_Q1 Ne_Q2 Ne_Q3 Ne_Q4 Ne_Q5 Ne_Q6 Ne_Q7 Ne_Q8 Op_Q1 Op_Q2 Op_Q3 Op_Q4 Op_Q5 Op_Q6 Op_Q7 0 2006-03-23 2 1 1 1 3 NA 3 3 1 3 2 3 0 6 3 3 3 3 3 3 1 3 3 3 2 4 4 0 0 3 3 1 NA 2 3 6 2 3 6 0 0 1 2006-02-08 0 6 0 0 6 6 0 0 6 0 0 6 6 0 6 0 6 0 0 6 0 0 6 6 0 6 0 6 0 6 0 6 0 0 6 0 6 0 6 6 6 2 2005-10-24 0 6 1 1 5 5 1 0 5 0 1 5 5 1 5 1 5 0 0 5 1 1 5 5 1 5 0 6 0 6 1 5 1 1 6 0 5 1 5 5 5 5 2005-12-07 4 0 1 4 0 2 1 3 5 3 3 5 5 4 1 1 4 2 3 5 3 6 3 4 4 5 4 4 1 2 4 1 2 1 6 4 5 6 2 1 5 8 2006-07-27 6 5 0 6 2 3 1 5 3 4 4 5 5 6 3 3 3 2 3 3 3 3 0 1 3 5 1 2 0 3 1 0 0 1 6 6 5 6 5 6 5 108 2006-02-28 5 4 4 5 5 4 3 4 5 3 4 3 3 5 4 4 4 4 4 3 3 3 4 2 4 2 2 1 1 2 1 1 2 1 3 4 4 3 4 4 4 4.6 Pipes Pipes are a way to order your code in a more readable format. Let’s say you have a small data table with 10 participant IDs, two columns with variable type A, and 2 columns with variable type B. You want to calculate the mean of the A variables and the mean of the B variables and return a table with 10 rows (1 for each participant) and 3 columns (id, A_mean and B_mean). One way you could do this is by creating a new object at every step and using that object in the next step. This is pretty clear, but you’ve created 6 unnecessary data objects in your environment. This can get confusing in very long scripts. # make a data table with 10 subjects data_original &lt;- tibble( id = 1:10, A1 = rnorm(10, 0), A2 = rnorm(10, 1), B1 = rnorm(10, 2), B2 = rnorm(10, 3) ) # gather columns A1 to B2 into &quot;variable&quot; and &quot;value&quot; columns data_gathered &lt;- gather(data_original, variable, value, A1:B2) # separate the variable column at the _ into &quot;var&quot; and &quot;var_n&quot; columns data_separated &lt;- separate(data_gathered, variable, c(&quot;var&quot;, &quot;var_n&quot;), sep = 1) # group the data by id and var data_grouped &lt;- group_by(data_separated, id, var) # calculate the mean value for each id/var data_summarised &lt;- summarise(data_grouped, mean = mean(value)) ## `summarise()` regrouping output by &#39;id&#39; (override with `.groups` argument) # spread the mean column into A and B columns data_spread &lt;- spread(data_summarised, var, mean) # rename A and B to A_mean and B_mean data &lt;- rename(data_spread, A_mean = A, B_mean = B) data ## # A tibble: 10 x 3 ## # Groups: id [10] ## id A_mean B_mean ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 -0.594 1.02 ## 2 2 0.744 2.72 ## 3 3 0.931 3.93 ## 4 4 0.720 1.97 ## 5 5 -0.0281 1.95 ## 6 6 -0.0983 3.21 ## 7 7 0.126 0.926 ## 8 8 1.45 2.38 ## 9 9 0.298 1.66 ## 10 10 0.559 2.10 You can name each object data and keep replacing the old data object with the new one at each step. This will keep your environment clean, but I don’t recommend it because it makes it too easy to accidentally run your code out of order when you are running line-by-line for development or debugging. One way to avoid extra objects is to nest your functions, literally replacing each data object with the code that generated it in the previous step. This can be fine for very short chains. mean_petal_width &lt;- round(mean(iris$Petal.Width), 2) But it gets extremely confusing for long chains: # do not ever do this!! data &lt;- rename( spread( summarise( group_by( separate( gather( tibble( id = 1:10, A1 = rnorm(10, 0), A2 = rnorm(10, 1), B1 = rnorm(10, 2), B2 = rnorm(10,3)), variable, value, A1:B2), variable, c(&quot;var&quot;, &quot;var_n&quot;), sep = 1), id, var), mean = mean(value)), var, mean), A_mean = A, B_mean = B) ## `summarise()` regrouping output by &#39;id&#39; (override with `.groups` argument) The pipe lets you “pipe” the result of each function into the next function, allowing you to put your code in a logical order without creating too many extra objects. # calculate mean of A and B variables for each participant data &lt;- tibble( id = 1:10, A1 = rnorm(10, 0), A2 = rnorm(10, 1), B1 = rnorm(10, 2), B2 = rnorm(10,3) ) %&gt;% gather(variable, value, A1:B2) %&gt;% separate(variable, c(&quot;var&quot;, &quot;var_n&quot;), sep=1) %&gt;% group_by(id, var) %&gt;% summarise(mean = mean(value)) %&gt;% spread(var, mean) %&gt;% rename(A_mean = A, B_mean = B) ## `summarise()` regrouping output by &#39;id&#39; (override with `.groups` argument) You can read this code from top to bottom as follows: Make a tibble called data with id of 1 to 10, A1 of 10 random numbers from a normal distribution, A2 of 10 random numbers from a normal distribution, B1 of 10 random numbers from a normal distribution, B2 of 10 random numbers from a normal distribution; and then Gather to create variable and value column from columns A_1 to B_2; and then Separate the column variable into 2 new columns called varand var_n, separate at character 1; and then Group by columns id and var; and then Summarise and new column called mean as the mean of the value column for each group; and then Spread to make new columns with the key names in var and values in mean; and then Rename to make columns called A_mean (old A) and B_mean (old B) You can make intermediate objects whenever you need to break up your code because it’s getting too complicated or you need to debug something. You can debug a pipe by highlighting from the beginning to just before the pipe you want to stop at. Try this by highlighting from data &lt;- to the end of the separate function and typing cmd-return. What does data look like now? Chain all the steps above using pipes. ocean &lt;- read_csv(&quot;https://psyteachr.github.io/msc-data-skills/data/personality.csv&quot;) %&gt;% gather(&quot;question&quot;, &quot;score&quot;, Op1:Ex9) %&gt;% separate(question, c(&quot;domain&quot;, &quot;qnumber&quot;), sep = 2) %&gt;% unite(&quot;domain_n&quot;, domain, qnumber, sep = &quot;_Q&quot;) %&gt;% spread(domain_n, score) ## Parsed with column specification: ## cols( ## .default = col_double(), ## date = col_date(format = &quot;&quot;) ## ) ## See spec(...) for full column specifications. 4.7 More Complex Example 4.7.1 Load Data {#data-infmort} Get data on infant mortality rates from the CSV file infmort.csv in the directory data. infmort &lt;- read_csv(&quot;data/infmort.csv&quot;) ## Parsed with column specification: ## cols( ## Country = col_character(), ## Year = col_double(), ## `Infant mortality rate (probability of dying between birth and age 1 per 1000 live births)` = col_character() ## ) glimpse(infmort) ## Rows: 5,044 ## Columns: 3 ## $ Country &lt;chr&gt; … ## $ Year &lt;dbl&gt; … ## $ `Infant mortality rate (probability of dying between birth and age 1 per 1000 live births)` &lt;chr&gt; … {#data-matmort} Get data on maternal mortality from from the excel file matmort.xls in the directory data matmort &lt;- read_xls(&quot;data/matmort.xls&quot;) glimpse(matmort) ## Rows: 181 ## Columns: 4 ## $ Country &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Albania&quot;, &quot;Algeria&quot;, &quot;Angola&quot;, &quot;Argentina&quot;, &quot;… ## $ `1990` &lt;chr&gt; &quot;1 340 [ 878 - 1 950]&quot;, &quot;71 [ 58 - 88]&quot;, &quot;216 [ 141 - 327]&quot;… ## $ `2000` &lt;chr&gt; &quot;1 100 [ 745 - 1 570]&quot;, &quot;43 [ 33 - 56]&quot;, &quot;170 [ 118 - 241]&quot;… ## $ `2015` &lt;chr&gt; &quot;396 [ 253 - 620]&quot;, &quot;29 [ 16 - 46]&quot;, &quot;140 [ 82 - 244]&quot;, &quot;4… Get data on country codes from https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv ccodes &lt;- read_csv(&quot;https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv&quot;) ## Parsed with column specification: ## cols( ## name = col_character(), ## `alpha-2` = col_character(), ## `alpha-3` = col_character(), ## `country-code` = col_character(), ## `iso_3166-2` = col_character(), ## region = col_character(), ## `sub-region` = col_character(), ## `intermediate-region` = col_character(), ## `region-code` = col_character(), ## `sub-region-code` = col_character(), ## `intermediate-region-code` = col_character() ## ) glimpse(ccodes) ## Rows: 249 ## Columns: 11 ## $ name &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Åland Islands&quot;, &quot;Albania&quot;,… ## $ `alpha-2` &lt;chr&gt; &quot;AF&quot;, &quot;AX&quot;, &quot;AL&quot;, &quot;DZ&quot;, &quot;AS&quot;, &quot;AD&quot;, &quot;AO&quot;, … ## $ `alpha-3` &lt;chr&gt; &quot;AFG&quot;, &quot;ALA&quot;, &quot;ALB&quot;, &quot;DZA&quot;, &quot;ASM&quot;, &quot;AND&quot;, … ## $ `country-code` &lt;chr&gt; &quot;004&quot;, &quot;248&quot;, &quot;008&quot;, &quot;012&quot;, &quot;016&quot;, &quot;020&quot;, … ## $ `iso_3166-2` &lt;chr&gt; &quot;ISO 3166-2:AF&quot;, &quot;ISO 3166-2:AX&quot;, &quot;ISO 316… ## $ region &lt;chr&gt; &quot;Asia&quot;, &quot;Europe&quot;, &quot;Europe&quot;, &quot;Africa&quot;, &quot;Oce… ## $ `sub-region` &lt;chr&gt; &quot;Southern Asia&quot;, &quot;Northern Europe&quot;, &quot;South… ## $ `intermediate-region` &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;Middle Africa&quot;, &quot;… ## $ `region-code` &lt;chr&gt; &quot;142&quot;, &quot;150&quot;, &quot;150&quot;, &quot;002&quot;, &quot;009&quot;, &quot;150&quot;, … ## $ `sub-region-code` &lt;chr&gt; &quot;034&quot;, &quot;154&quot;, &quot;039&quot;, &quot;015&quot;, &quot;061&quot;, &quot;039&quot;, … ## $ `intermediate-region-code` &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;017&quot;, &quot;029&quot;, NA, … 4.7.2 Wide to Long matmort is in wide format, with a separate column for each year. Change it to long format, with a row for each County/Year observation. This example is complicated because the column names to gather are numbers. If the column names are non-standard (e.g., have spaces, start with numbers, or have special characters), you can enclose them in backticks (`) like the example below. matmort_long &lt;- matmort %&gt;% gather(&quot;Year&quot;, &quot;stats&quot;, `1990`:`2015`) glimpse(matmort_long) ## Rows: 543 ## Columns: 3 ## $ Country &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Albania&quot;, &quot;Algeria&quot;, &quot;Angola&quot;, &quot;Argentina&quot;, &quot;… ## $ Year &lt;chr&gt; &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990… ## $ stats &lt;chr&gt; &quot;1 340 [ 878 - 1 950]&quot;, &quot;71 [ 58 - 88]&quot;, &quot;216 [ 141 - 327]&quot;… 4.7.3 One Piece of Data per Column The data in the stats column is in an unusual format with some sort of confidence interval in brackets and lots of extra spaces. We don’t need any of the spaces, so first we’ll remove them with mutate. The separate function will separate your data on anything that is not a number or letter, so try it first without specifying the sep argument. The into argument is a list of the new column names. matmort_split &lt;- matmort_long %&gt;% mutate(stats = gsub(&quot; &quot;, &quot;&quot;, stats)) %&gt;% separate(stats, c(&quot;rate&quot;, &quot;ci_low&quot;, &quot;ci_hi&quot;)) ## Warning: Expected 3 pieces. Additional pieces discarded in 543 rows [1, 2, 3, 4, ## 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, ...]. glimpse(matmort_split) ## Rows: 543 ## Columns: 5 ## $ Country &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Albania&quot;, &quot;Algeria&quot;, &quot;Angola&quot;, &quot;Argentina&quot;, &quot;… ## $ Year &lt;chr&gt; &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990… ## $ rate &lt;chr&gt; &quot;1340&quot;, &quot;71&quot;, &quot;216&quot;, &quot;1160&quot;, &quot;72&quot;, &quot;58&quot;, &quot;8&quot;, &quot;8&quot;, &quot;64&quot;, &quot;46&quot;… ## $ ci_low &lt;chr&gt; &quot;878&quot;, &quot;58&quot;, &quot;141&quot;, &quot;627&quot;, &quot;64&quot;, &quot;51&quot;, &quot;7&quot;, &quot;7&quot;, &quot;56&quot;, &quot;34&quot;, … ## $ ci_hi &lt;chr&gt; &quot;1950&quot;, &quot;88&quot;, &quot;327&quot;, &quot;2020&quot;, &quot;80&quot;, &quot;65&quot;, &quot;9&quot;, &quot;10&quot;, &quot;74&quot;, &quot;61… The gsub(pattern, replacement, x) function is a flexible way to do search and replace. The example above replaces all occurances of the pattern \" \" (a space), with the replacement \"\" (nothing), in the string x (the stats column). Use sub() instead if you only want to replace the first occurance of a pattern. We only used a simple pattern here, but you can use more complicated regex patterns to replace, for example, all even numbers (e.g., gsub(“[:02468:]”, \"“,”id = 123456“)) or all occurances of the word colour in US or UK spelling (e.g., gsub(”colo(u)?r“,”**“,”replace color, colour, or colours, but not collors\")). 4.7.3.1 Handle spare columns with extra The previous example should have given you an error warning about “Too many values at 543 locations”. This is because separate splits the column at the brackets and dashes, so the text 100[90-110] would split into four values c(“100”, “90”, “110”, \"“), but we only specified 3 new columns. The fourth value is always empty (just the part after the last bracket), so we are happy to drop it, but separate generates a warning so you don’t do that accidentally. You can turn off the warning by adding the extra argument and setting it to “drop”. Look at the help for ??tidyr::separate to see what the other options do. matmort_split &lt;- matmort_long %&gt;% mutate(stats = gsub(&quot; &quot;, &quot;&quot;, stats)) %&gt;% separate(stats, c(&quot;rate&quot;, &quot;ci_low&quot;, &quot;ci_hi&quot;), extra = &quot;drop&quot;) glimpse(matmort_split) ## Rows: 543 ## Columns: 5 ## $ Country &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Albania&quot;, &quot;Algeria&quot;, &quot;Angola&quot;, &quot;Argentina&quot;, &quot;… ## $ Year &lt;chr&gt; &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990… ## $ rate &lt;chr&gt; &quot;1340&quot;, &quot;71&quot;, &quot;216&quot;, &quot;1160&quot;, &quot;72&quot;, &quot;58&quot;, &quot;8&quot;, &quot;8&quot;, &quot;64&quot;, &quot;46&quot;… ## $ ci_low &lt;chr&gt; &quot;878&quot;, &quot;58&quot;, &quot;141&quot;, &quot;627&quot;, &quot;64&quot;, &quot;51&quot;, &quot;7&quot;, &quot;7&quot;, &quot;56&quot;, &quot;34&quot;, … ## $ ci_hi &lt;chr&gt; &quot;1950&quot;, &quot;88&quot;, &quot;327&quot;, &quot;2020&quot;, &quot;80&quot;, &quot;65&quot;, &quot;9&quot;, &quot;10&quot;, &quot;74&quot;, &quot;61… 4.7.3.2 Set delimiters with sep Now do the same with infmort. It’s already in long format, so you don’t need to use gather, but the third column has a crazy long name, so we can just refer to it by its column number (3). infmort_split &lt;- infmort %&gt;% separate(3, c(&quot;rate&quot;, &quot;ci_low&quot;, &quot;ci_hi&quot;), extra = &quot;drop&quot;) glimpse(infmort_split) ## Rows: 5,044 ## Columns: 5 ## $ Country &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;… ## $ Year &lt;dbl&gt; 2015, 2014, 2013, 2012, 2011, 2010, 2009, 2008, 2007, 2006, 2… ## $ rate &lt;chr&gt; &quot;66&quot;, &quot;68&quot;, &quot;69&quot;, &quot;71&quot;, &quot;73&quot;, &quot;75&quot;, &quot;76&quot;, &quot;78&quot;, &quot;80&quot;, &quot;82&quot;, &quot;… ## $ ci_low &lt;chr&gt; &quot;3&quot;, &quot;1&quot;, &quot;9&quot;, &quot;7&quot;, &quot;4&quot;, &quot;1&quot;, &quot;8&quot;, &quot;6&quot;, &quot;4&quot;, &quot;3&quot;, &quot;4&quot;, &quot;7&quot;, &quot;… ## $ ci_hi &lt;chr&gt; &quot;52&quot;, &quot;55&quot;, &quot;58&quot;, &quot;61&quot;, &quot;64&quot;, &quot;66&quot;, &quot;69&quot;, &quot;71&quot;, &quot;73&quot;, &quot;75&quot;, &quot;… Wait, that didn’t work at all! It split the column on spaces, brackets, and full stops. We just want to split on the spaces, brackets and dashes. So we need to manually set sep to what the delimiters are. Also, once there are more than a few arguments specified for a function, it’s easier to read them if you put one argument on each line. {#regex} You can use regular expressions to separate complex columns. Here, we want to separate on dashes and brackets. You can separate on a list of delimiters by putting them in parentheses, separated by “|”. It’s a little more complicated because brackets have a special meaning in regex, so you need to “escape” the left one with two backslashes “\\”. infmort_split &lt;- infmort %&gt;% separate( col = 3, into = c(&quot;rate&quot;, &quot;ci_low&quot;, &quot;ci_hi&quot;), extra = &quot;drop&quot;, sep = &quot;(\\\\[|-|])&quot; ) glimpse(infmort_split) ## Rows: 5,044 ## Columns: 5 ## $ Country &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;… ## $ Year &lt;dbl&gt; 2015, 2014, 2013, 2012, 2011, 2010, 2009, 2008, 2007, 2006, 2… ## $ rate &lt;chr&gt; &quot;66.3 &quot;, &quot;68.1 &quot;, &quot;69.9 &quot;, &quot;71.7 &quot;, &quot;73.4 &quot;, &quot;75.1 &quot;, &quot;76.8 &quot;… ## $ ci_low &lt;chr&gt; &quot;52.7&quot;, &quot;55.7&quot;, &quot;58.7&quot;, &quot;61.6&quot;, &quot;64.4&quot;, &quot;66.9&quot;, &quot;69.0&quot;, &quot;71.2… ## $ ci_hi &lt;chr&gt; &quot;83.9&quot;, &quot;83.6&quot;, &quot;83.5&quot;, &quot;83.7&quot;, &quot;84.2&quot;, &quot;85.1&quot;, &quot;86.1&quot;, &quot;87.3… 4.7.3.3 Fix data types with convert That’s better. Notice the next to Year, rate, ci_low and ci_hi. That means these columns hold characters (like words), not numbers or integers. This can cause problems when you try to do thigs like average the numbers (you can’t average words), so we can fix it by adding the argument convert and setting it to TRUE. infmort_split &lt;- infmort %&gt;% separate(3, c(&quot;rate&quot;, &quot;ci_low&quot;, &quot;ci_hi&quot;), extra = &quot;drop&quot;, sep = &quot;(\\\\[|-|])&quot;, convert = TRUE) glimpse(infmort_split) ## Rows: 5,044 ## Columns: 5 ## $ Country &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;… ## $ Year &lt;dbl&gt; 2015, 2014, 2013, 2012, 2011, 2010, 2009, 2008, 2007, 2006, 2… ## $ rate &lt;dbl&gt; 66.3, 68.1, 69.9, 71.7, 73.4, 75.1, 76.8, 78.6, 80.4, 82.3, 8… ## $ ci_low &lt;dbl&gt; 52.7, 55.7, 58.7, 61.6, 64.4, 66.9, 69.0, 71.2, 73.4, 75.5, 7… ## $ ci_hi &lt;dbl&gt; 83.9, 83.6, 83.5, 83.7, 84.2, 85.1, 86.1, 87.3, 88.9, 90.7, 9… Do the same for matmort. matmort_split &lt;- matmort_long %&gt;% mutate(stats = gsub(&quot; &quot;, &quot;&quot;, stats)) %&gt;% separate(stats, c(&quot;rate&quot;, &quot;ci_low&quot;, &quot;ci_hi&quot;), extra = &quot;drop&quot;, convert = TRUE) glimpse(matmort_split) ## Rows: 543 ## Columns: 5 ## $ Country &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Albania&quot;, &quot;Algeria&quot;, &quot;Angola&quot;, &quot;Argentina&quot;, &quot;… ## $ Year &lt;chr&gt; &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990… ## $ rate &lt;int&gt; 1340, 71, 216, 1160, 72, 58, 8, 8, 64, 46, 26, 569, 58, 33, 9… ## $ ci_low &lt;int&gt; 878, 58, 141, 627, 64, 51, 7, 7, 56, 34, 20, 446, 47, 28, 7, … ## $ ci_hi &lt;int&gt; 1950, 88, 327, 2020, 80, 65, 9, 10, 74, 61, 33, 715, 72, 38, … 4.7.4 All in one step We can chain all the steps above together, since we don’t need those intermediate dataframes. infmort &lt;- read_csv(&quot;data/infmort.csv&quot;) %&gt;% separate( 3, c(&quot;rate&quot;, &quot;ci_low&quot;, &quot;ci_hi&quot;), extra = &quot;drop&quot;, sep = &quot;(\\\\[|-|])&quot;, convert = TRUE ) ## Parsed with column specification: ## cols( ## Country = col_character(), ## Year = col_double(), ## `Infant mortality rate (probability of dying between birth and age 1 per 1000 live births)` = col_character() ## ) matmort &lt;- read_xls(&quot;data/matmort.xls&quot;) %&gt;% gather(&quot;Year&quot;, &quot;stats&quot;, `1990`:`2015`) %&gt;% mutate(stats = gsub(&quot; &quot;, &quot;&quot;, stats)) %&gt;% separate( stats, c(&quot;rate&quot;, &quot;ci_low&quot;, &quot;ci_hi&quot;), extra = &quot;drop&quot;, convert = TRUE ) glimpse(matmort) glimpse(infmort) ## Rows: 543 ## Columns: 5 ## $ Country &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Albania&quot;, &quot;Algeria&quot;, &quot;Angola&quot;, &quot;Argentina&quot;, &quot;… ## $ Year &lt;chr&gt; &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990… ## $ rate &lt;int&gt; 1340, 71, 216, 1160, 72, 58, 8, 8, 64, 46, 26, 569, 58, 33, 9… ## $ ci_low &lt;int&gt; 878, 58, 141, 627, 64, 51, 7, 7, 56, 34, 20, 446, 47, 28, 7, … ## $ ci_hi &lt;int&gt; 1950, 88, 327, 2020, 80, 65, 9, 10, 74, 61, 33, 715, 72, 38, … ## Rows: 5,044 ## Columns: 5 ## $ Country &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;… ## $ Year &lt;dbl&gt; 2015, 2014, 2013, 2012, 2011, 2010, 2009, 2008, 2007, 2006, 2… ## $ rate &lt;dbl&gt; 66.3, 68.1, 69.9, 71.7, 73.4, 75.1, 76.8, 78.6, 80.4, 82.3, 8… ## $ ci_low &lt;dbl&gt; 52.7, 55.7, 58.7, 61.6, 64.4, 66.9, 69.0, 71.2, 73.4, 75.5, 7… ## $ ci_hi &lt;dbl&gt; 83.9, 83.6, 83.5, 83.7, 84.2, 85.1, 86.1, 87.3, 88.9, 90.7, 9… 4.7.5 Columns by Year Spread out the infant mortality rate by year. infmort_wide &lt;- infmort %&gt;% spread(Year, rate) glimpse(infmort_wide) ## Rows: 4,934 ## Columns: 29 ## $ Country &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;… ## $ ci_low &lt;dbl&gt; 52.7, 55.7, 58.7, 61.6, 64.4, 66.9, 69.0, 71.2, 73.4, 75.5, 7… ## $ ci_hi &lt;dbl&gt; 83.9, 83.6, 83.5, 83.7, 84.2, 85.1, 86.1, 87.3, 88.9, 90.7, 9… ## $ `1990` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ `1991` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ `1992` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ `1993` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ `1994` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ `1995` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ `1996` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ `1997` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ `1998` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ `1999` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ `2000` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 9… ## $ `2001` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 93.4,… ## $ `2002` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 91.2, NA,… ## $ `2003` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 89, NA, NA, N… ## $ `2004` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 86.7, NA, NA, NA,… ## $ `2005` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 84.4, NA, NA, NA, NA,… ## $ `2006` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, 82.3, NA, NA, NA, NA, NA,… ## $ `2007` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 80.4, NA, NA, NA, NA, NA, NA,… ## $ `2008` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, 78.6, NA, NA, NA, NA, NA, NA, NA,… ## $ `2009` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 76.8, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ `2010` &lt;dbl&gt; NA, NA, NA, NA, NA, 75.1, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ `2011` &lt;dbl&gt; NA, NA, NA, NA, 73.4, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ `2012` &lt;dbl&gt; NA, NA, NA, 71.7, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ `2013` &lt;dbl&gt; NA, NA, 69.9, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ `2014` &lt;dbl&gt; NA, 68.1, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ `2015` &lt;dbl&gt; 66.3, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… Nope, that didn’t work at all, but it’s a really common mistake when spreading data. This is because spread matches on all the remaining columns, so Afghanistan with ci_low of 52.7 is treated as a different observation than Afghanistan with ci_low of 55.7. We can fix this by merging the rate, ci_low and ci_hi columns back together. 4.7.6 Merge Columns Merge the rate and confidence intervals into one column. infmort_united &lt;- infmort %&gt;% unite(rate_ci, rate, ci_low, ci_hi) glimpse(infmort_united) ## Rows: 5,044 ## Columns: 3 ## $ Country &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;… ## $ Year &lt;dbl&gt; 2015, 2014, 2013, 2012, 2011, 2010, 2009, 2008, 2007, 2006, 2… ## $ rate_ci &lt;chr&gt; &quot;66.3_52.7_83.9&quot;, &quot;68.1_55.7_83.6&quot;, &quot;69.9_58.7_83.5&quot;, &quot;71.7_6… 4.7.6.1 Control separation with sep unite() separates merged names with an underscore by default. Set the sep argument if you want to change that. infmort_united &lt;- infmort %&gt;% unite(rate_ci, rate, ci_low, ci_hi, sep = &quot;, &quot;) glimpse(infmort_united) ## Rows: 5,044 ## Columns: 3 ## $ Country &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;… ## $ Year &lt;dbl&gt; 2015, 2014, 2013, 2012, 2011, 2010, 2009, 2008, 2007, 2006, 2… ## $ rate_ci &lt;chr&gt; &quot;66.3, 52.7, 83.9&quot;, &quot;68.1, 55.7, 83.6&quot;, &quot;69.9, 58.7, 83.5&quot;, &quot;… What if you want to put it back into the format “rate [ci_low - ci_hi]”? Then, mutate and paste are a better choice than unite, but you have to get rid of the rate, ci_low and ci_hi columns with select. You’ll learn more about these function in the Data Manipulation lesson. infmort_united &lt;- infmort %&gt;% mutate(rate_ci = paste0(rate, &quot; [&quot;, ci_low, &quot; - &quot;, ci_hi, &quot;]&quot;)) glimpse(infmort_united) ## Rows: 5,044 ## Columns: 6 ## $ Country &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;… ## $ Year &lt;dbl&gt; 2015, 2014, 2013, 2012, 2011, 2010, 2009, 2008, 2007, 2006, 2… ## $ rate &lt;dbl&gt; 66.3, 68.1, 69.9, 71.7, 73.4, 75.1, 76.8, 78.6, 80.4, 82.3, 8… ## $ ci_low &lt;dbl&gt; 52.7, 55.7, 58.7, 61.6, 64.4, 66.9, 69.0, 71.2, 73.4, 75.5, 7… ## $ ci_hi &lt;dbl&gt; 83.9, 83.6, 83.5, 83.7, 84.2, 85.1, 86.1, 87.3, 88.9, 90.7, 9… ## $ rate_ci &lt;chr&gt; &quot;66.3 [52.7 - 83.9]&quot;, &quot;68.1 [55.7 - 83.6]&quot;, &quot;69.9 [58.7 - 83.… Now let’s try spreading on year again. Notice here we’re uniting columns rate:ci_hi, instead of rate, ci_low, ci_hi. The colon just says to select all the columns between the first and last named ones. Check the help documentation for ??tidyr::unite and ??tidyr::select to see other ways to select columns. infmort_wide &lt;- infmort %&gt;% unite(rate_ci, rate:ci_hi, sep = &quot;, &quot;) %&gt;% spread(Year, rate_ci) glimpse(infmort_wide) ## Rows: 194 ## Columns: 27 ## $ Country &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Albania&quot;, &quot;Algeria&quot;, &quot;Andorra&quot;, &quot;Angola&quot;, &quot;An… ## $ `1990` &lt;chr&gt; &quot;122.5, 111.6, 135.5&quot;, &quot;35.1, 31.3, 39.2&quot;, &quot;39.7, 37.1, 42.3&quot;… ## $ `1991` &lt;chr&gt; &quot;118.3, 108, 129.9&quot;, &quot;33.7, 30.2, 37.6&quot;, &quot;38.8, 36.1, 41.6&quot;, … ## $ `1992` &lt;chr&gt; &quot;114.4, 104.6, 125.2&quot;, &quot;32.5, 29.2, 36.2&quot;, &quot;38.1, 35.4, 41&quot;, … ## $ `1993` &lt;chr&gt; &quot;110.9, 101.4, 120.9&quot;, &quot;31.4, 28.2, 34.9&quot;, &quot;37.5, 34.9, 40.3&quot;… ## $ `1994` &lt;chr&gt; &quot;107.7, 98.6, 117.2&quot;, &quot;30.3, 27.1, 33.8&quot;, &quot;36.9, 34.6, 39.4&quot;,… ## $ `1995` &lt;chr&gt; &quot;105, 96.2, 114.1&quot;, &quot;29.1, 26, 32.7&quot;, &quot;36.3, 34.2, 38.4&quot;, &quot;5.… ## $ `1996` &lt;chr&gt; &quot;102.7, 94.5, 111.3&quot;, &quot;27.9, 24.8, 31.5&quot;, &quot;35.7, 34, 37.4&quot;, &quot;… ## $ `1997` &lt;chr&gt; &quot;100.7, 92.9, 109.1&quot;, &quot;26.8, 23.6, 30.4&quot;, &quot;35.1, 33.8, 36.5&quot;,… ## $ `1998` &lt;chr&gt; &quot;98.9, 91.4, 107.2&quot;, &quot;25.5, 22.4, 29.2&quot;, &quot;34.7, 33.7, 35.8&quot;, … ## $ `1999` &lt;chr&gt; &quot;97.2, 89.9, 105.4&quot;, &quot;24.4, 21.2, 28.1&quot;, &quot;34.4, 33.5, 35.2&quot;, … ## $ `2000` &lt;chr&gt; &quot;95.4, 88.2, 103.6&quot;, &quot;23.2, 20, 27&quot;, &quot;33.9, 33.2, 34.7&quot;, &quot;3.9… ## $ `2001` &lt;chr&gt; &quot;93.4, 86.3, 101.6&quot;, &quot;22.1, 18.8, 26&quot;, &quot;33.3, 32.7, 34&quot;, &quot;3.7… ## $ `2002` &lt;chr&gt; &quot;91.2, 84.3, 99.3&quot;, &quot;21, 17.6, 25.1&quot;, &quot;32.4, 31.8, 33&quot;, &quot;3.5,… ## $ `2003` &lt;chr&gt; &quot;89, 82.1, 97&quot;, &quot;20, 16.5, 24.3&quot;, &quot;31.3, 30.7, 31.9&quot;, &quot;3.3, 2… ## $ `2004` &lt;chr&gt; &quot;86.7, 79.9, 94.8&quot;, &quot;19.1, 15.4, 23.8&quot;, &quot;30.1, 29.5, 30.6&quot;, &quot;… ## $ `2005` &lt;chr&gt; &quot;84.4, 77.7, 92.6&quot;, &quot;18.3, 14.2, 23.4&quot;, &quot;28.8, 28.3, 29.4&quot;, &quot;… ## $ `2006` &lt;chr&gt; &quot;82.3, 75.5, 90.7&quot;, &quot;17.4, 13.2, 23.1&quot;, &quot;27.6, 27, 28.1&quot;, &quot;2.… ## $ `2007` &lt;chr&gt; &quot;80.4, 73.4, 88.9&quot;, &quot;16.7, 12.1, 22.9&quot;, &quot;26.4, 25.9, 26.9&quot;, &quot;… ## $ `2008` &lt;chr&gt; &quot;78.6, 71.2, 87.3&quot;, &quot;16, 11.2, 22.7&quot;, &quot;25.3, 24.8, 25.7&quot;, &quot;2.… ## $ `2009` &lt;chr&gt; &quot;76.8, 69, 86.1&quot;, &quot;15.4, 10.5, 22.6&quot;, &quot;24.3, 23.8, 24.7&quot;, &quot;2.… ## $ `2010` &lt;chr&gt; &quot;75.1, 66.9, 85.1&quot;, &quot;14.8, 9.8, 22.4&quot;, &quot;23.5, 23, 23.9&quot;, &quot;2.5… ## $ `2011` &lt;chr&gt; &quot;73.4, 64.4, 84.2&quot;, &quot;14.3, 9.1, 22.3&quot;, &quot;22.8, 22.4, 23.3&quot;, &quot;2… ## $ `2012` &lt;chr&gt; &quot;71.7, 61.6, 83.7&quot;, &quot;13.8, 8.5, 22.2&quot;, &quot;22.4, 22, 22.9&quot;, &quot;2.3… ## $ `2013` &lt;chr&gt; &quot;69.9, 58.7, 83.5&quot;, &quot;13.3, 7.9, 22.1&quot;, &quot;22.1, 21.7, 22.7&quot;, &quot;2… ## $ `2014` &lt;chr&gt; &quot;68.1, 55.7, 83.6&quot;, &quot;12.9, 7.5, 22.1&quot;, &quot;22, 21.3, 22.7&quot;, &quot;2.1… ## $ `2015` &lt;chr&gt; &quot;66.3, 52.7, 83.9&quot;, &quot;12.5, 7, 22.2&quot;, &quot;21.9, 20.8, 23&quot;, &quot;2.1, … 4.8 Quiz For questions 1-4, choose how to turn table A into table B. There may be more than one right answer. gather(A, \"time\", \"score\", morning:night) gather(A, \"pet\", \"time\", morning, noon, night) gather(A, \"time\", morning, noon, night) gather(A, \"time\", \"score\", morning, noon, night) gather(A, \"pet\", \"score\", dog:cat) Table 4.1: Table A (source) id pet morning noon night 1.1 S1 dog -1.4964122 -0.6278186 0.6267951 1.2 S2 dog -1.0112066 -0.5090254 1.0724757 1.3 S3 cat 0.6669006 0.4264954 -0.0924112 1.4 S4 cat 1.2740238 -0.2021898 0.7707817 Table 4.1: Table B (goal) id pet time score S1 dog morning -1.4964122 S2 dog morning -1.0112066 S3 cat morning 0.6669006 S4 cat morning 1.2740238 S1 dog noon -0.6278186 S2 dog noon -0.5090254 S3 cat noon 0.4264954 S4 cat noon -0.2021898 S1 dog night 0.6267951 S2 dog night 1.0724757 S3 cat night -0.0924112 S4 cat night 0.7707817 separate(A, \"pet\", \"number\") separate(A, \"pet\", \"number\", pet_number) separate(A, pet_number, c(\"pet\", \"number\"), sep = \".\") separate(A, pet_number, c(\"pet\", \"number\")) separate(A, pet_number, c(\"pet\", \"number\"), sep = \"\\\\.\") separate(A, pet_number, \"pet\", \"number\") separate(A, c(\"pet\", \"number\")) Table 4.2: Table A (source) id pet_number score 1.1 S1 dog.1 -0.3984966 1.2 S2 cat.1 0.9157141 1.3 S3 dog.2 0.7292385 1.4 S4 cat.2 -0.4399254 Table 4.2: Table B (goal) id pet number score 1.1 S1 dog 1 -0.3984966 1.2 S2 cat 1 0.9157141 1.3 S3 dog 2 0.7292385 1.4 S4 cat 2 -0.4399254 unite(A, pet_pref, pet, pref, remove = FALSE) unite(A, pet_pref, pet, pref) unite(A, \"pet_pref\", pet, pref, remove = FALSE) unite(A, pet, pref, pet_pref, remove = FALSE) unite(A, \"pet_pref\", pet, pref) unite(A, pet, pref, \"pet_pref\", remove = FALSE) Table 4.3: Table A (source) id pet pref score 1.1 S1 dog lover 0.0933798 1.2 S2 dog hater -0.6841774 1.3 S3 cat lover 1.3188061 1.4 S4 cat hater 0.9196899 Table 4.3: Table B (goal) id pet_pref pet pref score 1.1 S1 dog_lover dog lover 0.0933798 1.2 S2 dog_hater dog hater -0.6841774 1.3 S3 cat_lover cat lover 1.3188061 1.4 S4 cat_hater cat hater 0.9196899 spread(A, time, score, sep = \"_\") spread(A, 3, 4, sep = \"_\") spread(A, time, score, fill = 0) spread(A, time, score, drop = FALSE) spread(A, time, score) Table 4.4: Table A (source) id pet time score 1.S1.1 S1 dog morning 0.5455656 1.S2.1 S2 dog morning -0.4298306 1.S3.1 S3 cat morning -1.9146288 1.S4.1 S4 cat morning -1.4403162 1.S1.2 S1 dog night 1.4924050 1.S2.2 S2 dog night 0.4096266 1.S3.2 S3 cat night -0.9108425 Table 4.4: Table B (goal) id pet time_morning time_night S1 dog 0.5455656 1.4924050 S2 dog -0.4298306 0.4096266 S3 cat -1.9146288 -0.9108425 S4 cat -1.4403162 NA Put the built-in dataset iris into the following format. Species feature dimension value setosa Sepal Length 5.1 setosa Sepal Length 4.9 setosa Sepal Length 4.7 setosa Sepal Length 4.6 setosa Sepal Length 5.0 setosa Sepal Length 5.4 Solution iris %&gt;% gather(var, value, Sepal.Length:Petal.Width) %&gt;% separate(var, into = c(&quot;feature&quot;, &quot;dimension&quot;)) Re-write the following code using pipes. Assign the resulting data table to a variable called data. # make a data table with 5 subjects providing 2 scores (A and B) in each of 2 conditions data_original &lt;- tibble( id = c(1:5, 1:5), condition = rep(1:2, each = 5), A = rnorm(10), B = rnorm(10) ) # gather columns A and B into &quot;score_type&quot; and &quot;score&quot; columns data_gathered &lt;- gather(data_original, score_type, score, A:B) # unite the score_type and condition columns into a column called &quot;cell&quot; data_united &lt;- unite(data_gathered, cell, score_type, condition, sep = &quot;&quot;) # spread the score column into cells data_spread &lt;- spread(data_united, cell, score) Solution data &lt;- tibble( id = c(1:5, 1:5), condition = rep(1:2, each = 5), A = rnorm(10), B = rnorm(10) ) %&gt;% gather(score_type, score, A:B) %&gt;% unite(cell, score_type, condition, sep = &quot;&quot;) %&gt;% spread(cell, score) 4.9 Exercises Download the exercises. See the answers only after you’ve attempted all the questions. # run this to access the exercise dataskills::exercise(4) # run this to access the answers dataskills::exercise(4, answers = TRUE) "],
["dplyr.html", "Chapter 5 Data Wrangling 5.1 Learning Objectives 5.2 Resources 5.3 Setup 5.4 The disgust dataset 5.5 Six main dplyr verbs 5.6 Additional dplyr one-table verbs 5.7 Window functions 5.8 Exercises", " Chapter 5 Data Wrangling 5.1 Learning Objectives 5.1.1 Basic Be able to use the 6 main dplyr one-table verbs: select() filter() arrange() mutate() summarise() group_by() 5.1.2 Intermediate Also know these additional one-table verbs: rename() distinct() count() slice() pull() 5.1.3 Advanced Fine control of select() operations Use window functions 5.2 Resources Chapter 5: Data Transformation in R for Data Science Data transformation cheat sheet Lecture slides on dplyr one-table verbs Chapter 16: Date and times in R for Data Science 5.3 Setup You’ll need the following packages. # libraries needed for these examples library(tidyverse) library(lubridate) set.seed(8675309) # makes sure random numbers are reproducible 5.4 The disgust dataset These examples will use data from disgust.csv, which contains data from the Three Domain Disgust Scale. Each participant is identified by a unique user_id and each questionnaire completion has a unique id. disgust &lt;- read_csv(&quot;https://psyteachr.github.io/msc-data-skills/data/disgust.csv&quot;) Questionnaire Instructions: The following items describe a variety of concepts. Please rate how disgusting you find the concepts described in the items, where 0 means that you do not find the concept disgusting at all, and 6 means that you find the concept extremely disgusting. colname question moral1 | Shop Shoplifting a candy bar from a convenience store moral2 | Steal Stealing from a neighbor moral3 | A stu A student cheating to get good grades moral4 | Decei Deceiving a friend moral5 | Forgi Forging someone’s signature on a legal document moral6 | Cutti Cutting to the front of a line to purchase the last few tickets to a show moral7 | Inten Intentionally lying during a business transaction sexual1 | Hear Hearing two strangers having sex sexual2 | Perf Performing oral sex sexual3 | Watc Watching a pornographic video sexual4 | Find Finding out that someone you don’t like has sexual fantasies about you sexual5 | Brin Bringing someone you just met back to your room to have sex sexual6 | A st A stranger of the opposite sex intentionally rubbing your thigh in an elevator sexual7 | Havi Having anal sex with someone of the opposite sex pathogen1 | St Stepping on dog poop pathogen2 | Si Sitting next to someone who has red sores on their arm pathogen3 | Sh Shaking hands with a stranger who has sweaty palms pathogen4 | Se Seeing some mold on old leftovers in your refrigerator pathogen5 | St Standing close to a person who has body odor pathogen6 | Se Seeing a cockroach run across the floor pathogen7 | Ac Accidentally touching a person’s bloody cut 5.5 Six main dplyr verbs Most of the data wrangling you’ll want to do with psychological data will involve the tidyr verbs you learned in Chapter 3 and the six main dplyr verbs: select, filter, arrange, mutate, summarise, and group_by. 5.5.1 select() Select columns by name or number. You can select each column individually, separated by commas (e.g., col1, col2). You can also select all columns between two columns by separating them with a colon (e.g., start_col:end_col). moral &lt;- disgust %&gt;% select(user_id, moral1:moral7) names(moral) ## [1] &quot;user_id&quot; &quot;moral1&quot; &quot;moral2&quot; &quot;moral3&quot; &quot;moral4&quot; &quot;moral5&quot; &quot;moral6&quot; ## [8] &quot;moral7&quot; You can select columns by number, which is useful when the column names are long or complicated. sexual &lt;- disgust %&gt;% select(2, 11:17) names(sexual) ## [1] &quot;user_id&quot; &quot;sexual1&quot; &quot;sexual2&quot; &quot;sexual3&quot; &quot;sexual4&quot; &quot;sexual5&quot; &quot;sexual6&quot; ## [8] &quot;sexual7&quot; You can use a minus symbol to unselect columns, leaving all of the other columns. If you want to exclude a span of columns, put parentheses around the span first (e.g., -(moral1:moral7), not -moral1:moral7). pathogen &lt;- disgust %&gt;% select(-id, -date, -(moral1:sexual7)) names(pathogen) ## [1] &quot;user_id&quot; &quot;pathogen1&quot; &quot;pathogen2&quot; &quot;pathogen3&quot; &quot;pathogen4&quot; &quot;pathogen5&quot; ## [7] &quot;pathogen6&quot; &quot;pathogen7&quot; You can select columns based on criteria about the column names.{#select_helpers} 5.5.1.1 starts_with() Select columns that start with a character string. u &lt;- disgust %&gt;% select(starts_with(&quot;u&quot;)) names(u) ## [1] &quot;user_id&quot; 5.5.1.2 ends_with() Select columns that end with a character string. firstq &lt;- disgust %&gt;% select(ends_with(&quot;1&quot;)) names(firstq) ## [1] &quot;moral1&quot; &quot;sexual1&quot; &quot;pathogen1&quot; 5.5.1.3 contains() Select columns that contain a character string. pathogen &lt;- disgust %&gt;% select(contains(&quot;pathogen&quot;)) names(pathogen) ## [1] &quot;pathogen1&quot; &quot;pathogen2&quot; &quot;pathogen3&quot; &quot;pathogen4&quot; &quot;pathogen5&quot; &quot;pathogen6&quot; ## [7] &quot;pathogen7&quot; 5.5.1.4 num_range() Select columns with a name that matches the pattern prefix. moral2_4 &lt;- disgust %&gt;% select(num_range(&quot;moral&quot;, 2:4)) names(moral2_4) ## [1] &quot;moral2&quot; &quot;moral3&quot; &quot;moral4&quot; Use width to set the number of digits with leading zeros. For example, num_range(‘var_’, 8:10, width=2) selects columns var_08, var_09, and var_10. 5.5.2 filter() Select rows by matching column criteria. Select all rows where the user_id is 1 (that’s Lisa). disgust %&gt;% filter(user_id == 1) ## # A tibble: 1 x 24 ## id user_id date moral1 moral2 moral3 moral4 moral5 moral6 moral7 ## &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 2008-07-10 2 2 1 2 1 1 1 ## # … with 14 more variables: sexual1 &lt;dbl&gt;, sexual2 &lt;dbl&gt;, sexual3 &lt;dbl&gt;, ## # sexual4 &lt;dbl&gt;, sexual5 &lt;dbl&gt;, sexual6 &lt;dbl&gt;, sexual7 &lt;dbl&gt;, ## # pathogen1 &lt;dbl&gt;, pathogen2 &lt;dbl&gt;, pathogen3 &lt;dbl&gt;, pathogen4 &lt;dbl&gt;, ## # pathogen5 &lt;dbl&gt;, pathogen6 &lt;dbl&gt;, pathogen7 &lt;dbl&gt; Remember to use == and not = to check if two things are equivalent. A single = assigns the righthand value to the lefthand variable and (usually) evaluates to TRUE. You can select on multiple criteria by separating them with commas. amoral &lt;- disgust %&gt;% filter( moral1 == 0, moral2 == 0, moral3 == 0, moral4 == 0, moral5 == 0, moral6 == 0, moral7 == 0 ) You can use the symbols &amp;, |, and ! to mean “and”, “or”, and “not”. You can also use other operators to make equations. # everyone who chose either 0 or 7 for question moral1 moral_extremes &lt;- disgust %&gt;% filter(moral1 == 0 | moral1 == 7) # everyone who chose the same answer for all moral questions moral_consistent &lt;- disgust %&gt;% filter( moral2 == moral1 &amp; moral3 == moral1 &amp; moral4 == moral1 &amp; moral5 == moral1 &amp; moral6 == moral1 &amp; moral7 == moral1 ) # everyone who did not answer 7 for all 7 moral questions moral_no_ceiling &lt;- disgust %&gt;% filter(moral1+moral2+moral3+moral4+moral5+moral6+moral7 != 7*7) Sometimes you need to exclude some participant IDs for reasons that can’t be described in code. the %in% operator is useful here for testing if a column value is in a list. Surround the equation with parentheses and put ! in front to test that a value is not in the list. no_researchers &lt;- disgust %&gt;% filter(!(user_id %in% c(1,2))) 5.5.2.1 Dates You can use the lubridate package to work with dates. For example, you can use the year() function to return just the year from the date column and then select only data collected in 2010. disgust2010 &lt;- disgust %&gt;% filter(year(date) == 2010) Or select data from at least 5 years ago. You can use the range function to check the minimum and maxiumum dates in the resulting dataset. disgust_5ago &lt;- disgust %&gt;% filter(date &lt; today() - dyears(5)) range(disgust_5ago$date) ## [1] &quot;2008-07-10&quot; &quot;2015-08-18&quot; 5.5.3 arrange() Sort your dataset using arrange(). disgust_order &lt;- disgust %&gt;% arrange(id) head(disgust_order) ## # A tibble: 6 x 24 ## id user_id date moral1 moral2 moral3 moral4 moral5 moral6 moral7 ## &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 2008-07-10 2 2 1 2 1 1 1 ## 2 3 155324 2008-07-11 2 4 3 5 2 1 4 ## 3 4 155366 2008-07-12 6 6 6 3 6 6 6 ## 4 5 155370 2008-07-12 6 6 4 6 6 6 6 ## 5 6 155386 2008-07-12 2 4 0 4 0 0 0 ## 6 7 155409 2008-07-12 4 5 5 4 5 1 5 ## # … with 14 more variables: sexual1 &lt;dbl&gt;, sexual2 &lt;dbl&gt;, sexual3 &lt;dbl&gt;, ## # sexual4 &lt;dbl&gt;, sexual5 &lt;dbl&gt;, sexual6 &lt;dbl&gt;, sexual7 &lt;dbl&gt;, ## # pathogen1 &lt;dbl&gt;, pathogen2 &lt;dbl&gt;, pathogen3 &lt;dbl&gt;, pathogen4 &lt;dbl&gt;, ## # pathogen5 &lt;dbl&gt;, pathogen6 &lt;dbl&gt;, pathogen7 &lt;dbl&gt; Reverse the order using desc() disgust_order &lt;- disgust %&gt;% arrange(desc(id)) head(disgust_order) ## # A tibble: 6 x 24 ## id user_id date moral1 moral2 moral3 moral4 moral5 moral6 moral7 ## &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 39456 356866 2017-08-21 1 1 1 1 1 1 1 ## 2 39447 128727 2017-08-13 2 4 1 2 2 5 3 ## 3 39371 152955 2017-06-13 6 6 3 6 6 6 6 ## 4 39342 48303 2017-05-22 4 5 4 4 6 4 5 ## 5 39159 151633 2017-04-04 4 5 6 5 3 6 2 ## 6 38942 370464 2017-02-01 1 5 0 6 5 5 5 ## # … with 14 more variables: sexual1 &lt;dbl&gt;, sexual2 &lt;dbl&gt;, sexual3 &lt;dbl&gt;, ## # sexual4 &lt;dbl&gt;, sexual5 &lt;dbl&gt;, sexual6 &lt;dbl&gt;, sexual7 &lt;dbl&gt;, ## # pathogen1 &lt;dbl&gt;, pathogen2 &lt;dbl&gt;, pathogen3 &lt;dbl&gt;, pathogen4 &lt;dbl&gt;, ## # pathogen5 &lt;dbl&gt;, pathogen6 &lt;dbl&gt;, pathogen7 &lt;dbl&gt; 5.5.4 mutate() Add new columns. This is one of the most useful functions in the tidyverse. Refer to other columns by their names (unquoted). You can add more than one column, just separate the columns with a comma. Once you make a new column, you can use it in further column definitions e.g., total below). disgust_total &lt;- disgust %&gt;% mutate( pathogen = pathogen1 + pathogen2 + pathogen3 + pathogen4 + pathogen5 + pathogen6 + pathogen7, moral = moral1 + moral2 + moral3 + moral4 + moral5 + moral6 + moral7, sexual = sexual1 + sexual2 + sexual3 + sexual4 + sexual5 + sexual6 + sexual7, total = pathogen + moral + sexual, user_id = paste0(&quot;U&quot;, user_id) ) You can overwrite a column by giving a new column the same name as the old column. Make sure that you mean to do this and that you aren’t trying to use the old column value after you redefine it. 5.5.5 summarise() Create summary statistics for the dataset. Check the Data Wrangling Cheat Sheet or the Data Transformation Cheat Sheet for various summary functions. Some common ones are: mean(), sd(), n(), sum(), and quantile(). disgust_total %&gt;% summarise( n = n(), q25 = quantile(total, .25, na.rm = TRUE), q50 = quantile(total, .50, na.rm = TRUE), q75 = quantile(total, .75, na.rm = TRUE), avg_total = mean(total, na.rm = TRUE), sd_total = sd(total, na.rm = TRUE), min_total = min(total, na.rm = TRUE), max_total = max(total, na.rm = TRUE) ) ## # A tibble: 1 x 8 ## n q25 q50 q75 avg_total sd_total min_total max_total ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 20000 59 71 83 70.7 18.2 0 126 5.5.6 group_by() Create subsets of the data. You can use this to create summaries, like the mean value for all of your experimental groups. Here, we’ll use mutate to create a new column called year, group by year, and calculate the average scores. disgust_total %&gt;% mutate(year = year(date)) %&gt;% group_by(year) %&gt;% summarise( n = n(), avg_total = mean(total, na.rm = TRUE), sd_total = sd(total, na.rm = TRUE), min_total = min(total, na.rm = TRUE), max_total = max(total, na.rm = TRUE) ) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 10 x 6 ## year n avg_total sd_total min_total max_total ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2008 2578 70.3 18.5 0 126 ## 2 2009 2580 69.7 18.6 3 126 ## 3 2010 1514 70.6 18.9 6 126 ## 4 2011 6046 71.3 17.8 0 126 ## 5 2012 5938 70.4 18.4 0 126 ## 6 2013 1251 71.6 17.6 0 126 ## 7 2014 58 70.5 17.2 19 113 ## 8 2015 21 74.3 16.9 43 107 ## 9 2016 8 67.9 32.6 0 110 ## 10 2017 6 57.2 27.9 21 90 You can use filter after group_by. The following example returns the lowest total score from each year. disgust_total %&gt;% mutate(year = year(date)) %&gt;% select(user_id, year, total) %&gt;% group_by(year) %&gt;% filter(rank(total) == 1) %&gt;% arrange(year) ## # A tibble: 7 x 3 ## # Groups: year [7] ## user_id year total ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 U236585 2009 3 ## 2 U292359 2010 6 ## 3 U245384 2013 0 ## 4 U206293 2014 19 ## 5 U407089 2015 43 ## 6 U453237 2016 0 ## 7 U356866 2017 21 You can also use mutate after group_by. The following example calculates subject-mean-centered scores by grouping the scores by user_id and then subtracting the group-specific mean from each score. Note the use of gather to tidy the data into a long format first. disgust_smc &lt;- disgust %&gt;% gather(&quot;question&quot;, &quot;score&quot;, moral1:pathogen7) %&gt;% group_by(user_id) %&gt;% mutate(score_smc = score - mean(score, na.rm = TRUE)) 5.5.7 All Together A lot of what we did above would be easier if the data were tidy, so let’s do that first. Then we can use group_by to calculate the domain scores. It is good practice to use ungroup() after using group_by and summarise. Forgetting to ungroup the dataset won’t affect some further processing, but can really mess up other things. Then we can spread out the 3 domains, calculate the total score, remove any rows with a missing (NA) total, and calculate mean values by year. disgust_tidy &lt;- read_csv(&quot;data/disgust.csv&quot;) %&gt;% gather(&quot;question&quot;, &quot;score&quot;, moral1:pathogen7) %&gt;% separate(question, c(&quot;domain&quot;,&quot;q_num&quot;), sep = -1) %&gt;% group_by(id, user_id, date, domain) %&gt;% summarise(score = mean(score)) %&gt;% ungroup() ## Parsed with column specification: ## cols( ## .default = col_double(), ## date = col_date(format = &quot;&quot;) ## ) ## See spec(...) for full column specifications. ## `summarise()` regrouping output by &#39;id&#39;, &#39;user_id&#39;, &#39;date&#39; (override with `.groups` argument) disgust_tidy2 &lt;- disgust_tidy %&gt;% spread(domain, score) %&gt;% mutate( total = moral + sexual + pathogen, year = year(date) ) %&gt;% filter(!is.na(total)) %&gt;% arrange(user_id) disgust_tidy3 &lt;- disgust_tidy2 %&gt;% group_by(year) %&gt;% summarise( n = n(), avg_pathogen = mean(pathogen), avg_moral = mean(moral), avg_sexual = mean(sexual), first_user = first(user_id), last_user = last(user_id) ) ## `summarise()` ungrouping output (override with `.groups` argument) 5.6 Additional dplyr one-table verbs Use the code examples below and the help pages to figure out what the following one-table verbs do. Most have pretty self-explanatory names. 5.6.1 rename() iris_underscore &lt;- iris %&gt;% rename(sepal_length = Sepal.Length, sepal_width = Sepal.Width, petal_length = Petal.Length, petal_width = Petal.Width) names(iris_underscore) ## [1] &quot;sepal_length&quot; &quot;sepal_width&quot; &quot;petal_length&quot; &quot;petal_width&quot; &quot;Species&quot; Almost everyone gets confused at some point with rename() and tries to put the original names on the left and the new names on the right. Try it and see what the error message looks like. 5.6.2 distinct() # create a data table with duplicated values dupes &lt;- tibble( id = rep(1:5, 2), dv = rep(LETTERS[1:5], 2) ) distinct(dupes) ## # A tibble: 5 x 2 ## id dv ## &lt;int&gt; &lt;chr&gt; ## 1 1 A ## 2 2 B ## 3 3 C ## 4 4 D ## 5 5 E 5.6.3 count() # how many observations from each species are in iris? count(iris, Species) ## Species n ## 1 setosa 50 ## 2 versicolor 50 ## 3 virginica 50 5.6.4 slice() tibble( id = 1:10, condition = rep(c(&quot;A&quot;,&quot;B&quot;), 5) ) %&gt;% slice(3:6, 9) ## # A tibble: 5 x 2 ## id condition ## &lt;int&gt; &lt;chr&gt; ## 1 3 A ## 2 4 B ## 3 5 A ## 4 6 B ## 5 9 A 5.6.5 pull() iris %&gt;% group_by(Species) %&gt;% summarise_all(mean) %&gt;% pull(Sepal.Length) ## [1] 5.006 5.936 6.588 5.7 Window functions Window functions use the order of rows to calculate values. You can use them to do things that require ranking or ordering, like choose the top scores in each class, or acessing the previous and next rows, like calculating cumulative sums or means. The dplyr window functions vignette has very good detailed explanations of these functions, but we’ve described a few of the most useful ones below. 5.7.1 Ranking functions tibble( id = 1:5, &quot;Data Skills&quot; = c(16, 17, 17, 19, 20), &quot;Statistics&quot; = c(14, 16, 18, 18, 19) ) %&gt;% gather(class, grade, 2:3) %&gt;% group_by(class) %&gt;% mutate(row_number = row_number(), rank = rank(grade), min_rank = min_rank(grade), dense_rank = dense_rank(grade), quartile = ntile(grade, 4), percentile = ntile(grade, 100)) ## # A tibble: 10 x 9 ## # Groups: class [2] ## id class grade row_number rank min_rank dense_rank quartile percentile ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 Data Sk… 16 1 1 1 1 1 1 ## 2 2 Data Sk… 17 2 2.5 2 2 1 2 ## 3 3 Data Sk… 17 3 2.5 2 2 2 3 ## 4 4 Data Sk… 19 4 4 4 3 3 4 ## 5 5 Data Sk… 20 5 5 5 4 4 5 ## 6 1 Statist… 14 1 1 1 1 1 1 ## 7 2 Statist… 16 2 2 2 2 1 2 ## 8 3 Statist… 18 3 3.5 3 3 2 3 ## 9 4 Statist… 18 4 3.5 3 3 3 4 ## 10 5 Statist… 19 5 5 5 4 4 5 What are the differences among row_number(), rank(), min_rank(), dense_rank(), and ntile()? Why doesn’t row_number() need an argument? What would happen if you gave it the argument grade or class? What do you think would happen if you removed the group_by(class) line above? What if you added id to the grouping? What happens if you change the order of the rows? What does the second argument in ntile() do? You can use window functions to group your data into quantiles. iris %&gt;% group_by(tertile = ntile(Sepal.Length, 3)) %&gt;% summarise(mean.Sepal.Length = mean(Sepal.Length)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 3 x 2 ## tertile mean.Sepal.Length ## &lt;int&gt; &lt;dbl&gt; ## 1 1 4.94 ## 2 2 5.81 ## 3 3 6.78 5.7.2 Offset functions tibble( trial = 1:10, cond = rep(c(&quot;exp&quot;, &quot;ctrl&quot;), c(6, 4)), score = rpois(10, 4) ) %&gt;% mutate( score_change = score - lag(score, order_by = trial), last_cond_trial = cond != lead(cond, default = &quot;last&quot;) ) ## # A tibble: 10 x 5 ## trial cond score score_change last_cond_trial ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;lgl&gt; ## 1 1 exp 2 NA FALSE ## 2 2 exp 4 2 FALSE ## 3 3 exp 5 1 FALSE ## 4 4 exp 5 0 FALSE ## 5 5 exp 3 -2 FALSE ## 6 6 exp 5 2 TRUE ## 7 7 ctrl 9 4 FALSE ## 8 8 ctrl 6 -3 FALSE ## 9 9 ctrl 6 0 FALSE ## 10 10 ctrl 4 -2 TRUE Look at the help pages for lag() and lead(). What happens if you remove the order_by argument or change it to cond? What does the default argument do? Can you think of circumstances in your own data where you might need to use lag() or lead()? 5.7.3 Cumulative aggregates cumsum(), cummin(), and cummax() are base R functions for calcumaling cumulative means, minimums, and maximums. The dplyr package introduces cumany() and cumall(), which return TRUE if any or all of the previous values meet their criteria. tibble( time = 1:10, obs = c(1, 0, 1, 2, 4, 3, 1, 0, 3, 5) ) %&gt;% mutate( cumsum = cumsum(obs), cummin = cummin(obs), cummax = cummax(obs), cumany = cumany(obs == 3), cumall = cumall(obs &lt; 4) ) ## # A tibble: 10 x 7 ## time obs cumsum cummin cummax cumany cumall ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 1 1 1 1 1 FALSE TRUE ## 2 2 0 1 0 1 FALSE TRUE ## 3 3 1 2 0 1 FALSE TRUE ## 4 4 2 4 0 2 FALSE TRUE ## 5 5 4 8 0 4 FALSE FALSE ## 6 6 3 11 0 4 TRUE FALSE ## 7 7 1 12 0 4 TRUE FALSE ## 8 8 0 12 0 4 TRUE FALSE ## 9 9 3 15 0 4 TRUE FALSE ## 10 10 5 20 0 5 TRUE FALSE What would happen if you change cumany(obs == 3) to cumany(obs &gt; 2)? What would happen if you change cumall(obs &lt; 4) to cumall(obs &lt; 2)? Can you think of circumstances in your own data where you might need to use cumany() or cumall()? 5.8 Exercises Download the exercises. See the answers only after you’ve attempted all the questions. # run this to access the exercise dataskills::exercise(5) # run this to access the answers dataskills::exercise(5, answers = TRUE) "],
["joins.html", "Chapter 6 Data Relations 6.1 Learning Objectives 6.2 Resources 6.3 Data 6.4 Mutating Joins 6.5 Filtering Joins 6.6 Binding Joins 6.7 Set Operations 6.8 Exercises", " Chapter 6 Data Relations 6.1 Learning Objectives 6.1.1 Beginner Be able to use the 4 mutating join verbs: left_join() right_join() inner_join() full_join() Use the by argument to set the join columns 6.1.2 Intermediate Use the suffix argument to distinguish columns with the same name Be able to use the 2 filtering join verbs: semi_join() anti_join() Be able to use the 2 binding join verbs: bind_rows() bind_cols() Be able to use the 3 set operations: intersect() union() setdiff() 6.2 Resources Chapter 13: Relational Data in R for Data Science Cheatsheet for dplyr join functions Lecture slides on dplyr two-table verbs 6.3 Data First, we’ll create two small data tables. subject has id, sex and age for subjects 1-5. Age and sex are missing for subject 3. subject &lt;- tibble( id = seq(1,5), sex = c(&quot;m&quot;, &quot;m&quot;, NA, &quot;f&quot;, &quot;f&quot;), age = c(19, 22, NA, 19, 18) ) id sex age 1 m 19 2 m 22 3 NA NA 4 f 19 5 f 18 exp has subject id and the score from an experiment. Some subjects are missing, some completed twice, and some are not in the subject table. exp &lt;- tibble( id = c(2, 3, 4, 4, 5, 5, 6, 6, 7), score = c(10, 18, 21, 23, 9, 11, 11, 12, 3) ) id score 2 10 3 18 4 21 4 23 5 9 5 11 6 11 6 12 7 3 6.4 Mutating Joins All the mutating joins have this basic syntax: ****_join(x, y, by = NULL, suffix = c(\".x\", \".y\") x = the first (left) table y = the second (right) table {#join-by} by = what columns to match on. If you leave this blank, it will match on all columns with the same names in the two tables. {#join-suffix} suffix = if columns have the same name in the two tables, but you aren’t joining by them, they get a suffix to make them unambiguous. This defaults to “.x” and “.y”, but you can change it to something more meaningful. You can leave out the by argument if you’re matching on all of the columns with the same name, but it’s good practice to always specify it so your code is robust to changes in the loaded data. 6.4.1 left_join() Figure 6.1: Left Join A left_join keeps all the data from the first (left) table and joins anything that matches from the second (right) table. If the right table has more than one match for a row in the right table, there will be more than one row in the joined table (see ids 4 and 5). left_join(subject, exp, by = &quot;id&quot;) ## # A tibble: 7 x 4 ## id sex age score ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 m 19 NA ## 2 2 m 22 10 ## 3 3 &lt;NA&gt; NA 18 ## 4 4 f 19 21 ## 5 4 f 19 23 ## 6 5 f 18 9 ## 7 5 f 18 11 Figure 6.2: Left Join (reversed) The order of tables is swapped here, so the result is all rows from the exp table joined to any matching rows from the subject table. left_join(exp, subject, by = &quot;id&quot;) ## # A tibble: 9 x 4 ## id score sex age ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2 10 m 22 ## 2 3 18 &lt;NA&gt; NA ## 3 4 21 f 19 ## 4 4 23 f 19 ## 5 5 9 f 18 ## 6 5 11 f 18 ## 7 6 11 &lt;NA&gt; NA ## 8 6 12 &lt;NA&gt; NA ## 9 7 3 &lt;NA&gt; NA 6.4.2 right_join() Figure 6.3: Right Join A right_join keeps all the data from the second (right) table and joins anything that matches from the first (left) table. right_join(subject, exp, by = &quot;id&quot;) ## # A tibble: 9 x 4 ## id sex age score ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 m 22 10 ## 2 3 &lt;NA&gt; NA 18 ## 3 4 f 19 21 ## 4 4 f 19 23 ## 5 5 f 18 9 ## 6 5 f 18 11 ## 7 6 &lt;NA&gt; NA 11 ## 8 6 &lt;NA&gt; NA 12 ## 9 7 &lt;NA&gt; NA 3 This table has the same information as left_join(exp, subject, by = “id”), but the columns are in a different order (left table, then right table). 6.4.3 inner_join() Figure 6.4: Inner Join An inner_join returns all the rows that have a match in the other table. inner_join(subject, exp, by = &quot;id&quot;) ## # A tibble: 6 x 4 ## id sex age score ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 m 22 10 ## 2 3 &lt;NA&gt; NA 18 ## 3 4 f 19 21 ## 4 4 f 19 23 ## 5 5 f 18 9 ## 6 5 f 18 11 6.4.4 full_join() Figure 6.5: Full Join A full_join lets you join up rows in two tables while keeping all of the information from both tables. If a row doesn’t have a match in the other table, the other table’s column values are set to NA. full_join(subject, exp, by = &quot;id&quot;) ## # A tibble: 10 x 4 ## id sex age score ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 m 19 NA ## 2 2 m 22 10 ## 3 3 &lt;NA&gt; NA 18 ## 4 4 f 19 21 ## 5 4 f 19 23 ## 6 5 f 18 9 ## 7 5 f 18 11 ## 8 6 &lt;NA&gt; NA 11 ## 9 6 &lt;NA&gt; NA 12 ## 10 7 &lt;NA&gt; NA 3 6.5 Filtering Joins 6.5.1 semi_join() Figure 6.6: Semi Join A semi_join returns all rows from the left table where there are matching values in the right table, keeping just columns from the left table. semi_join(subject, exp, by = &quot;id&quot;) ## # A tibble: 4 x 3 ## id sex age ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2 m 22 ## 2 3 &lt;NA&gt; NA ## 3 4 f 19 ## 4 5 f 18 Unlike an inner join, a semi join will never duplicate the rows in the left table if there is more than one maching row in the right table. Figure 6.7: Semi Join (Reversed) Order matters in a semi join. semi_join(exp, subject, by = &quot;id&quot;) ## # A tibble: 6 x 2 ## id score ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2 10 ## 2 3 18 ## 3 4 21 ## 4 4 23 ## 5 5 9 ## 6 5 11 6.5.2 anti_join() Figure 6.8: Anti Join A anti_join return all rows from the left table where there are not matching values in the right table, keeping just columns from the left table. anti_join(subject, exp, by = &quot;id&quot;) ## # A tibble: 1 x 3 ## id sex age ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 m 19 Figure 6.9: Anti Join (Reversed) Order matters in an anti join. anti_join(exp, subject, by = &quot;id&quot;) ## # A tibble: 3 x 2 ## id score ## &lt;dbl&gt; &lt;dbl&gt; ## 1 6 11 ## 2 6 12 ## 3 7 3 6.6 Binding Joins 6.6.1 bind_rows() You can combine the rows of two tables with bind_rows. Here we’ll add subject data for subjects 6-9 and bind that to the original subject table. new_subjects &lt;- tibble( id = seq(6, 9), sex = c(&quot;m&quot;, &quot;m&quot;, &quot;f&quot;, &quot;f&quot;), age = c(19, 16, 20, 19) ) bind_rows(subject, new_subjects) ## # A tibble: 9 x 3 ## id sex age ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 m 19 ## 2 2 m 22 ## 3 3 &lt;NA&gt; NA ## 4 4 f 19 ## 5 5 f 18 ## 6 6 m 19 ## 7 7 m 16 ## 8 8 f 20 ## 9 9 f 19 The columns just have to have the same names, they don’t have to be in the same order. Any columns that differ between the two tables will just have NA values for entries from the other table. If a row is duplicated between the two tables (like id 5 below), the row will also be duplicated in the resulting table. If your tables have the exact same columns, you can use union() (see below) to avoid duplicates. new_subjects &lt;- tibble( id = seq(5, 9), age = c(18, 19, 16, 20, 19), sex = c(&quot;f&quot;, &quot;m&quot;, &quot;m&quot;, &quot;f&quot;, &quot;f&quot;), new = c(1,2,3,4,5) ) bind_rows(subject, new_subjects) ## # A tibble: 10 x 4 ## id sex age new ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 m 19 NA ## 2 2 m 22 NA ## 3 3 &lt;NA&gt; NA NA ## 4 4 f 19 NA ## 5 5 f 18 NA ## 6 5 f 18 1 ## 7 6 m 19 2 ## 8 7 m 16 3 ## 9 8 f 20 4 ## 10 9 f 19 5 6.6.2 bind_cols() You can merge two tables with the same number of rows using bind_cols. This is only useful if the two tables have their rows in the exact same order. The only advantage over a left join is when the tables don’t have any IDs to join by and you have to rely solely on their order. new_info &lt;- tibble( colour = c(&quot;red&quot;, &quot;orange&quot;, &quot;yellow&quot;, &quot;green&quot;, &quot;blue&quot;) ) bind_cols(subject, new_info) ## # A tibble: 5 x 4 ## id sex age colour ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 m 19 red ## 2 2 m 22 orange ## 3 3 &lt;NA&gt; NA yellow ## 4 4 f 19 green ## 5 5 f 18 blue 6.7 Set Operations 6.7.1 intersect() intersect() returns all rows in two tables that match exactly. The columns don’t have to be in the same order. new_subjects &lt;- tibble( id = seq(4, 9), age = c(19, 18, 19, 16, 20, 19), sex = c(&quot;f&quot;, &quot;f&quot;, &quot;m&quot;, &quot;m&quot;, &quot;f&quot;, &quot;f&quot;) ) dplyr::intersect(subject, new_subjects) ## # A tibble: 2 x 3 ## id sex age ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 4 f 19 ## 2 5 f 18 6.7.2 union() union() returns all the rows from both tables, removing duplicate rows. dplyr::union(subject, new_subjects) ## # A tibble: 9 x 3 ## id sex age ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 m 19 ## 2 2 m 22 ## 3 3 &lt;NA&gt; NA ## 4 4 f 19 ## 5 5 f 18 ## 6 6 m 19 ## 7 7 m 16 ## 8 8 f 20 ## 9 9 f 19 6.7.3 setdiff() setdiff returns rows that are in the first table, but not in the second table. setdiff(subject, new_subjects) ## # A tibble: 3 x 3 ## id sex age ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 m 19 ## 2 2 m 22 ## 3 3 &lt;NA&gt; NA Order matters for setdiff. setdiff(new_subjects, subject) ## # A tibble: 4 x 3 ## id age sex ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 6 19 m ## 2 7 16 m ## 3 8 20 f ## 4 9 19 f 6.8 Exercises Download the exercises. See the answers only after you’ve attempted all the questions. # run this to access the exercise dataskills::exercise(6) # run this to access the answers dataskills::exercise(6, answers = TRUE) "],
["func.html", "Chapter 7 Iteration &amp; Functions 7.1 Learning Objectives 7.2 Resources 7.3 Iteration functions 7.4 Custom functions 7.5 Iterating your own functions 7.6 Exercises", " Chapter 7 Iteration &amp; Functions 7.1 Learning Objectives You will learn about functions and iteration by using simulation to calculate a power analysis for an independent samples t-test. 7.1.1 Basic Work with iteration functions rep, seq, and replicate Use arguments by order or name Write your own custom functions with function() Set default values for the arguments in your functions 7.1.2 Intermediate Understand scope Use error handling and warnings in a function 7.1.3 Advanced The topics below are not (yet) covered in these materials, but they are directions for independent learning. Repeat commands and handle result using purrr::rerun(), purrr::map_*(), purrr::walk() Repeat commands having multiple arguments using purrr::map2_*() and purrr::pmap_*() Create nested data frames using dplyr::group_by() and tidyr::nest() Work with nested data frames in dplyr Capture and deal with errors using ‘adverb’ functions purrr::safely() and purrr::possibly() 7.2 Resources Chapters 19 and 21 of R for Data Science RStudio Apply Functions Cheatsheet Stub for this lesson In the next two lectures, we are going to learn more about iteration (doing the same commands over and over) and custom functions through a data simulation exercise, which will also lead us into more traditional statistical topics. Along the way you will also learn more about how to create vectors and tables in R. # libraries needed for these examples library(tidyverse) ## contains purrr, tidyr, dplyr library(broom) ## converts test output to tidy tables set.seed(8675309) # makes sure random numbers are reproducible 7.3 Iteration functions 7.3.1 rep() The function rep() lets you repeat the first argument a number of times. Use rep() to create a vector of alternating \"A\" and \"B\" values of length 24. rep(c(&quot;A&quot;, &quot;B&quot;), 12) ## [1] &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; ## [20] &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; If you don’t specify what the second argument is, it defaults to times, repeating the vector in the first argument that many times. Make the same vector as above, setting the second argument explicitly. rep(c(&quot;A&quot;, &quot;B&quot;), times = 12) ## [1] &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; ## [20] &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; If the second argument is a vector that is the same length as the first argument, each element in the first vector is repeated than many times. Use rep() to create a vector of 11 \"A\" values followed by 3 \"B\" values. rep(c(&quot;A&quot;, &quot;B&quot;), c(11, 3)) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; You can repeat each element of the vector a sepcified number of times using the each argument, Use rep() to create a vector of 12 \"A\" values followed by 12 \"B\" values. rep(c(&quot;A&quot;, &quot;B&quot;), each = 12) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; ## [20] &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; What do you think will happen if you set both times to 3 and each to 2? rep(c(&quot;A&quot;, &quot;B&quot;), times = 3, each = 2) ## [1] &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; 7.3.2 seq() The function seq() is useful for generating a sequence of numbers with some pattern. Use seq() to create a vector of the integers 0 to 10. seq(0, 10) ## [1] 0 1 2 3 4 5 6 7 8 9 10 You can set the by argument to count by numbers other than 1 (the default). Use seq() to create a vector of the numbers 0 to 100 by 10s. seq(0, 100, by = 10) ## [1] 0 10 20 30 40 50 60 70 80 90 100 The argument length.out is useful if you know how many steps you want to divide something into. Use seq() to create a vector that starts with 0, ends with 100, and has 12 equally spaced steps (hint: how many numbers would be in a vector with 2 steps?). seq(0, 100, length.out = 13) ## [1] 0.000000 8.333333 16.666667 25.000000 33.333333 41.666667 ## [7] 50.000000 58.333333 66.666667 75.000000 83.333333 91.666667 ## [13] 100.000000 7.4 Custom functions In addition to the built-in functions and functions you can access from packages, you can also write your own functions (and eventually even packages!). 7.4.1 Structuring a function The general structure of a function is as follows: function_name &lt;- function(my_args) { # process the arguments # return some value } Here is a very simple function. Can you guess what it does? add1 &lt;- function(my_number) { my_number + 1 } add1(10) ## [1] 11 Let’s make a function that reports p-values in APA format (with “p = rounded value” when p &gt;= .001 and “p &lt; .001” when p &lt; .001). First, we have to name the function. You can name it anything, but try not to duplicate existing functions or you will overwrite them. For example, if you call your function rep, then you will need to use base::rep() to access the normal rep function. Let’s call our p-value function report_p and set up the framework of the function. report_p &lt;- function() { } 7.4.2 Arguments We need to add one argument, the p-value you want to report. The names you choose for the arguments are private to that argument, so it is not a problem if they conflict with other variables in your script. You put the arguments in the parentheses after function in the order you want them to default (just like the built-in functions you’ve used before). report_p &lt;- function(p) { } 7.4.3 Argument defaults You can add a default value to any argument. If that argument is skipped, then the function uses the default argument. It probably doesn’t make sense to run this function without specifying the p-value, but we can add a second argument called digits that defaults to 3, so we can round p-values to 3 digits. report_p &lt;- function(p, digits = 3) { } Now we need to write some code inside the function to process the input arguments and turn them into a returned output. Put the output as the last item in function. report_p &lt;- function(p, digits = 3) { if (p &lt; .001) { reported = &quot;p &lt; .001&quot; } else { roundp &lt;- round(p, digits) reported = paste(&quot;p =&quot;, roundp) } reported } You might also see the returned output inside of the return() function. This does the same thing. report_p &lt;- function(p, digits = 3) { if (p &lt; .001) { reported = &quot;p &lt; .001&quot; } else { roundp &lt;- round(p, digits) reported = paste(&quot;p =&quot;, roundp) } return(reported) } When you run the code defining your function, it doesn’t output anything, but makes a new object in the Environment tab under Functions. Now you can run the function. report_p(0.04869) report_p(0.0000023) ## [1] &quot;p = 0.049&quot; ## [1] &quot;p &lt; .001&quot; 7.4.4 Scope What happens in a function stays in a function. You can change the value of a variable passed to a function, but that won’t change the value of the variable outside of the function, even if that variable has the same name as the one in the function. half &lt;- function(x) { x &lt;- x/2 return(x) } x &lt;- 10 list( &quot;half(x)&quot; = half(x), &quot;x&quot; = x ) ## $`half(x)` ## [1] 5 ## ## $x ## [1] 10 7.4.5 Warnings and errors What happens when you omit the argument for p? Or if you set p to 1.5 or “a”? You might want to add a more specific warning and stop running the function code if someone enters a value that isn’t a number. You can do this with the stop() function. If someone enters a number that isn’t possible for a p-value (0-1), you might want to warn them that this is probably not what they intended, but still continue with the function. You can do this with warning(). report_p &lt;- function(p, digits = 3) { if (!is.numeric(p)) stop(&quot;p must be a number&quot;) if (p &lt;= 0) warning(&quot;p-values are normally greater than 0&quot;) if (p &gt;= 1) warning(&quot;p-values are normally less than 1&quot;) if (p &lt; .001) { reported = &quot;p &lt; .001&quot; } else { roundp &lt;- round(p, digits) reported = paste(&quot;p =&quot;, roundp) } reported } report_p() ## Error in report_p(): argument &quot;p&quot; is missing, with no default report_p(&quot;a&quot;) ## Error in report_p(&quot;a&quot;): p must be a number report_p(-2) ## Warning in report_p(-2): p-values are normally greater than 0 report_p(2) ## Warning in report_p(2): p-values are normally less than 1 ## [1] &quot;p &lt; .001&quot; ## [1] &quot;p = 2&quot; 7.5 Iterating your own functions First, let’s build up the code that we want to iterate. 7.5.1 rnorm() Create a vector of 20 random numbers drawn from a normal distribution with a mean of 5 and standard deviation of 1 using the rnorm() function and store them in the variable A. A &lt;- rnorm(20, mean = 5, sd = 1) 7.5.2 tibble::tibble() A tibble is a type of table or data.frame. The function tibble::tibble() creates a tibble with a column for each argument. Each argument takes the form column_name = data_vector. Create a table called dat including two vectors: A that is a vector of 20 random normally distributed numbers with a mean of 5 and SD of 1, and B that is a vector of 20 random normally distributed numbers with a mean of 5.5 and SD of 1. dat &lt;- tibble( A = rnorm(20, 5, 1), B = rnorm(20, 5.5, 1) ) 7.5.3 t.test You can run a Welch two-sample t-test by including the two samples you made as the first two arguments to the function t.test. You can reference one column of a table by its names using the format table_name$column_name t.test(dat$A, dat$B) ## ## Welch Two Sample t-test ## ## data: dat$A and dat$B ## t = -1.5937, df = 36.528, p-value = 0.1196 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.9559243 0.1144139 ## sample estimates: ## mean of x mean of y ## 4.965341 5.386096 You can also convert the table to long format using the gather function and specify the t-test using the format dv_column~grouping_column. longdat &lt;- gather(dat, group, score, A:B) t.test(score~group, data = longdat) ## ## Welch Two Sample t-test ## ## data: score by group ## t = -1.5937, df = 36.528, p-value = 0.1196 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.9559243 0.1144139 ## sample estimates: ## mean in group A mean in group B ## 4.965341 5.386096 7.5.4 broom::tidy() You can use the function broom::tidy() to extract the data from a statistical test in a table format. The example below pipes everything together. tibble( A = rnorm(20, 5, 1), B = rnorm(20, 5.5, 1) ) %&gt;% gather(group, score, A:B) %&gt;% t.test(score~group, data = .) %&gt;% broom::tidy() ## # A tibble: 1 x 10 ## estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.578 4.97 5.54 -1.84 0.0747 34.3 -1.22 0.0606 ## # … with 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt; Finally, we can extract a single value from this results table using pull(). tibble( A = rnorm(20, 5, 1), B = rnorm(20, 5.5, 1) ) %&gt;% gather(group, score, A:B) %&gt;% t.test(score~group, data = .) %&gt;% broom::tidy() %&gt;% pull(p.value) ## [1] 0.256199 7.5.5 Turn into a function First, name your function t_sim and wrap the code above in a function with no arguments. t_sim &lt;- function() { tibble( A = rnorm(20, 5, 1), B = rnorm(20, 5.5, 1) ) %&gt;% gather(group, score, A:B) %&gt;% t.test(score~group, data = .) %&gt;% broom::tidy() %&gt;% pull(p.value) } Run it a few times to see what happens. t_sim() ## [1] 0.0558203 7.5.6 replicate() You can use the replicate function to run a function any number of times. replicate(3, rnorm(5)) ## [,1] [,2] [,3] ## [1,] 0.2398579 1.0060960 -0.08836476 ## [2,] -1.7685708 0.8362997 0.08114036 ## [3,] 0.1457033 0.4557277 1.38814525 ## [4,] 0.4462924 0.5616177 -0.02341062 ## [5,] 0.5916637 1.4850093 0.98759269 Let’s run the t_sim function 1000 times, assign the resulting p-values to a vector called reps, and check what proportion of p-values are lower than alpha (e.g., .05). This number is the power for this analysis. reps &lt;- replicate(1000, t_sim()) alpha &lt;- .05 power &lt;- mean(reps &lt; alpha) power ## [1] 0.304 7.5.7 Set seed You can use the set.seed function before you run a function that uses random numbers to make sure that you get the same random data back each time. You can use any integer you like as the seed. set.seed(90201) Make sure you don’t ever use set.seed() inside of a simulation function, or you will just simulate the exact same data over and over again. Figure 7.1: @KellyBodwin 7.5.8 Add arguments You can just edit your function each time you want to cacluate power for a different sample n, but it is more efficent to build this into your fuction as an arguments. Redefine t_sim, setting arguments for the mean and SD of group A, the mean and SD of group B, and the number of subjects per group. Give them all default values. t_sim &lt;- function(n = 10, m1=0, sd1=1, m2=0, sd2=1) { tibble( A = rnorm(n, m1, sd1), B = rnorm(n, m2, sd2) ) %&gt;% gather(group, score, A:B) %&gt;% t.test(score~group, data = .) %&gt;% broom::tidy() %&gt;% pull(p.value) } Test your function with some different values to see if the results make sense. t_sim(100) t_sim(100, 0, 1, 0.5, 1) ## [1] 0.5065619 ## [1] 0.001844064 Use replicate to calculate power for 100 subjects/group with an effect size of 0.2 (e.g., A: m = 0, SD = 1; B: m = 0.2, SD = 1). Use 1000 replications. reps &lt;- replicate(1000, t_sim(100, 0, 1, 0.2, 1)) power &lt;- mean(reps &lt; .05) power ## [1] 0.268 Compare this to power calculated from the power.t.test function. power.t.test(n = 100, delta = 0.2, sd = 1, type=&quot;two.sample&quot;) ## ## Two-sample t test power calculation ## ## n = 100 ## delta = 0.2 ## sd = 1 ## sig.level = 0.05 ## power = 0.2902664 ## alternative = two.sided ## ## NOTE: n is number in *each* group Calculate power via simulation and power.t.test for the following tests: 20 subjects/group, A: m = 0, SD = 1; B: m = 0.2, SD = 1 40 subjects/group, A: m = 0, SD = 1; B: m = 0.2, SD = 1 20 subjects/group, A: m = 10, SD = 1; B: m = 12, SD = 1.5 7.6 Exercises Download the exercises. See the answers only after you’ve attempted all the questions. # run this to access the exercise dataskills::exercise(7) # run this to access the answers dataskills::exercise(7, answers = TRUE) "],
["sim.html", "Chapter 8 Probability &amp; Simulation 8.1 Learning Objectives 8.2 Resources 8.3 Distributions 8.4 Example 8.5 Exercises", " Chapter 8 Probability &amp; Simulation 8.1 Learning Objectives 8.1.1 Basic Understand what types of data are best modeled by different distributions uniform binomial normal poisson Generate and plot data randomly sampled from the above distributions Test sampled distributions against a null hypothesis exact binomial test t-test (1-sample, independent samples, paired samples) correlation (pearson, kendall and spearman) Define the following statistical terms: p-value alpha power smallest effect size of interest (SESOI) false positive (type I error) false negative (type II error) confidence interval (CI) Calculate power using iteration and a sampling function 8.1.2 Intermediate Generate 3+ variables from a multivariate normal distribution and plot them 8.1.3 Advanced Calculate the minimum sample size for a specific power level and design 8.2 Resources Stub for this lesson Distribution Shiny App (or run dataskills::app(\"simulate\") Simulation tutorials Chapter 21: Iteration of R for Data Science Improving your statistical inferences on Coursera (week 1) Faux package for data simulation Simulation-Based Power-Analysis for Factorial ANOVA Designs (Lakens and Caldwell 2019) Understanding mixed effects models through data simulation (DeBruine and Barr 2019) 8.3 Distributions Simulating data is a very powerful way to test your understanding of statistical concepts. We are going to use simulations to learn the basics of probability. # libraries needed for these examples library(tidyverse) library(MASS) set.seed(8675309) # makes sure random numbers are reproducible 8.3.1 Uniform Distribution The uniform distribution is the simplest distribution. All numbers in the range have an equal probability of being sampled. Take a minute to think of things in your own research that are uniformly distributed. 8.3.1.1 Sample continuous distribution runif(n, min=0, max=1) Use runif() to sample from a continuous uniform distribution. u &lt;- runif(100000, min = 0, max = 1) # plot to visualise ggplot() + geom_histogram(aes(u), binwidth = 0.05, boundary = 0, fill = &quot;white&quot;, colour = &quot;black&quot;) 8.3.1.2 Sample discrete distribution sample(x, size, replace = FALSE, prob = NULL) Use sample() to sample from a discrete distribution. You can use sample() to simulate events like rolling dice or choosing from a deck of cards. The code below simulates rolling a 6-sided die 10000 times. We set replace to TRUE so that each event is independent. See what happens if you set replace to FALSE. rolls &lt;- sample(1:6, 10000, replace = TRUE) # plot the results ggplot() + geom_histogram(aes(rolls), binwidth = 1, fill = &quot;white&quot;, color = &quot;black&quot;) Figure 8.1: Distribution of dice rolls. You can also use sample to sample from a list of named outcomes. pet_types &lt;- c(&quot;cat&quot;, &quot;dog&quot;, &quot;ferret&quot;, &quot;bird&quot;, &quot;fish&quot;) sample(pet_types, 10, replace = TRUE) ## [1] &quot;cat&quot; &quot;cat&quot; &quot;cat&quot; &quot;cat&quot; &quot;ferret&quot; &quot;dog&quot; &quot;bird&quot; &quot;cat&quot; ## [9] &quot;dog&quot; &quot;fish&quot; Ferrets are a much less common pet than cats and dogs, so our sample isn’t very realistic. You can set the probabilities of each item in the list with the prob argument. pet_types &lt;- c(&quot;cat&quot;, &quot;dog&quot;, &quot;ferret&quot;, &quot;bird&quot;, &quot;fish&quot;) pet_prob &lt;- c(0.3, 0.4, 0.1, 0.1, 0.1) sample(pet_types, 10, replace = TRUE, prob = pet_prob) ## [1] &quot;fish&quot; &quot;dog&quot; &quot;cat&quot; &quot;dog&quot; &quot;cat&quot; &quot;dog&quot; &quot;fish&quot; &quot;dog&quot; &quot;cat&quot; &quot;fish&quot; 8.3.2 Binomial Distribution The binomial distribution is useful for modeling binary data, where each observation can have one of two outcomes, like success/failure, yes/no or head/tails. 8.3.2.1 Sample distribution rbinom(n, size, prob) The rbinom function will generate a random binomial distribution. n = number of observations size = number of trials prob = probability of success on each trial Coin flips are a typical example of a binomial distribution, where we can assign heads to 1 and tails to 0. # 20 individual coin flips of a fair coin rbinom(20, 1, 0.5) ## [1] 1 1 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 0 # 20 individual coin flips of a baised (0.75) coin rbinom(20, 1, 0.75) ## [1] 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 You can generate the total number of heads in 1 set of 20 coin flips by setting size to 20 and n to 1. rbinom(1, 20, 0.75) ## [1] 13 You can generate more sets of 20 coin flips by increasing the n. rbinom(10, 20, 0.5) ## [1] 10 14 11 7 11 13 6 10 9 9 You should always check your randomly generated data to check that it makes sense. For large samples, it’s easiest to do that graphically. A histogram is usually the best choice for plotting binomial data. flips &lt;- rbinom(1000, 20, 0.5) ggplot() + geom_histogram( aes(flips), binwidth = 1, fill = &quot;white&quot;, color = &quot;black&quot; ) Run the simulation above several times, noting how the histogram changes. Try changing the values of n, size, and prob. 8.3.2.2 Exact binomial test binom.test(x, n, p) You can test a binomial distribution against a specific probability using the exact binomial test. x = the number of successes n = the number of trials p = hypothesised probability of success Here we can test a series of 10 coin flips from a fair coin and a biased coin against the hypothesised probability of 0.5 (even odds). n &lt;- 10 fair_coin &lt;- rbinom(1, n, 0.5) biased_coin &lt;- rbinom(1, n, 0.6) binom.test(fair_coin, n, p = 0.5) binom.test(biased_coin, n, p = 0.5) ## ## Exact binomial test ## ## data: fair_coin and n ## number of successes = 4, number of trials = 10, p-value = 0.7539 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.1215523 0.7376219 ## sample estimates: ## probability of success ## 0.4 ## ## ## Exact binomial test ## ## data: biased_coin and n ## number of successes = 7, number of trials = 10, p-value = 0.3438 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.3475471 0.9332605 ## sample estimates: ## probability of success ## 0.7 Run the code above several times, noting the p-values for the fair and biased coins. Alternatively, you can simulate coin flips online and build up a graph of results and p-values. How does the p-value vary for the fair and biased coins? What happens to the confidence intervals if you increase n from 10 to 100? What criterion would you use to tell if the observed data indicate the coin is fair or biased? How often do you conclude the fair coin is biased (false positives)? How often do you conclude the biased coin is fair (false negatives)? 8.3.2.3 Statistical terms The effect is some measure of your data. This will depend on the type of data you have and the type of statistical test you are using. For example, if you flipped a coin 100 times and it landed heads 66 times, the effect would be 66/100. You can then use the exact binomial test to compare this effect to the null effect you would expect from a fair coin (50/100) or to any other effect you choose. The effect size refers to the difference between the effect in your data and the null effect (usually a chance value). {#p-value} The p-value of a test is the probability of seeing an effect at least as extreme as what you have, if the real effect was the value you are testing against (e.g., a null effect). So if you used a binomial test to test against a chance probability of 1/6 (e.g., the probability of rolling 1 with a 6-sided die), then a p-value of 0.17 means that you could expect to see effects at least as extreme as your data 17% of the time just by chance alone. {#alpha} If you are using null hypothesis significance testing (NHST), then you need to decide on a cutoff value (alpha) for making a decision to reject the null hypothesis. We call p-values below the alpha cutoff significant. In psychology, alpha is traditionally set at 0.05, but there are good arguments for setting a different criterion in some circumstances. {#false-pos}{#false-neg} The probability that a test concludes there is an effect when there is really no effect (e.g., concludes a fair coin is biased) is called the false positive rate (or Type I Error Rate). The alpha is the false positive rate we accept for a test. The probability that a test concludes there is no effect when there really is one (e.g., concludes a biased coin is fair) is called the false negative rate (or Type II Error Rate). The beta is the false negative rate we accept for a test. The false positive rate is not the overall probability of getting a false positive, but the probability of a false positive under the null hypothesis. Similarly, the false negative rate is the probability of a false negative under the alternative hypothesis. Unless we know the probability that we are testing a null effect, we can’t say anything about the overall probability of false positives or negatives. If 100% of the hypotheses we test are false, then all significant effects are false positives, but if all of the hypotheses we test are true, then all of the positives are true positives and the overall false positive rate is 0. {#power}{#sesoi} Power is equal to 1 minus beta (i.e., the true positive rate), and depends on the effect size, how many samples we take (n), and what we set alpha to. For any test, if you specify all but one of these values, you can calculate the last. The effect size you use in power calculations should be the smallest effect size of interest (SESOI). See (Lakens, Scheel, and Isager 2018)(https://doi.org/10.1177/2515245918770963) for a tutorial on methods for choosing an SESOI. Let’s say you want to be able to detect at least a 15% difference from chance (50%) in a coin’s fairness, and you want your test to have a 5% chance of false positives and a 10% chance of false negatives. What are the following values? alpha = beta = false positive rate = false negative rate = power = SESOI = {#conf-int} The confidence interval is a range around some value (such as a mean) that has some probability (usually 95%, but you can calculate CIs for any percentage) of containing the parameter, if you repeated the process many times. A 95% CI does not mean that there is a 95% probability that the true mean lies within this range, but that, if you repeated the study many times and calculated the CI this same way every time, you’d expect the true mean to be inside the CI in 95% of the studies. This seems like a subtle distinction, but can lead to some misunderstandings. See (Morey et al. 2016)(https://link.springer.com/article/10.3758/s13423-015-0947-8) for more detailed discussion. 8.3.2.4 Sampling function To estimate these rates, we need to repeat the sampling above many times. A function is ideal for repeating the exact same procedure over and over. Set the arguments of the function to variables that you might want to change. Here, we will want to estimate power for: different sample sizes (n) different effects (bias) different hypothesised probabilities (p, defaults to 0.5) sim_binom_test &lt;- function(n, bias, p = 0.5) { # simulate 1 coin flip n times with the specified bias coin &lt;- rbinom(1, n, bias) # run a binomial test on the simulated data for the specified p btest &lt;- binom.test(coin, n, p) # returun the p-value of this test btest$p.value } Once you’ve created your function, test it a few times, changing the values. sim_binom_test(100, 0.6) ## [1] 0.271253 8.3.2.5 Calculate power Then you can use the replicate() function to run it many times and save all the output values. You can calculate the power of your analysis by checking the proportion of your simulated analyses that have a p-value less than your alpha (the probability of rejecting the null hypothesis when the null hypothesis is true). my_reps &lt;- replicate(1e4, sim_binom_test(100, 0.6)) alpha &lt;- 0.05 # this does not always have to be 0.05 mean(my_reps &lt; alpha) ## [1] 0.4678 1e4 is just scientific notation for a 1 followed by 4 zeros (10000). When you’re running simulations, you usually want to run a lot of them. It’s a pain to keep track of whether you’ve typed 5 or 6 zeros (100000 vs 1000000) and this will change your running time by an order of magnitude. 8.3.3 Normal Distribution 8.3.3.1 Sample distribution rnorm(n, mean, sd) We can simulate a normal distribution of size n if we know the mean and standard deviation (sd). A density plot is usually the best way to visualise this type of data if your n is large. dv &lt;- rnorm(1e5, 10, 2) # proportions of normally-distributed data # within 1, 2, or 3 SD of the mean sd1 &lt;- .6827 sd2 &lt;- .9545 sd3 &lt;- .9973 ggplot() + geom_density(aes(dv), fill = &quot;white&quot;) + geom_vline(xintercept = mean(dv), color = &quot;red&quot;) + geom_vline(xintercept = quantile(dv, .5 - sd1/2), color = &quot;darkgreen&quot;) + geom_vline(xintercept = quantile(dv, .5 + sd1/2), color = &quot;darkgreen&quot;) + geom_vline(xintercept = quantile(dv, .5 - sd2/2), color = &quot;blue&quot;) + geom_vline(xintercept = quantile(dv, .5 + sd2/2), color = &quot;blue&quot;) + geom_vline(xintercept = quantile(dv, .5 - sd3/2), color = &quot;purple&quot;) + geom_vline(xintercept = quantile(dv, .5 + sd3/2), color = &quot;purple&quot;) + scale_x_continuous( limits = c(0,20), breaks = seq(0,20) ) Run the simulation above several times, noting how the density plot changes. What do the vertical lines represent? Try changing the values of n, mean, and sd. 8.3.3.2 T-test t.test(x, y, alternative, mu, paired) Use a t-test to compare the mean of one distribution to a null hypothesis (one-sample t-test), compare the means of two samples (independent-samples t-test), or compare pairs of values (paired-samples t-test). You can run a one-sample t-test comparing the mean of your data to mu. Here is a simulated distribution with a mean of 0.5 and an SD of 1, creating an effect size of 0.5 SD when tested against a mu of 0. Run the simulation a few times to see how often the t-test returns a significant p-value (or run it in the shiny app). sim_norm &lt;- rnorm(100, 0.5, 1) t.test(sim_norm, mu = 0) ## ## One Sample t-test ## ## data: sim_norm ## t = 5.1431, df = 99, p-value = 1.367e-06 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 0.3027835 0.6831659 ## sample estimates: ## mean of x ## 0.4929747 Run an independent-samples t-test by comparing two lists of values. a &lt;- rnorm(100, 0.5, 1) b &lt;- rnorm(100, 0.7, 1) t_ind &lt;- t.test(a, b, paired = FALSE) t_ind ## ## Welch Two Sample t-test ## ## data: a and b ## t = 0.043602, df = 197.5, p-value = 0.9653 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.2813281 0.2940499 ## sample estimates: ## mean of x mean of y ## 0.5123162 0.5059554 The paired argument defaults to FALSE, but it’s good practice to always explicitly set it so you are never confused about what type of test you are performing. 8.3.3.3 Sampling function We can use the names() function to find out the names of all the t.test parameters and use this to just get one type of data, like the test statistic (e.g., t-value). names(t_ind) t_ind$statistic ## [1] &quot;statistic&quot; &quot;parameter&quot; &quot;p.value&quot; &quot;conf.int&quot; &quot;estimate&quot; ## [6] &quot;null.value&quot; &quot;stderr&quot; &quot;alternative&quot; &quot;method&quot; &quot;data.name&quot; ## t ## 0.04360244 Alternatively, use broom::tidy() to convert the output into a tidy table. broom::tidy(t_ind) ## # A tibble: 1 x 10 ## estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.00636 0.512 0.506 0.0436 0.965 197. -0.281 0.294 ## # … with 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt; If you want to run the simulation many times and record information each time, first you need to turn your simulation into a function. sim_t_ind &lt;- function(n, m1, sd1, m2, sd2) { # simulate v1 v1 &lt;- rnorm(n, m1, sd1) #simulate v2 v2 &lt;- rnorm(n, m2, sd2) # compare using an independent samples t-test t_ind &lt;- t.test(v1, v2, paired = FALSE) # return the p-value return(t_ind$p.value) } Run it a few times to check that it gives you sensible values. sim_t_ind(100, 0.7, 1, 0.5, 1) ## [1] 0.1002539 Now replicate the simulation 1000 times. my_reps &lt;- replicate(1e4, sim_t_ind(100, 0.7, 1, 0.5, 1)) alpha &lt;- 0.05 power &lt;- mean(my_reps &lt; alpha) power ## [1] 0.2926 Run the code above several times. How much does the power value fluctuate? How many replications do you need to run to get a reliable estimate of power? Compare your power estimate from simluation to a power calculation using power.t.test(). Here, delta is the difference between m1 and m2 above. power.t.test(n = 100, delta = 0.2, sd = 1, sig.level = alpha, type = &quot;two.sample&quot;) ## ## Two-sample t test power calculation ## ## n = 100 ## delta = 0.2 ## sd = 1 ## sig.level = 0.05 ## power = 0.2902664 ## alternative = two.sided ## ## NOTE: n is number in *each* group You can plot the distribution of p-values. ggplot() + geom_histogram( aes(my_reps), binwidth = 0.05, boundary = 0, fill = &quot;white&quot;, color = &quot;black&quot; ) What do you think the distribution of p-values is when there is no effect (i.e., the means are identical)? Check this yourself. Make sure the boundary argument is set to 0 for p-value histograms. See what happens with a null effect if boundary is not set. 8.3.4 Bivariate Normal 8.3.4.1 Correlation You can test if two continuous variables are related to each other using the cor() function. Below is one way to generate two correlated variables: a is drawn from a normal distribution, while x and y the sum of and another value drawn from a random normal distribution. We’ll learn later how to generate specific correlations in simulated data. n &lt;- 100 # number of random samples a &lt;- rnorm(n, 0, 1) x &lt;- a + rnorm(n, 0, 1) y &lt;- a + rnorm(n, 0, 1) cor(x, y) ## [1] 0.5500246 Set n to a large number like 1e6 so that the correlations are less affected by chance. Change the value of the mean for a, x, or y. Does it change the correlation between x and y? What happens when you increase or decrease the sd for a? Can you work out any rules here? cor() defaults to Pearson’s correlations. Set the method argument to use Kendall or Spearman correlations. cor(x, y, method = &quot;spearman&quot;) ## [1] 0.529553 8.3.4.2 Sample distribution What if we want to sample from a population with specific relationships between variables? We can sample from a bivariate normal distribution using mvrnorm() from the MASS package. n &lt;- 1000 # number of random samples rho &lt;- 0.5 # population correlation between the two variables mu &lt;- c(10, 20) # the means of the samples stdevs &lt;- c(5, 6) # the SDs of the samples # correlation matrix cor_mat &lt;- matrix(c( 1, rho, rho, 1), 2) # create the covariance matrix sigma &lt;- (stdevs %*% t(stdevs)) * cor_mat # sample from bivariate normal distribution bvn &lt;- MASS::mvrnorm(n, mu, sigma) cor(bvn) # check correlation matrix ## [,1] [,2] ## [1,] 1.0000000 0.5081377 ## [2,] 0.5081377 1.0000000 Plot your sampled variables to check everything worked like you expect. It’s easiest to convert the output of mvnorm into a tibble in order to use it in ggplot. bvn %&gt;% as_tibble() %&gt;% ggplot(aes(V1, V2)) + geom_point(alpha = 0.5) + geom_smooth(method = &quot;lm&quot;) + geom_density2d() ## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0. ## Using compatibility `.name_repair`. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. ## `geom_smooth()` using formula &#39;y ~ x&#39; 8.3.5 Multivariate Normal You can generate more than 2 correlated variables, but it gets a little trickier to create the correlation matrix. 8.3.5.1 Sample distribution n &lt;- 200 # number of random samples rho1_2 &lt;- 0.5 # correlation betwen v1 and v2 rho1_3 &lt;- 0 # correlation betwen v1 and v3 rho2_3 &lt;- 0.7 # correlation betwen v2 and v3 mu &lt;- c(10, 20, 30) # the means of the samples stdevs &lt;- c(8, 9, 10) # the SDs of the samples # correlation matrix cor_mat &lt;- matrix(c( 1, rho1_2, rho1_3, rho1_2, 1, rho2_3, rho1_3, rho2_3, 1), 3) sigma &lt;- (stdevs %*% t(stdevs)) * cor_mat bvn3 &lt;- MASS::mvrnorm(n, mu, sigma) cor(bvn3) # check correlation matrix ## [,1] [,2] [,3] ## [1,] 1.0000000 0.5983590 0.1529026 ## [2,] 0.5983590 1.0000000 0.6891871 ## [3,] 0.1529026 0.6891871 1.0000000 Alternatively, you can use the (in-development) package faux to generate any number of correlated variables. It also allows to to easily name the variables and has a function for checking the parameters of your new simulated data (check_sim_stats()). #devtools::install_github(&quot;debruine/faux&quot;) library(faux) ## ## ************ ## Welcome to faux. For support and examples visit: ## http://debruine.github.io/faux/ ## - Get and set global package options with: faux_options() ## ************ bvn3 &lt;- faux::rnorm_multi( n = n, vars = 3, mu = mu, sd = stdevs, r = c(rho1_2, rho1_3, rho2_3), varnames = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) ) faux::check_sim_stats(bvn3) ## n var A B C mean sd ## 1 200 A 1.00 0.54 0.10 10.47 7.27 ## 2 200 B 0.54 1.00 0.73 20.64 9.46 ## 3 200 C 0.10 0.73 1.00 30.70 9.64 8.3.5.2 3D Plots You can use the plotly library to make a 3D graph. library(plotly) ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:MASS&#39;: ## ## select ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout marker_style = list( color = &quot;#ff0000&quot;, line = list( color = &quot;#444&quot;, width = 1 ), opacity = 0.5, size = 5 ) bvn3 %&gt;% as_tibble() %&gt;% plot_ly(x = ~A, y = ~B, z = ~C, marker = marker_style) %&gt;% add_markers() ## Warning: `arrange_()` is deprecated as of dplyr 0.7.0. ## Please use `arrange()` instead. ## See vignette(&#39;programming&#39;) for more help ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. 8.4 Example This example uses the Growth Chart Data Tables from the US CDC. The data consist of height in centimeters for the z-scores of –2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, and 2 by sex (1=male; 2=female) and half-month of age (from 24.0 to 240.5 months). 8.4.1 Load &amp; wrangle We have to do a little data wrangling first. Have a look at the data after you import it and relabel Sex to male and female instead of 1 and 2. Also convert Agemos (age in months) to years. Relabel the column 0 as mean and calculate a new column named sd as the difference between columns 1 and 0. orig_height_age &lt;- read_csv(&quot;https://www.cdc.gov/growthcharts/data/zscore/zstatage.csv&quot;) ## Parsed with column specification: ## cols( ## Sex = col_character(), ## Agemos = col_character(), ## `-2` = col_double(), ## `-1.5` = col_double(), ## `-1` = col_double(), ## `-0.5` = col_double(), ## `0` = col_double(), ## `0.5` = col_double(), ## `1` = col_double(), ## `1.5` = col_double(), ## `2` = col_double() ## ) height_age &lt;- orig_height_age %&gt;% filter(Sex %in% c(1,2)) %&gt;% mutate( sex = recode(Sex, &quot;1&quot; = &quot;male&quot;, &quot;2&quot; = &quot;female&quot;), age = as.numeric(Agemos)/12, sd = `1` - `0` ) %&gt;% dplyr::select(sex, age, mean = `0`, sd) If you run the code above without putting dplyr:: before the select() function, you might get an error message. This is because the MASS package also has a function called select() and, since we loaded MASS after tidyverse, the MASS function becomes the default. When you loaded MASS, you should have seen a warning like “The following object is masked from ‘package:dplyr’: select”. You can use functions with the same name from different packages by specifying the package before the function name, separated by two colons. 8.4.2 Plot Plot your new data frame to see how mean height changes with age for boys and girls. ggplot(height_age, aes(age, mean, color = sex)) + geom_smooth(aes(ymin = mean - sd, ymax = mean + sd), stat=&quot;identity&quot;) 8.4.3 Get means and SDs Create new variables for the means and SDs for 20-year-old men and women. height_sub &lt;- height_age %&gt;% filter(age == 20) m_mean &lt;- height_sub %&gt;% filter(sex == &quot;male&quot;) %&gt;% pull(mean) m_sd &lt;- height_sub %&gt;% filter(sex == &quot;male&quot;) %&gt;% pull(sd) f_mean &lt;- height_sub %&gt;% filter(sex == &quot;female&quot;) %&gt;% pull(mean) f_sd &lt;- height_sub %&gt;% filter(sex == &quot;female&quot;) %&gt;% pull(sd) height_sub ## # A tibble: 2 x 4 ## sex age mean sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 male 20 177. 7.12 ## 2 female 20 163. 6.46 8.4.4 Simulate a population Simulate 50 random male heights and 50 random female heights using the rnorm() function and the means and SDs above. Plot the data. sim_height &lt;- tibble( male = rnorm(50, m_mean, m_sd), female = rnorm(50, f_mean, f_sd) ) %&gt;% gather(&quot;sex&quot;, &quot;height&quot;, male:female) ggplot(sim_height) + geom_density(aes(height, fill = sex), alpha = 0.5) + xlim(125, 225) Run the simulation above several times, noting how the density plot changes. Try changing the age you’re simulating. 8.4.5 Analyse simulated data Use the sim_t_ind(n, m1, sd1, m2, sd2) function we created above to generate one simulation with a sample size of 50 in each group using the means and SDs of male and female 14-year-olds. height_sub &lt;- height_age %&gt;% filter(age == 14) m_mean &lt;- height_sub %&gt;% filter(sex == &quot;male&quot;) %&gt;% pull(mean) m_sd &lt;- height_sub %&gt;% filter(sex == &quot;male&quot;) %&gt;% pull(sd) f_mean &lt;- height_sub %&gt;% filter(sex == &quot;female&quot;) %&gt;% pull(mean) f_sd &lt;- height_sub %&gt;% filter(sex == &quot;female&quot;) %&gt;% pull(sd) sim_t_ind(50, m_mean, m_sd, f_mean, f_sd) ## [1] 0.002962042 8.4.6 Replicate simulation Now replicate this 1e4 times using the replicate() function. This function will save the returned p-values in a list (my_reps). We can then check what proportion of those p-values are less than our alpha value. This is the power of our test. my_reps &lt;- replicate(1e4, sim_t_ind(50, m_mean, m_sd, f_mean, f_sd)) alpha &lt;- 0.05 power &lt;- mean(my_reps &lt; alpha) power ## [1] 0.6428 8.4.7 One-tailed prediction This design has about 65% power to detect the sex difference in height (with a 2-tailed test). Modify the sim_t_ind function for a 1-tailed prediction. You could just set alternative equal to “greater” in the function, but it might be better to add the alternative argument to your function (giving it the same default value as t.test) and change the value of alternative in the function to alternative. sim_t_ind &lt;- function(n, m1, sd1, m2, sd2, alternative = &quot;two.sided&quot;) { v1 &lt;- rnorm(n, m1, sd1) v2 &lt;- rnorm(n, m2, sd2) t_ind &lt;- t.test(v1, v2, paired = FALSE, alternative = alternative) return(t_ind$p.value) } alpha &lt;- 0.05 my_reps &lt;- replicate(1e4, sim_t_ind(50, m_mean, m_sd, f_mean, f_sd, &quot;greater&quot;)) mean(my_reps &lt; alpha) ## [1] 0.761 8.4.8 Range of sample sizes What if we want to find out what sample size will give us 80% power? We can try trial and error. We know the number should be slightly larger than 50. But you can search more systematically by repeating your power calculation for a range of sample sizes. This might seem like overkill for a t-test, where you can easily look up sample size calculators online, but it is a valuable skill to learn for when your analyses become more complicated. Start with a relatively low number of replications and/or more spread-out samples to estimate where you should be looking more specifically. Then you can repeat with a narrower/denser range of sample sizes and more iterations. alpha &lt;- 0.05 power_table &lt;- tibble( n = seq(20, 100, by = 5) ) %&gt;% mutate(power = map_dbl(n, function(n) { ps &lt;- replicate(1e3, sim_t_ind(n, m_mean, m_sd, f_mean, f_sd, &quot;greater&quot;)) mean(ps &lt; alpha) })) ggplot(power_table, aes(n, power)) + geom_smooth() + geom_point() + geom_hline(yintercept = 0.8) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Now we can narrow down our search to values around 55 (plus or minus 5) and increase the number of replications from 1e3 to 1e4. power_table &lt;- tibble( n = seq(50, 60) ) %&gt;% mutate(power = map_dbl(n, function(n) { ps &lt;- replicate(1e3, sim_t_ind(n, m_mean, m_sd, f_mean, f_sd, &quot;greater&quot;)) mean(ps &lt; alpha) })) ##ggplot(power_table, aes(n, power)) + ## geom_smooth() + ## geom_point() + ## geom_hline(yintercept = 0.8) + ## scale_x_continuous(breaks = sample_size) 8.5 Exercises Download the exercises. See the answers only after you’ve attempted all the questions. # run this to access the exercise dataskills::exercise(8) # run this to access the answers dataskills::exercise(8, answers = TRUE) E References "],
["glm.html", "Chapter 9 Introduction to GLM 9.1 Learning Objectives 9.2 Resources 9.3 Setup 9.4 GLM 9.5 Relationships among tests 9.6 Understanding ANOVA 9.7 Exercises", " Chapter 9 Introduction to GLM 9.1 Learning Objectives 9.1.1 Basic Define the components of the GLM Simulate data using GLM equations Identify the model parameters that correspond to the data-generation parameters Understand and plot residuals Predict new values using the model Explain the differences among coding schemes 9.1.2 Intermediate Demonstrate the relationships among two-sample t-test, one-way ANOVA, and linear regression Given data and a GLM, generate a decomposition matrix and calculate sums of squares, mean squares, and F ratios for a one-way ANOVA 9.2 Resources Stub for this lesson Jeff Miller and Patricia Haden, Statistical Analysis with the Linear Model (free online textbook) lecture slides introducing the General Linear Model GLM shiny app F distribution 9.3 Setup You’ll need the following packages. # libraries needed for these examples library(tidyverse) library(broom) set.seed(30250) # makes sure random numbers are reproducible 9.4 GLM 9.4.1 What is the GLM? The General Linear Model (GLM) a general mathematical framework for expressing relationships among variables that can express or test linear relationships between a numerical dependent variable and any combination of categorical or continuous independent variables. 9.4.2 Components There are some mathematical conventions that you need to learn to understand the equations representing linear models. Once you understand those, learning about the GLM will get much easier. Component of GLM Notation Dependent Variable (DV) \\(Y\\) Grand Average \\(\\mu\\) (the Greek letter “mu”) Main Effects \\(A, B, C, \\ldots\\) Interactions \\(AB, AC, BC, ABC, \\ldots\\) Random Error \\(S(Group)\\) The linear equation predicts the dependent variable (\\(Y\\)) as the sum of the grand average value of \\(Y\\) (\\(\\mu\\), also called the intercept), the main effects of all the predictor variables (\\(A+B+C+ \\ldots\\)), the interactions among all the predictor variables (\\(AB, AC, BC, ABC, \\ldots\\)), and some random error (\\(S(Group)\\)). The equation for a model with two predictor variables (\\(A\\) and \\(B\\)) and their interaction (\\(AB\\)) is written like this: \\(Y\\) ~ \\(\\mu+A+B+AB+S(Group)\\) Don’t worry if this doesn’t make sense until we walk through a concrete example. 9.4.3 Simulating data from GLM A good way to learn about linear models is to simulate data where you know exactly how the variables are related, and then analyse this simulated data to see where the parameters show up in the analysis. We’ll start with a very simple linear model that just has a single categorical factor with two levels. Let’s say we’re predicting reaction times for congruent and incongruent trials in a Stroop task for a single participant. Average reaction time (mu) is 800ms, and is 50ms faster for congruent than incongruent trials (effect). A factor is a categorical variable that is used to divide subjects into groups, usually to draw some comparison. Factors are composed of different levels. Do not confuse factors with levels! In the example above, trial type is a factor level, incongrunt is a factor level, and congruent is a factor level. You need to represent categorical factors with numbers. The numbers, or coding you choose will affect the numbers you get out of the analysis and how you need to interpret them. Here, we will effect code the trial types so that congruent trials are coded as +0.5, and incongruent trials are coded as -0.5. A person won’t always respond exactly the same way. They might be a little faster on some trials than others, due to random fluctuations in attention, learning about the task, or fatigue. So we can add an error term to each trial. We can’t know how much any specific trial will differ, but we can characterise the distribution of how much trials differ from average and then sample from this distribution. Here, we’ll assume the error term is sampled from a normal distribution with a standard deviation of 100 ms (the mean of the error term distribution is always 0). We’ll also sample 100 trials of each type, so we can see a range of variation. So first create variables for all of the parameters that describe your data. n_per_grp &lt;- 100 mu &lt;- 800 # average RT effect &lt;- 50 # average difference between congruent and incongruent trials error_sd &lt;- 100 # standard deviation of the error term trial_types &lt;- c(&quot;congruent&quot; = 0.5, &quot;incongruent&quot; = -0.5) # effect code Then simulate the data by creating a data table with a row for each trial and columns for the trial type and the error term (random numbers samples from a normal distribution with the SD specified by error_sd). For categorical variables, include both a column with the text labels (trial_type) and another column with the coded version (trial_type.e) to make it easier to check what the codings mean and to use for graphing. Calculate the dependent variable (RT) as the sum of the grand mean (mu), the coefficient (effect) multiplied by the effect-coded predictor variable (trial_type.e), and the error term. dat &lt;- data.frame( trial_type = rep(names(trial_types), each = n_per_grp) ) %&gt;% mutate( trial_type.e = recode(trial_type, !!!trial_types), error = rnorm(nrow(.), 0, error_sd), RT = mu + effect*trial_type.e + error ) The !!! (triple bang) in the code recode(trial_type, !!!trial_types) is a way to expand the vector trial_types &lt;- c(“congruent” = 0.5, “incongruent” = -0.5). It’s equivalent to recode(trial_type, “congruent” = 0.5, “incongruent” = -0.5). This pattern avoids making mistakes with recoding because there is only one place where you set up the category to code mapping (in the trial_types vector). Last but not least, always plot simulated data to make sure it looks like you expect. ggplot(dat, aes(trial_type, RT)) + geom_violin() + geom_boxplot(aes(fill = trial_type), width = 0.25, show.legend = FALSE) Figure 9.1: Simulated Data 9.4.4 Linear Regression Now we can analyse the data we simulated using the function lm(). It takes the formula as the first argument. This is the same as the data-generating equation, but you can omit the error term (this is implied), and takes the data table as the second argument. Use the summary() function to see the statistical summary. my_lm &lt;- lm(RT ~ trial_type.e, data = dat) summary(my_lm) ## ## Call: ## lm(formula = RT ~ trial_type.e, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -302.110 -70.052 0.948 68.262 246.220 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 788.192 7.206 109.376 &lt; 2e-16 *** ## trial_type.e 61.938 14.413 4.297 2.71e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 101.9 on 198 degrees of freedom ## Multiple R-squared: 0.08532, Adjusted R-squared: 0.0807 ## F-statistic: 18.47 on 1 and 198 DF, p-value: 2.707e-05 Notice how the estimate for the (Intercept) is close to the value we set for mu and the estimate for trial_type.e is close to the value we set for effect. Change the values of mu and effect, resimulate the data, and re-run the linear model. What happens to the estimates? 9.4.5 Residuals You can use the residuals() function to extract the error term for each each data point. This is the DV values, minus the estimates for the intercept and trial type. We’ll make a density plot of the residuals below and compare it to the normal distribution we used for the error term. res &lt;- residuals(my_lm) ggplot(dat) + stat_function(aes(0), color = &quot;grey60&quot;, fun = dnorm, n = 101, args = list(mean = 0, sd = error_sd)) + geom_density(aes(res, color = trial_type)) Figure 9.2: Model residuals should be approximately normally distributed for each group You can also compare the model residuals to the simulated error values. If the model is accurate, they should be almost identical. If the intercept estimate is slightly off, the points will be slightly above or below the black line. If the estimate for the effect of trial type is slightly off, there will be a small, systematic difference between residuals for congruent and incongruent trials. ggplot(dat) + geom_abline(slope = 1) + geom_point(aes(error, res,color = trial_type)) + ylab(&quot;Model Residuals&quot;) + xlab(&quot;Simulated Error&quot;) Figure 9.3: Model residuals should be very similar to the simulated error What happens to the residuals if you fit a model that ignores trial type (e.g., lm(Y ~ 1, data = dat))? 9.4.6 Predict New Values You can use the estimates from your model to predict new data points, given values for the model parameters. For this simple example, we just need to know the trial type to make a prediction. For congruent trials, you would predict that a new data point would be equal to the intercept estimate plus the trial type estimate multiplied by 0.5 (the effect code for congruent trials). int_est &lt;- my_lm$coefficients[[&quot;(Intercept)&quot;]] tt_est &lt;- my_lm$coefficients[[&quot;trial_type.e&quot;]] tt_code &lt;- trial_types[[&quot;congruent&quot;]] new_congruent_RT &lt;- int_est + tt_est * tt_code new_congruent_RT ## [1] 819.1605 You can also use the predict() function to do this more easily. The second argument is a data table with columns for the factors in the model and rows with the values that you want to use for the prediction. predict(my_lm, newdata = tibble(trial_type.e = 0.5)) ## 1 ## 819.1605 If you look up this function using ?predict, you will see that “The function invokes particular methods which depend on the class of the first argument.” What this means is that predict() works differently depending on whether you’re predicting from the output of lm() or other analysis functions. You can search for help on the lm version with ?predict.lm. 9.4.7 Coding Categorical Variables In the example above, we used effect coding for trial type. You can also use sum coding, which assigns +1 and -1 to the levels instead of +0.5 and -0.5. More commonly, you might want to use treatment coding, which assigns 0 to one level (usually a baseline or control condition) and 1 to the other level (usually a treatment or experimental condition). Here we will add sum-coded and treatment-coded versions of trial_type to the dataset using the recode() function. dat &lt;- dat %&gt;% mutate( trial_type.sum = recode(trial_type, &quot;congruent&quot; = +1, &quot;incongruent&quot; = -1), trial_type.tr = recode(trial_type, &quot;congruent&quot; = 1, &quot;incongruent&quot; = 0) ) If you define named vectors with your levels and coding, you can use them with the recode() function if you expand them using !!!. tt_sum &lt;- c(&quot;congruent&quot; = +1, &quot;incongruent&quot; = -1) tt_tr &lt;- c(&quot;congruent&quot; = 1, &quot;incongruent&quot; = 0) dat &lt;- dat %&gt;% mutate( trial_type.sum = recode(trial_type, !!!tt_sum), trial_type.tr = recode(trial_type, !!!tt_tr) ) Here are the coefficients for the effect-coded version. They should be the same as those from the last analysis. lm(RT ~ trial_type.e, data = dat)$coefficients ## (Intercept) trial_type.e ## 788.19166 61.93773 Here are the coefficients for the sum-coded version. This give the same results as effect coding, except the estimate for the categorical factor will be exactly half as large, as it represents the difference between each trial type and the hypothetical condition of 0 (the overall mean RT), rather than the difference between the two trial types. lm(RT ~ trial_type.sum, data = dat)$coefficients ## (Intercept) trial_type.sum ## 788.19166 30.96887 Here are the coefficients for the treatment-coded version. The estimate for the categorical factor will be the same as in the effect-coded version, but the intercept will decrease. It will be equal to the intercept minus the estimate for trial type from the sum-coded version. lm(RT ~ trial_type.tr, data = dat)$coefficients ## (Intercept) trial_type.tr ## 757.22279 61.93773 9.5 Relationships among tests 9.5.1 T-test The t-test is just a special, limited example of a general linear model. t.test(RT ~ trial_type.e, data = dat, var.equal = TRUE) ## ## Two Sample t-test ## ## data: RT by trial_type.e ## t = -4.2975, df = 198, p-value = 2.707e-05 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -90.35945 -33.51601 ## sample estimates: ## mean in group -0.5 mean in group 0.5 ## 757.2228 819.1605 What happens when you use other codings for trial type in the t-test above? Which coding maps onto the results of the t-test best? 9.5.2 ANOVA ANOVA is also a special, limited version of the linear model. my_aov &lt;- aov(RT ~ trial_type.e, data = dat) summary(my_aov, intercept = TRUE) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## (Intercept) 1 124249219 124249219 11963.12 &lt; 2e-16 *** ## trial_type.e 1 191814 191814 18.47 2.71e-05 *** ## Residuals 198 2056432 10386 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The easiest way to get parameters out of an analysis is to use the broom::tidy() function. This returns a tidy table that you can extract numbers of interest from. Here, we just want to get the F-value for the effect of trial_type. Compare the square root of this value to the t-value from the t-tests above. f &lt;- broom::tidy(my_aov)$statistic[1] sqrt(f) ## [1] 4.297498 9.6 Understanding ANOVA We’ll walk through an example of a one-way ANOVA with the following equation: \\(Y_{ij} = \\mu + A_i + S(A)_{ij}\\) This means that each data point (\\(Y_{ij}\\)) is predicted to be the sum of the grand mean (\\(\\mu\\)), plus the effect of factor A (\\(A_i\\)), plus some residual error (\\(S(A)_{ij}\\)). 9.6.1 Means, Variability, and Deviation Scores Let’s create a simple simulation function so you can quickly create a two-sample dataset with specified Ns, means, and SDs. two_sample &lt;- function(n = 10, m1 = 0, m2 = 0, sd1 = 1, sd2 = 1) { s1 &lt;- rnorm(n, m1, sd1) s2 &lt;- rnorm(n, m2, sd2) data.frame( Y = c(s1, s2), grp = rep(c(&quot;A&quot;, &quot;B&quot;), each = n) ) } Now we will use two_sample() to create a dataset dat with N=5 per group, means of -2 and +2, and SDs of 1 and 1 (yes, this is an effect size of d = 4). dat &lt;- two_sample(5, -2, +2, 1, 1) You can calculate how each data point (Y) deviates from the overall sample mean (\\(\\hat{\\mu}\\)), which is represented by the horizontal grey line below and the deviations are the vertical grey lines. You can also calculate how different each point is from its group-specific mean (\\(\\hat{A_i}\\)), which are represented by the horizontal coloured lines below and the deviations are the coloured vertical lines. Figure 9.4: Deviations of each data point (Y) from the overall and group means You can use these deviations to calculate variability between groups and within groups. ANOVA tests whether the variability between groups is larger than that within groups, accounting for the number of groups and observations. 9.6.2 Decomposition matrices We can use the estimation equations for a one-factor ANOVA to calculate the model components. mu is the overall mean a is how different each group mean is from the overall mean err is residual error, calculated by subtracting mu and a from Y This produces a decomposition matrix, a table with columns for Y, mu, a, and err. decomp &lt;- dat %&gt;% select(Y, grp) %&gt;% mutate(mu = mean(Y)) %&gt;% # calculate mu_hat group_by(grp) %&gt;% mutate(a = mean(Y) - mu) %&gt;% # calculate a_hat for each grp ungroup() %&gt;% mutate(err = Y - mu - a) # calculate residual error Y grp mu a err -1.4770938 A 0.1207513 -1.533501 -0.0643443 -2.9508741 A 0.1207513 -1.533501 -1.5381246 -0.6376736 A 0.1207513 -1.533501 0.7750759 -1.7579084 A 0.1207513 -1.533501 -0.3451589 -0.2401977 A 0.1207513 -1.533501 1.1725518 0.1968155 B 0.1207513 1.533501 -1.4574367 2.6308008 B 0.1207513 1.533501 0.9765486 2.0293297 B 0.1207513 1.533501 0.3750775 2.1629037 B 0.1207513 1.533501 0.5086516 1.2514112 B 0.1207513 1.533501 -0.4028410 Calculate sums of squares for mu, a, and err. SS &lt;- decomp %&gt;% summarise(mu = sum(mu*mu), a = sum(a*a), err = sum(err*err)) mu a err 0.1458088 23.51625 8.104182 If you’ve done everything right, SS$mu + SS$a + SS$err should equal the sum of squares for Y. SS_Y &lt;- sum(decomp$Y^2) all.equal(SS_Y, SS$mu + SS$a + SS$err) ## [1] TRUE Divide each sum of squares by its corresponding degrees of freedom (df) to calculate mean squares. The df for mu is 1, the df for factor a is K-1 (K is the number of groups), and the df for err is N - K (N is the number of observations). K &lt;- n_distinct(dat$grp) N &lt;- nrow(dat) df &lt;- c(mu = 1, a = K - 1, err = N - K) MS &lt;- SS / df mu a err 0.1458088 23.51625 1.013023 Then calculate an F-ratio for mu and a by dividing their mean squares by the error term mean square. Get the p-values that correspond to these F-values using the pf() function. F_mu &lt;- MS$mu / MS$err F_a &lt;- MS$a / MS$err p_mu &lt;- pf(F_mu, df1 = df[&#39;mu&#39;], df2 = df[&#39;err&#39;], lower.tail = FALSE) p_a &lt;- pf(F_a, df1 = df[&#39;a&#39;], df2 = df[&#39;err&#39;], lower.tail = FALSE) Put everything into a data frame to display it in the same way as the ANOVA summary function. my_calcs &lt;- data.frame( term = c(&quot;Intercept&quot;, &quot;grp&quot;, &quot;Residuals&quot;), Df = df, SS = c(SS$mu, SS$a, SS$err), MS = c(MS$mu, MS$a, MS$err), F = c(F_mu, F_a, NA), p = c(p_mu, p_a, NA) ) term Df SS MS F p mu Intercept 1 0.146 0.146 0.144 0.714 a grp 1 23.516 23.516 23.214 0.001 err Residuals 8 8.104 1.013 NA NA Now run a one-way ANOVA on your results and compare it to what you obtained in your calculations. aov(Y ~ grp, data = dat) %&gt;% summary(intercept = TRUE) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## (Intercept) 1 0.146 0.146 0.144 0.71427 ## grp 1 23.516 23.516 23.214 0.00132 ** ## Residuals 8 8.104 1.013 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Using the code above, write your own function that takes a table of data and returns the ANOVA results table like above. 9.7 Exercises Download the exercises. See the answers only after you’ve attempted all the questions. # run this to access the exercise dataskills::exercise(9) # run this to access the answers dataskills::exercise(9, answers = TRUE) "],
["repro.html", "Chapter 10 Reproducible Workflows 10.1 Learning Objectives 10.2 Resources 10.3 R Markdown 10.4 References", " Chapter 10 Reproducible Workflows 10.1 Learning Objectives 10.1.1 Basic Create a reproducible script in R Markdown Edit the YAML header to add table of contents and other options Include a table Include a figure Use source() to include code from an external file Report the output of an analysis using inline R 10.1.2 Intermediate Output doc and PDF formats Add a bibliography and in-line citations Format tables using kableExtra 10.1.3 Advanced Create a computationally reproducible project in Code Ocean 10.2 Resources Chapter 27: R Markdown in R for Data Science R Markdown Cheat Sheet R Markdown reference Guide R Markdown Tutorial R Markdown: The Definitive Guide by Yihui Xie, J. J. Allaire, &amp; Garrett Grolemund Papaja Reproducible APA Manuscripts Code Ocean for Computational Reproducibility 10.3 R Markdown library(tidyverse) By now you should be pretty comfortable working with R Markdown files from the weekly formative exercises and set exercises. Here, we’ll explore some of the more advanced options and create an R Markdown document that produces a reproducible manuscript. First, make a new R Markdown document. 10.3.1 knitr options When you create a new R Markdown file in RStudio, a setup chunk is automatically created. ```{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE) ``` You can set more default options for code chunks here. See the knitr options documentation for explanations of the possible options. ```{r setup, include=FALSE} knitr::opts_chunk$set( fig.width = 8, fig.height = 5, fig.path = &#39;images/&#39;, echo = FALSE, warning = TRUE, message = FALSE, cache = FALSE ) ``` The code above sets the following options: fig.width = 8 : figure width is 8 inches fig.height = 5 : figure height is 5 inches fig.path = 'images/' : figures are saved in the directory “images” echo = FALSE : do not show code chunks in the rendered document warning = FALSE : do not show any function warnings message = FALSE : do not show any function messages cache = FALSE : run all the code to create all of the images and objects each time you knit (set to TRUE if you have time-consuming code) 10.3.2 YAML Header The YAML header is where you can set several options. --- title: &quot;My Demo Document&quot; author: &quot;Me&quot; output: html_document: theme: spacelab highlight: tango toc: true toc_float: collapsed: false smooth_scroll: false toc_depth: 3 number_sections: false --- The built-in themes are: “cerulean”, “cosmo”, “flatly”, “journal”, “lumen”, “paper”, “readable”, “sandstone”, “simplex”, “spacelab”, “united”, and “yeti”. You can view and download more themes. Try changing the values from false to true to see what the options do. 10.3.3 TOC and Document Headers If you include a table of contents (toc), it is created from your document headers. Headers in markdown are created by prefacing the header title with one or more hashes (#). Add a typical paper structure to your document like the one below. ## Abstract My abstract here... ## Introduction What&#39;s the question; why is it interesting? ## Methods ### Participants How many participants and why? Do your power calculation here. ### Procedure What will they do? ### Analysis Describe the analysis plan... ## Results Demo results for simulated data... ## Discussion What does it all mean? ## References 10.3.4 Code Chunks You can include code chunks that create and display images, tables, or computations to include in your text. Let’s start by simulating some data. First, create a code chunk in your document. You can put this before the abstract, since we won’t be showing the code in this document. We’ll use a modified version of the two_sample function from the GLM lecture to create two groups with a difference of 0.75 and 100 observations per group. This function was modified to add sex and effect-code both sex and group. Using the recode function to set effect or difference coding makes it clearer which value corresponds to which level. There is no effect of sex or interaction with group in these simulated data. two_sample &lt;- function(diff = 0.5, n_per_group = 20) { tibble(Y = c(rnorm(n_per_group, -.5 * diff, sd = 1), rnorm(n_per_group, .5 * diff, sd = 1)), grp = factor(rep(c(&quot;a&quot;, &quot;b&quot;), each = n_per_group)), sex = factor(rep(c(&quot;female&quot;, &quot;male&quot;), times = n_per_group)) ) %&gt;% mutate( grp_e = recode(grp, &quot;a&quot; = -0.5, &quot;b&quot; = 0.5), sex_e = recode(sex, &quot;female&quot; = -0.5, &quot;male&quot; = 0.5) ) } This function requires the tibble and dplyr packages, so remember to load the whole tidyverse package at the top of this script (e.g., in the setup chunk). Now we can make a separate code chunk to create our simulated dataset dat. dat &lt;- two_sample(diff = 0.75, n_per_group = 100) 10.3.4.1 Tables Next, create a code chunk where you want to display a table of the descriptives (e.g., Participants section of the Methods). We’ll use tidyverse functions you learned in the data wrangling lectures to create summary statistics for each group. &#96;&#96;&#96;{r, results='asis'} dat %>% group_by(grp, sex) %>% summarise(n = n(), Mean = mean(Y), SD = sd(Y)) %>% rename(group = grp) %>% mutate_if(is.numeric, round, 3) %>% knitr::kable() &#96;&#96;&#96; ## `summarise()` regrouping output by &#39;grp&#39; (override with `.groups` argument) ## `mutate_if()` ignored the following grouping variables: ## Column `group` group sex n Mean SD a female 50 -0.361 0.796 a male 50 -0.284 1.052 b female 50 0.335 1.080 b male 50 0.313 0.904 Notice that the r chunk specifies the option results='asis'. This lets you format the table using the kable() function from knitr. You can also use more specialised functions from papaja or kableExtra to format your tables. 10.3.4.2 Images Next, create a code chunk where you want to display the image in your document. Let’s put it in the Results section. Use what you learned in the data visualisation lecture to show violin-boxplots for the two groups. &#96;&#96;&#96;{r, fig1, fig.cap=\"Figure 1. Scores by group and sex.\"} ggplot(dat, aes(grp, Y, fill = sex)) + geom_violin(alpha = 0.5) + geom_boxplot(width = 0.25, position = position_dodge(width = 0.9), show.legend = FALSE) + scale_fill_manual(values = c(\"orange\", \"purple\")) + xlab(\"Group\") + ylab(\"Score\") + theme(text = element_text(size = 30, family = \"Times\")) &#96;&#96;&#96; The last line changes the default text size and font, which can be useful for generating figures that meet a journal’s requirements. Figure 10.1: Figure 1. Scores by group and sex. You can also include images that you did not create in R using the typical markdown syntax for images: ![All the Things by [Hyperbole and a Half](http://hyperboleandahalf.blogspot.com/)](images/memes/x-all-the-things.png) All the Things by Hyperbole and a Half 10.3.4.3 In-line R Now let’s use what you learned in the GLM lecture to analyse our simulated data. The document is getting a little cluttered, so let’s move this code to external scripts. Create a new R script called “functions.R” Move the library(tidyverse) line and the two_sample() function definition to this file. Create a new R script called “analysis.R” Move the code for creating dat to this file. Add the following code to the end of the setup chunk: source(&quot;functions.R&quot;) source(&quot;analysis.R&quot;) The source function lets you include code from an external file. This is really useful for making your documents readable. Just make sure you call your source files in the right order (e.g., include function definitions before you use the functions). In the “analysis.R” file, we’re going to run the analysis code and save any numbers we might want to use in our manuscript to variables. grp_lm &lt;- lm(Y ~ grp_e * sex_e, data = dat) stats &lt;- grp_lm %&gt;% broom::tidy() %&gt;% mutate_if(is.numeric, round, 3) The code above runs our analysis predicting Y from the effect-coded group variable grp_e, the effect-coded sex variable sex_e and their intereaction. The tidy function from the broom package turns the output into a tidy table. The mutate_if function uses the function is.numeric to check if each column should be mutated, adn if it is numeric, applies the round function with the digits argument set to 3. If you want to report the results of the analysis in a paragraph istead of a table, you need to know how to refer to each number in the table. Like with everything in R, there are many wways to do this. One is by specifying the column and row number like this: stats$p.value[2] ## [1] 0 Another way is to create variables for each row like this: grp_stats &lt;- filter(stats, term == &quot;grp_e&quot;) sex_stats &lt;- filter(stats, term == &quot;sex_e&quot;) ixn_stats &lt;- filter(stats, term == &quot;grp_e:sex_e&quot;) Add the above code to the end of your analysis.R file. Then you can refer to columns by name like this: grp_stats$p.value sex_stats$statistic ixn_stats$estimate ## [1] 0 ## [1] 0.197 ## [1] -0.099 You can insert these numbers into a paragraph with inline R code that looks like this: Scores were higher in group B than group A (B = &#96;r grp_stats$estimate&#96;, t = &#96;r grp_stats$statistic&#96;, p = &#96;r grp_stats$p.value&#96;). There was no significant difference between men and women (B = &#96;r sex_statsestimate&#96;, t = &#96;r sex_stats$statistic&#96;, p = &#96;r sex_stats$p.value&#96;) and the effect of group was not qualified by an interaction with sex (B = &#96;r ixn_stats$estimate&#96;, t = &#96;r ixn_stats$statistic&#96;, p = &#96;r ixn_stats$p.value&#96;). Rendered text: Scores were higher in group B than group A (B = 0.647, t = 4.74, p = 0). There was no significant difference between men and women (B = 0.027, t = 0.197, p = 0.844) and the effect of group was not qualified by an interaction with sex (B = -0.099, t = -0.363, p = 0.717). Remember, line breaks are ignored when you render the file (unless you add two spaces at the end of lines), so you can use line breaks to make it easier to read your text with inline R code. The p-values aren’t formatted in APA style. We wrote a function to deal with this in the function lecture. Add this function to the “functions.R” file and change the inline text to use the report_p function. report_p &lt;- function(p, digits = 3) { if (!is.numeric(p)) stop(&quot;p must be a number&quot;) if (p &lt;= 0) warning(&quot;p-values are normally greater than 0&quot;) if (p &gt;= 1) warning(&quot;p-values are normally less than 1&quot;) if (p &lt; .001) { reported = &quot;p &lt; .001&quot; } else { roundp &lt;- round(p, digits) fmt &lt;- paste0(&quot;p = %.&quot;, digits, &quot;f&quot;) reported = sprintf(fmt, roundp) } reported } Scores were higher in group B than group A (B = &#96;r grp_stats$estimate&#96;, t = &#96;r grp_stats$statistic&#96;, &#96;r report_p(grp_stats$p.value, 3)&#96;). There was no significant difference between men and women (B = &#96;r sex_stats$estimate&#96;, t = &#96;r sex_stats$statistic&#96;, &#96;r report_p(sex_stats$p.value, 3)&#96;) and the effect of group was not qualified by an interaction with sex (B = &#96;r ixn_stats$estimate&#96;, t = &#96;r ixn_stats$statistic&#96;, &#96;r report_p(ixn_stats$p.value, 3)&#96;). Rendered text: Scores were higher in group B than group A (B = 0.647, t = 4.74, p &lt; .001). There was no significant difference between men and women (B = 0.027, t = 0.197, p = 0.844) and the effect of group was not qualified by an interaction with sex (B = -0.099, t = -0.363, p = 0.717). You might also want to report the statistics for the regression. There are a lot of numbers to format and insert, so it is easier to do this in the analysis script using sprintf for formatting. s &lt;- summary(grp_lm) # calculate p value from fstatistic fstat.p &lt;- pf(s$fstatistic[1], s$fstatistic[2], s$fstatistic[3], lower=FALSE) adj_r &lt;- sprintf( &quot;The regression equation had an adjusted $R^{2}$ of %.3f ($F_{(%i, %i)}$ = %.3f, %s).&quot;, round(s$adj.r.squared, 3), s$fstatistic[2], s$fstatistic[3], round(s$fstatistic[1], 3), report_p(fstat.p, 3) ) Then you can just insert the text in your manuscript like this: ` adj_r`: The regression equation had an adjusted \\(R^{2}\\) of 0.090 (\\(F_{(3, 196)}\\) = 7.546, p &lt; .001). 10.3.5 Bibliography There are several ways to do in-text citations and automatically generate a bibliography in RMarkdown. 10.3.5.1 Create a BibTeX File Manually You can just make a BibTeX file and add citations manually. Make a new Text File in RStudio called “bibliography.bib”. Next, add the line bibliography: bibliography.bib to your YAML header. You can add citations in the following format: @article{shortname, author = {Author One and Author Two and Author Three}, title = {Paper Title}, journal = {Journal Title}, volume = {vol}, number = {issue}, pages = {startpage--endpage}, year = {year}, doi = {doi} } 10.3.5.2 Citing R packages You can get the citation for an R package using the function citation. You can paste the bibtex entry into your bibliography.bib file. Make sure to add a short name (e.g., “rmarkdown”) before the first comma to refer to the reference. citation(package=&quot;rmarkdown&quot;) ## ## To cite the &#39;rmarkdown&#39; package in publications, please use: ## ## JJ Allaire and Yihui Xie and Jonathan McPherson and Javier Luraschi ## and Kevin Ushey and Aron Atkins and Hadley Wickham and Joe Cheng and ## Winston Chang and Richard Iannone (2020). rmarkdown: Dynamic ## Documents for R. R package version 2.3. URL ## https://rmarkdown.rstudio.com. ## ## Yihui Xie and J.J. Allaire and Garrett Grolemund (2018). R Markdown: ## The Definitive Guide. Chapman and Hall/CRC. ISBN 9781138359338. URL ## https://bookdown.org/yihui/rmarkdown. ## ## To see these entries in BibTeX format, use &#39;print(&lt;citation&gt;, ## bibtex=TRUE)&#39;, &#39;toBibtex(.)&#39;, or set ## &#39;options(citation.bibtex.max=999)&#39;. 10.3.5.3 Download Citation Info You can get the BibTeX formatted citation from most publisher websites. For example, go to the publisher’s page for Equivalence Testing for Psychological Research: A Tutorial, click on the Cite button (in the sidebar or under the bottom Explore More menu), choose BibTeX format, and download the citation. You can open up the file in a text editor and copy the text. It should look like this: @article{doi:10.1177/2515245918770963, author = {Daniël Lakens and Anne M. Scheel and Peder M. Isager}, title ={Equivalence Testing for Psychological Research: A Tutorial}, journal = {Advances in Methods and Practices in Psychological Science}, volume = {1}, number = {2}, pages = {259-269}, year = {2018}, doi = {10.1177/2515245918770963}, URL = { https://doi.org/10.1177/2515245918770963 }, eprint = { https://doi.org/10.1177/2515245918770963 } , abstract = { Psychologists must be able to test both for the presence of an effect and for the absence of an effect. In addition to testing against zero, researchers can use the two one-sided tests (TOST) procedure to test for equivalence and reject the presence of a smallest effect size of interest (SESOI). The TOST procedure can be used to determine if an observed effect is surprisingly small, given that a true effect at least as extreme as the SESOI exists. We explain a range of approaches to determine the SESOI in psychological science and provide detailed examples of how equivalence tests should be performed and reported. Equivalence tests are an important extension of the statistical tools psychologists currently use and enable researchers to falsify predictions about the presence, and declare the absence, of meaningful effects. } } Paste the reference into your bibliography.bib file. Change doi:10.1177/2515245918770963 in the first line of the reference to a short string you will use to cite the reference in your manuscript. We’ll use TOSTtutorial. 10.3.5.4 Converting from reference software Most reference software like EndNote, Zotero or mendeley have exporting options that can export to BibTeX format. You just need to check the shortnames in the resulting file. 10.3.5.5 In-text citations You can cite reference in text like this: This tutorial uses several R packages [@tidyverse;@rmarkdown]. This tutorial uses several R packages (Wickham 2017; Allaire et al. 2018). Put a minus in front of the @ if you just want the year: Lakens, Scheel and Isengar [-@TOSTtutorial] wrote a tutorial explaining how to test for the absence of an effect. Lakens, Scheel and Isengar (2018) wrote a tutorial explaining how to test for the absence of an effect. 10.3.5.6 Citation Styles You can search a list of style files for various journals and download a file that will format your bibliography for a specific journal’s style. You’ll need to add the line csl: filename.csl to your YAML header. Add some citations to your bibliography.bib file, reference them in your text, and render your manuscript to see the automatically generated reference section. Try a few different citation style files. 10.3.6 Output Formats You can knit your file to PDF or Word if you have the right packages installed on your computer. 10.3.7 Computational Reproducibility Computational reproducibility refers to making all aspects of your analysis reproducible, including specifics of the software you used to run the code you wrote. R packages get updated periodically and some of these updates may break your code. Using a computational reproducibility platform guards against this by always running your code in the same environment. Code Ocean is a new platform that lets you run your code in the cloud via a web browser. 10.4 References E References "],
["acknowledgements.html", "Acknowledgements 10.5 Contributors", " Acknowledgements The whole psyTeachR team at the University of Glasgow School of Psychology deserves enormous thanks for making it possible and rewarding to teach methods with a focus on reproducibility and open science. Particularly Heather Cleland Woods, Phil McAleer, Helena Paterson, Emily Nordmann, Benedict Jones, and Niamh Stack. We greatly appreciate Iris Holzleitner’s volunteer in-class assistance with the first year of this course. We were ever so lucky to get Rebecca Lai as a teaching assistant in the second year; her kind and patient approach to teaching technical skills is an inspiration. Thanks to Daniël Lakens for many inspirational discussions and resources. 10.5 Contributors Several people contributed to testing these materials. Rebecca Lai Richard Morey Mossa Merhi Reimert "],
["installingr.html", "A Installing R A.1 Installing Base R A.2 Installing RStudio A.3 Installing LaTeX", " A Installing R Installing R and RStudio is usually straightforward. The sections below explain how and there is a helpful YouTube video here. A.1 Installing Base R Install base R from https://cran.rstudio.com/. Choose the download link for your operating system (Linux, Mac OS X, or Windows). If you have a Mac, install the latest release from the newest R-x.x.x.pkg link (or a legacy version if you have an older operating system). After you install R, you should also install XQuartz to be able to use some visualisation packages. If you are installing the Windows version, choose the “base” subdirectory and click on the download link at the top of the page. After you install R, you should also install RTools; use the “recommended” version highlighted near the top of the list. If you are using Linux, choose your specific operating system and follow the installation instructions. A.2 Installing RStudio Go to rstudio.com and download the RStudio Desktop (Open Source License) version for your operating system under the list titled Installers for Supported Platforms. A.3 Installing LaTeX You can install the LaTeX typesetting system to produce PDF reports from RStudio. Without this additional installation, you will be able to produce reports in HTML but not PDF. To generate PDF reports, you will additionally need: pandoc, and LaTeX, a typesetting language, available for WINDOWS: MikTeX Mac OS: MacTex (3.2GB download) or BasicTeX (78MB download, but should work fine) Linux: TeX Live "],
["symbols.html", "B Symbols", " B Symbols Symbol psyTeachR Term Also Known As () (round) brackets parentheses [] square brackets brackets {} curly brackets squiggly brackets &lt;&gt; chevrons angled brackets / guillemets &lt; less than &gt; greater than &amp; ampersand “and” symbol # hash pound / octothorpe / slash forward slash \\ backslash - dash hyphen / minus _ underscore * asterisk star ^ caret power symbol ~ tilde twiddle / squiggle = equal sign == double equal sign . full stop period / point ! exclamation mark bang / not ? question mark ’ single quote quote / apostrophe \" double quote quote %&gt;% pipe magrittr pipe | vertical bar pipe , comma ; semi-colon : colon @ “at” symbol various hilarious regional terms "],
["exercise-answers.html", "C Exercise Answers", " C Exercise Answers Download all exercises and data files below as a ZIP archive. The answers are not included in the zip file. 01 intro (answers): Intro to R, functions, R markdown 02 data (answers): Vectors, tabular data, data import, pipes Essential Skills (answers): You must be able to complete these exercises to advance in the class beyond the first two lectures 03 ggplot (answers): Data visualisation 04 tidyr (answers): Tidy Data 05 dplyr (answers): Data wrangling 06 joins (answers): Data relations 07 functions (answers): Functions and iteration 08 simulation (answers): Simulation 09 glm (answers): GLM "],
["datasets.html", "D Datasets", " D Datasets disgust.csv Three domain disgust questionnaire dplyr lesson disgust_cors.csv Correlations between questions on the three domain disgust questionnaire (calculated from disgust.csv) ggplot exercise disgust_scores.csv Subscale scores for the three domain disgust questionnaire (calculated from disgust.csv) data exercise ggplot exercise joins exercise EMBU_mother.csv EMBU parenting questionnaire (not used in chapters yet) empathizing.csv Empathizing questionnaire (not used in chapters yet) eye_descriptions.csv Text descriptions of eyes tidyr exercise dplyr exercise family_composition.csv Parents’ ages and number of siblings of different types tidyr exercise dplyr exercise infmort.csv Infant mortality by country and year tidyr lesson matmort.xls Maternal mortality by country and year tidyr lesson personality.csv Five-factor personality questionnaire tidyr lesson personality_scores.csv Subscale scores on the five-factor personality questionnaire (calculated from personality.csv) joins exercise sensation_seeking.csv Sensation-seeking questionnaire tidyr exercise sortmasc.csv Data for face sorting task where 5 versions of averaged faces of varying masculinity are sorted in order of preference (not used in chapters yet) systemizing.csv Systemizing questionnaire (not used in chapters yet) users.csv ID, sex, and birthday for participants in most questionnaires joins exercise users2.csv ID, sex, and birthday for participants in other questionnaires joins exercise "],
["references.html", "E References", " E References "]
]
